1
00:00:00,000 --> 00:00:01,480
 All right.

2
00:00:01,480 --> 00:00:04,840
 It's about that time.

3
00:00:04,840 --> 00:00:06,960
 OK.

4
00:00:06,960 --> 00:00:09,360
 Welcome back, everybody.

5
00:00:09,360 --> 00:00:14,480
 So as we've been forecasting for a little while,

6
00:00:14,480 --> 00:00:16,120
 it's deep learning week.

7
00:00:16,120 --> 00:00:18,720
 So it's deep perception week.

8
00:00:18,720 --> 00:00:21,580
 And so just to remind you some of the times

9
00:00:21,580 --> 00:00:25,120
 that we've mentioned it before, I

10
00:00:25,120 --> 00:00:29,000
 think we've done a lot of nice algorithmic work

11
00:00:29,000 --> 00:00:33,400
 with geometry and used it for perception.

12
00:00:33,400 --> 00:00:35,720
 But in this example that we were talking about at the end

13
00:00:35,720 --> 00:00:38,800
 last time, there are limitations that I

14
00:00:38,800 --> 00:00:42,320
 think are fairly fundamental of using only geometry.

15
00:00:42,320 --> 00:00:47,120
 So in particular, if you don't really know what an object is,

16
00:00:47,120 --> 00:00:49,320
 you just know where the points are in space,

17
00:00:49,320 --> 00:00:51,280
 then the systems that we were talking about that

18
00:00:51,280 --> 00:00:54,920
 were just using geometry to pick antipodal grasps,

19
00:00:54,920 --> 00:00:56,960
 or anything really just based on geometry,

20
00:00:56,960 --> 00:00:58,320
 that wasn't about antipodalness.

21
00:00:58,320 --> 00:01:02,320
 That was about just not understanding anything more

22
00:01:02,320 --> 00:01:04,200
 than point clouds.

23
00:01:04,200 --> 00:01:07,080
 You'll get things where you just don't have

24
00:01:07,080 --> 00:01:08,600
 any reasoning about objects.

25
00:01:08,600 --> 00:01:11,280
 You'll get double picks, meaning that one grab might

26
00:01:11,280 --> 00:01:12,620
 have picked two objects, and that

27
00:01:12,620 --> 00:01:14,580
 might have been OK or might not have been OK,

28
00:01:14,580 --> 00:01:17,920
 depending on the application.

29
00:01:17,920 --> 00:01:20,280
 You might pick up the hammer from the corner

30
00:01:20,280 --> 00:01:22,200
 and have it slip out because of a huge wrench,

31
00:01:22,200 --> 00:01:25,000
 just because you have no concept from the geometry alone

32
00:01:25,000 --> 00:01:26,620
 that there's this object there, and there's

33
00:01:26,620 --> 00:01:28,040
 a center of mass of the object, and it'd

34
00:01:28,040 --> 00:01:29,920
 be better to pick it up at the center of mass

35
00:01:29,920 --> 00:01:33,200
 so you don't get a big moment.

36
00:01:33,200 --> 00:01:36,320
 If you have geometry sensors and you're dependent on them,

37
00:01:36,320 --> 00:01:39,200
 and you can't see behind certain things,

38
00:01:39,200 --> 00:01:44,040
 then you have to think about all the pieces of information

39
00:01:44,040 --> 00:01:47,400
 that we have in our perception system that's just not

40
00:01:47,400 --> 00:01:48,820
 geometric.

41
00:01:48,820 --> 00:01:52,360
 Somehow if I see one side of a mug or one side of a whatever

42
00:01:52,360 --> 00:01:55,560
 I've got, I can infer that there's probably

43
00:01:55,560 --> 00:01:58,440
 a handle on the back side, even if I can't see that handle.

44
00:01:58,440 --> 00:02:01,440
 And if the geometry point cloud that I have right now

45
00:02:01,440 --> 00:02:02,840
 doesn't contain that information,

46
00:02:02,840 --> 00:02:06,560
 then a geometric method alone won't infer that information.

47
00:02:06,560 --> 00:02:12,240
 Sort of a practical thing is that our depth cameras

48
00:02:12,240 --> 00:02:13,720
 aren't very good at transparency.

49
00:02:13,720 --> 00:02:14,960
 But the list goes on.

50
00:02:14,960 --> 00:02:18,800
 At some point, in order to do more effective manipulation,

51
00:02:18,800 --> 00:02:21,640
 we need more than just a geometric understanding

52
00:02:21,640 --> 00:02:22,200
 of the world.

53
00:02:22,200 --> 00:02:24,440
 We need to understand what objects are.

54
00:02:24,440 --> 00:02:26,640
 We need to understand something about the properties

55
00:02:26,640 --> 00:02:28,600
 of objects.

56
00:02:28,600 --> 00:02:31,480
 So the approach in deep learning and deep perception

57
00:02:31,480 --> 00:02:34,400
 is, as we said, sort of a statistical approach.

58
00:02:34,400 --> 00:02:38,540
 So the reason you know to pick up a hammer in the middle

59
00:02:38,540 --> 00:02:41,560
 is because you picked up a bunch of things that look like hammers

60
00:02:41,560 --> 00:02:42,920
 in the image.

61
00:02:42,920 --> 00:02:45,840
 And some of them fell out of my hand and some of them didn't.

62
00:02:45,840 --> 00:02:48,840
 Or the reason you know that the mug should be on the table

63
00:02:48,840 --> 00:02:50,300
 is because I saw a lot of pictures

64
00:02:50,300 --> 00:02:52,160
 and the mugs were never up in the air.

65
00:02:52,160 --> 00:02:53,960
 They were always at the table.

66
00:02:53,960 --> 00:02:56,920
 And if you can correlate your current perception

67
00:02:56,920 --> 00:02:59,840
 with enough of your data, then it

68
00:02:59,840 --> 00:03:02,200
 can be a very strong prior that overcomes

69
00:03:02,200 --> 00:03:04,320
 some of the limitations and does,

70
00:03:04,320 --> 00:03:08,200
 in some mystical, magical way, capture some

71
00:03:08,200 --> 00:03:12,200
 of these object understanding, the things

72
00:03:12,200 --> 00:03:17,960
 that I would have attributed to really reasoning about objects.

73
00:03:17,960 --> 00:03:18,680
 I would say--

74
00:03:18,680 --> 00:03:21,560
 I mean, some people ask me, why did you

75
00:03:21,560 --> 00:03:23,720
 switch from working on legged robots

76
00:03:23,720 --> 00:03:26,000
 to working on manipulation?

77
00:03:26,000 --> 00:03:28,800
 Even though I have a bit of a love-hate relationship

78
00:03:28,800 --> 00:03:31,240
 with deep learning, just because I think sometimes it's

79
00:03:31,240 --> 00:03:34,680
 overstated and doesn't involve deep thinking or whatever.

80
00:03:34,680 --> 00:03:39,440
 But I would say it's the reason I changed.

81
00:03:39,440 --> 00:03:42,040
 That I think that the fact that perception

82
00:03:42,040 --> 00:03:44,400
 started to work so well and open up

83
00:03:44,400 --> 00:03:47,160
 so many possibilities in manipulation

84
00:03:47,160 --> 00:03:51,280
 really just kind of cracks open this huge, interesting idea

85
00:03:51,280 --> 00:03:52,480
 of where you can bring--

86
00:03:52,480 --> 00:03:54,720
 for me, it's bringing some of the ideas from dynamics

87
00:03:54,720 --> 00:03:57,040
 and control closer to what I think

88
00:03:57,040 --> 00:03:59,880
 of as common sense reasoning and about a much more

89
00:03:59,880 --> 00:04:06,120
 diverse set of tasks, a much more diverse set of objectives.

90
00:04:06,120 --> 00:04:09,240
 So that only happened because perception started working.

91
00:04:09,240 --> 00:04:11,320
 And you could start dreaming of doing manipulation

92
00:04:11,320 --> 00:04:14,320
 for many more diverse tasks than we ever did before.

93
00:04:14,320 --> 00:04:15,840
 And the manipulation of the 1980s

94
00:04:15,840 --> 00:04:17,680
 just did not look like what we're trying

95
00:04:17,680 --> 00:04:19,120
 to do in manipulation today.

96
00:04:19,120 --> 00:04:27,000
 So OK, so the problem--

97
00:04:27,000 --> 00:04:32,160
 so even though I do love deep learning in some ways,

98
00:04:32,160 --> 00:04:34,560
 I hate teaching it.

99
00:04:34,560 --> 00:04:36,040
 I mean, I struggle to teach it.

100
00:04:36,040 --> 00:04:37,320
 Let me say that.

101
00:04:37,320 --> 00:04:40,080
 Because I know some of you are probably training

102
00:04:40,080 --> 00:04:41,080
 deep networks right now.

103
00:04:41,080 --> 00:04:48,560
 And some of you haven't touched a deep network.

104
00:04:48,560 --> 00:04:50,840
 But I think I've got a pretty good strategy.

105
00:04:50,840 --> 00:04:55,440
 You can give me feedback and ask questions or yawn or whatever

106
00:04:55,440 --> 00:04:58,880
 and help me walk the landscape.

107
00:04:58,880 --> 00:04:59,560
 Oh, that's true.

108
00:04:59,560 --> 00:05:00,080
 I can see.

109
00:05:00,080 --> 00:05:01,200
 I can guess.

110
00:05:01,200 --> 00:05:02,520
 Just do a really dramatic yawn.

111
00:05:02,520 --> 00:05:06,560
 And through the mask, I'll understand.

112
00:05:06,560 --> 00:05:08,680
 OK, so but I think I can say a few things that will

113
00:05:08,680 --> 00:05:13,160
 bring everybody up to speed.

114
00:05:13,160 --> 00:05:14,880
 My basic strategy is I'm not going

115
00:05:14,880 --> 00:05:17,960
 to talk a lot about neural architectures

116
00:05:17,960 --> 00:05:19,600
 for different things.

117
00:05:19,600 --> 00:05:21,360
 There's plenty of good resources online.

118
00:05:21,360 --> 00:05:23,760
 I will put some pointers in the notes.

119
00:05:23,760 --> 00:05:26,400
 I'm going to try to focus in on the parts that

120
00:05:26,400 --> 00:05:29,480
 are more relevant to robotics and manipulation

121
00:05:29,480 --> 00:05:33,400
 and the way that you feed these tools data properly,

122
00:05:33,400 --> 00:05:37,240
 the way you ask reasonable questions of these tools.

123
00:05:37,240 --> 00:05:40,720
 And I'll say a little bit about how they work only in the sense

124
00:05:40,720 --> 00:05:42,720
 that you can use them better, not in the sense

125
00:05:42,720 --> 00:05:45,880
 that I think if you want to architect them differently

126
00:05:45,880 --> 00:05:48,040
 and choose different numbers of layers and stuff,

127
00:05:48,040 --> 00:05:51,960
 that's not what I'm going to talk about here.

128
00:05:51,960 --> 00:05:59,200
 OK, but the basics I think we just have to get out there

129
00:05:59,200 --> 00:06:01,880
 is there's some basic questions about just what kind

130
00:06:01,880 --> 00:06:04,840
 of questions are we formulating for a deep network,

131
00:06:04,840 --> 00:06:07,720
 and then how do we generate training data

132
00:06:07,720 --> 00:06:09,760
 if we're going to start manipulating

133
00:06:09,760 --> 00:06:11,960
 our objects in our bins.

134
00:06:11,960 --> 00:06:15,800
 So let's tell that story.

135
00:06:15,800 --> 00:06:21,080
 So the first question is if my deep neural network is trying

136
00:06:21,080 --> 00:06:24,800
 to do perception, the simplest version of the picture

137
00:06:24,800 --> 00:06:27,080
 is I've got a big neural network.

138
00:06:27,080 --> 00:06:31,560
 I'm going to send images into it.

139
00:06:31,560 --> 00:06:39,320
 So almost certainly they're going to be RGB.

140
00:06:39,320 --> 00:06:41,240
 It's interesting.

141
00:06:41,240 --> 00:06:43,120
 Sometimes people send the depth channel in.

142
00:06:43,120 --> 00:06:44,040
 Sometimes they don't.

143
00:06:44,040 --> 00:06:46,800
 Sometimes they try to change the depth channel's representation

144
00:06:46,800 --> 00:06:48,640
 and shove it in.

145
00:06:48,640 --> 00:06:51,160
 But I think a surprising amount of--

146
00:06:51,160 --> 00:06:53,560
 even when people have depth data,

147
00:06:53,560 --> 00:06:56,480
 a surprising number of the techniques use RGB alone.

148
00:06:56,480 --> 00:06:58,800
 So let's focus on that first.

149
00:06:58,800 --> 00:07:00,760
 So I'm going to send an RGB image in,

150
00:07:00,760 --> 00:07:04,160
 and I'm going to ask some question out.

151
00:07:04,160 --> 00:07:06,520
 And we'll talk about various questions

152
00:07:06,520 --> 00:07:11,560
 that we'd like to have the deep part of our perception system

153
00:07:11,560 --> 00:07:13,800
 answer.

154
00:07:13,800 --> 00:07:17,040
 For instance, we'll do pose estimation

155
00:07:17,040 --> 00:07:18,160
 by the end of the lecture.

156
00:07:18,160 --> 00:07:22,120
 Maybe if I've trained this to be the mug pose estimation

157
00:07:22,120 --> 00:07:26,640
 algorithm, then maybe I can have a pose coming out.

158
00:07:26,640 --> 00:07:28,520
 But to build up to that, we can ask

159
00:07:28,520 --> 00:07:30,640
 some of the more standard computer vision questions

160
00:07:30,640 --> 00:07:33,880
 and just make sure we understand the types of questions

161
00:07:33,880 --> 00:07:38,040
 and the type of loss functions and the like

162
00:07:38,040 --> 00:07:41,040
 that people have been very successful with.

163
00:07:41,040 --> 00:07:43,120
 So the first version of the question

164
00:07:43,120 --> 00:07:44,760
 would just be image recognition.

165
00:07:44,760 --> 00:07:46,400
 I think this picture says--

166
00:07:46,400 --> 00:07:51,160
 almost is very efficiently explains

167
00:07:51,160 --> 00:07:52,840
 some of the basic landscape.

168
00:07:52,840 --> 00:07:55,360
 So if I send a picture in, and I'm

169
00:07:55,360 --> 00:07:58,440
 asking for just image recognition,

170
00:07:58,440 --> 00:08:01,400
 then the output of that is just a set of numbers.

171
00:08:01,400 --> 00:08:04,240
 So I might have many outputs.

172
00:08:04,240 --> 00:08:09,600
 In this case, I'd have a sheep output.

173
00:08:09,600 --> 00:08:10,840
 What is it?

174
00:08:10,840 --> 00:08:11,960
 Dog, cat, horse.

175
00:08:11,960 --> 00:08:21,160
 I have many outputs of my neural network.

176
00:08:21,160 --> 00:08:23,880
 This is just some big function that has a vector output

177
00:08:23,880 --> 00:08:27,120
 and a big vector input.

178
00:08:27,120 --> 00:08:29,520
 And the most basic question is just,

179
00:08:29,520 --> 00:08:33,720
 is there a sheep in the image, true or false?

180
00:08:33,720 --> 00:08:35,840
 I can generate a lot of training data

181
00:08:35,840 --> 00:08:37,960
 where someone tells me, yes, there's a sheep in it.

182
00:08:37,960 --> 00:08:40,480
 In fact, you guys are training deep networks

183
00:08:40,480 --> 00:08:43,200
 whenever you're doing these CAPTCHAs nowadays.

184
00:08:43,200 --> 00:08:45,440
 Click on the ones that have a traffic light in there,

185
00:08:45,440 --> 00:08:47,680
 and you're training all the autonomous cars.

186
00:08:47,680 --> 00:08:51,640
 So you're generating training data for these systems.

187
00:08:51,640 --> 00:08:56,040
 And if my training data says, for every image I put in,

188
00:08:56,040 --> 00:08:59,760
 I have 1, 0, 0, 0 for the ones that have a sheep and only

189
00:08:59,760 --> 00:09:05,120
 a sheep, and I have 0, 1, 0, 0 for the ones that have a dog,

190
00:09:05,120 --> 00:09:09,040
 then I can answer the basic image recognition question.

191
00:09:09,040 --> 00:09:12,640
 And the network will predict a probability.

192
00:09:12,640 --> 00:09:15,240
 It'll do its best to predict 1.

193
00:09:15,240 --> 00:09:19,480
 The right answer here would be, I guess, 1, 1, 0, 0, maybe,

194
00:09:19,480 --> 00:09:22,040
 since there are sheep and a dog.

195
00:09:22,040 --> 00:09:26,080
 But it's going to do its best to predict the training data.

196
00:09:26,080 --> 00:09:29,480
 And in practice, we say, we'll take the largest detection

197
00:09:29,480 --> 00:09:31,520
 and maybe call that a sheep.

198
00:09:31,520 --> 00:09:35,760
 Yes, there's a sheep in that image, or we'll threshold it.

199
00:09:35,760 --> 00:09:38,120
 One step more refined than that is

200
00:09:38,120 --> 00:09:40,680
 if you try to do object detection, where you're

201
00:09:40,680 --> 00:09:43,920
 actually not just saying, yes, there's a sheep in the image,

202
00:09:43,920 --> 00:09:46,600
 but there's a sheep and it's here.

203
00:09:46,600 --> 00:09:50,520
 Typically, with bounding box is the simplest.

204
00:09:50,520 --> 00:09:52,000
 In 2D, it's just a bounding box.

205
00:09:52,000 --> 00:09:54,200
 In 3D, you have a choice about whether you're

206
00:09:54,200 --> 00:09:56,720
 going to do an oriented bounding box or an axis-aligned bounding

207
00:09:56,720 --> 00:09:57,200
 box.

208
00:09:57,200 --> 00:09:59,800
 But basically, you just put a box around the things

209
00:09:59,800 --> 00:10:04,760
 that you're trying to detect.

210
00:10:04,760 --> 00:10:09,800
 And you can detect now potentially a dog and a sheep.

211
00:10:09,800 --> 00:10:12,920
 And then there's a distinction between whether you're

212
00:10:12,920 --> 00:10:17,400
 doing semantic segmentation or instance segmentation.

213
00:10:17,400 --> 00:10:21,500
 So a more refined query is to say,

214
00:10:21,500 --> 00:10:23,880
 show me the pixels in the image.

215
00:10:23,880 --> 00:10:28,520
 Basically, now I have an image in and an image out.

216
00:10:29,520 --> 00:10:30,520
 Where did my eraser go?

217
00:10:30,520 --> 00:10:48,880
 Labeled image out, where every pixel is labeled either dog,

218
00:10:48,880 --> 00:10:51,760
 sheep, or I don't know, for instance.

219
00:10:51,760 --> 00:10:53,400
 And this would be my training data

220
00:10:53,400 --> 00:10:55,840
 for a semantic segmentation, which just says,

221
00:10:55,840 --> 00:11:00,040
 for each pixel, is it a dog, is it a sheep?

222
00:11:00,040 --> 00:11:02,800
 And we'll distinguish that from instance segmentation,

223
00:11:02,800 --> 00:11:08,400
 where different sheep are colored a different pixel.

224
00:11:08,400 --> 00:11:14,440
 So that you can now tell that there are multiple sheep

225
00:11:14,440 --> 00:11:18,120
 and you can distinguish their boundaries.

226
00:11:18,120 --> 00:11:21,940
 So that's just the basic vocabulary.

227
00:11:21,940 --> 00:11:23,860
 There's going to be more questions, of course,

228
00:11:23,860 --> 00:11:27,120
 that we want for manipulation than just these questions.

229
00:11:27,120 --> 00:11:29,400
 But these are the ones that we have massive data

230
00:11:29,400 --> 00:11:32,360
 sets to train with.

231
00:11:32,360 --> 00:11:35,360
 It turns out that if you get to instance segmentation, which

232
00:11:35,360 --> 00:11:38,440
 is a standard computer vision ask,

233
00:11:38,440 --> 00:11:40,480
 you can do a lot of stuff for manipulation.

234
00:11:40,480 --> 00:11:43,800
 That already would power a lot of the pipelines

235
00:11:43,800 --> 00:11:46,240
 we've already talked about for manipulation.

236
00:11:50,680 --> 00:11:58,580
 So if I'm looking in my dirty bin, my cluttered bin,

237
00:11:58,580 --> 00:12:00,340
 and I say, now pick out the spam can,

238
00:12:00,340 --> 00:12:03,580
 or pick out the cheese it box, or pick out the mustard,

239
00:12:03,580 --> 00:12:06,700
 then what you really want here is an instance segmentation

240
00:12:06,700 --> 00:12:08,660
 pipeline, where you can say, look down,

241
00:12:08,660 --> 00:12:11,300
 tell me the pixels that are associated with the mustard

242
00:12:11,300 --> 00:12:12,500
 bottle.

243
00:12:12,500 --> 00:12:14,860
 And then I can do, for instance, antipodal grasping,

244
00:12:14,860 --> 00:12:17,440
 but restrict my search to the points that are associated

245
00:12:17,440 --> 00:12:20,660
 with the mustard bottle.

246
00:12:20,660 --> 00:12:24,180
 So if we can use the existing data sets and standard pipelines

247
00:12:24,180 --> 00:12:25,540
 and get to instance segmentation,

248
00:12:25,540 --> 00:12:28,780
 we're already in pretty good shape.

249
00:12:28,780 --> 00:12:32,380
 Now, I think a lot of people know a lot about the--

250
00:12:32,380 --> 00:12:35,060
 the history of deep learning getting here

251
00:12:35,060 --> 00:12:40,180
 was through large labeled data sets, typically crowdsourced.

252
00:12:40,180 --> 00:12:45,300
 And we really did have lots of people clicking pixel by pixel

253
00:12:45,300 --> 00:12:49,100
 and labeling these images and making massive--

254
00:12:49,100 --> 00:12:51,340
 the COCO data set is the one that goes all the way

255
00:12:51,340 --> 00:12:53,760
 through to instance segmentation that has been very famous

256
00:12:53,760 --> 00:12:55,220
 and very fruitful.

257
00:12:55,220 --> 00:12:58,880
 ImageNet was the biggest first one,

258
00:12:58,880 --> 00:13:01,300
 but COCO is the one that added the instance segmentation

259
00:13:01,300 --> 00:13:03,740
 labels.

260
00:13:03,740 --> 00:13:08,260
 And it was extremely expensive to generate to begin with.

261
00:13:08,260 --> 00:13:13,460
 But once we had it, a lot of things happened very quickly.

262
00:13:13,460 --> 00:13:17,580
 It's actually kind of fun to look at the categories that

263
00:13:17,580 --> 00:13:19,540
 are available in COCO.

264
00:13:19,540 --> 00:13:25,740
 It's a little bit small, but the website resisted zooming.

265
00:13:25,740 --> 00:13:27,900
 So if you want to manipulate elephant--

266
00:13:27,900 --> 00:13:35,420
 did I spell elephant wrong?

267
00:13:35,420 --> 00:13:36,500
 OK, is there?

268
00:13:36,500 --> 00:13:38,100
 Or giraffe.

269
00:13:38,100 --> 00:13:39,900
 So it's really good for manipulating elephants

270
00:13:39,900 --> 00:13:41,600
 and giraffes or whatever, because they've

271
00:13:41,600 --> 00:13:45,820
 got lots of labeled data of elephants and giraffes

272
00:13:45,820 --> 00:13:48,580
 and lots of random categories.

273
00:13:48,580 --> 00:13:50,020
 But they don't have every category.

274
00:13:50,020 --> 00:13:51,060
 They've got some--

275
00:13:51,060 --> 00:13:53,020
 I forget-- 2,000 categories or something

276
00:13:53,020 --> 00:13:55,220
 like this of pretty good stuff.

277
00:13:55,220 --> 00:13:56,940
 Some of them that if you wanted--

278
00:13:56,940 --> 00:14:01,300
 maybe bears are less useful than umbrellas or shoes

279
00:14:01,300 --> 00:14:02,820
 for manipulation.

280
00:14:02,820 --> 00:14:06,940
 But although they did a very good job

281
00:14:06,940 --> 00:14:11,300
 of picking a diversity of categories that provided

282
00:14:11,300 --> 00:14:13,620
 a lot of interesting coverage, they

283
00:14:13,620 --> 00:14:17,020
 didn't put in all the objects that we have in our bin.

284
00:14:17,020 --> 00:14:21,140
 They just couldn't possibly have done the mustard bottle

285
00:14:21,140 --> 00:14:23,380
 we care about, the spam can we care about,

286
00:14:23,380 --> 00:14:25,660
 the Cheez-It box we care about.

287
00:14:25,660 --> 00:14:28,660
 So one of the first things that is absolutely

288
00:14:28,660 --> 00:14:30,500
 important to understand is that I

289
00:14:30,500 --> 00:14:33,580
 think one of the biggest results about all

290
00:14:33,580 --> 00:14:36,740
 of this deep learning for perception

291
00:14:36,740 --> 00:14:41,540
 is that these networks actually transfer incredibly well.

292
00:14:41,540 --> 00:14:45,860
 So if you train a network on this big labeled cocoa data

293
00:14:45,860 --> 00:14:49,660
 set and you want to apply it to your problem,

294
00:14:49,660 --> 00:14:52,100
 you don't have to regenerate a data

295
00:14:52,100 --> 00:14:55,020
 set that compares with the size or quality of labels

296
00:14:55,020 --> 00:14:56,260
 of the cocoa data set.

297
00:14:56,260 --> 00:14:59,620
 You can generate a very small data set

298
00:14:59,620 --> 00:15:03,420
 and transfer, fine tune a network that

299
00:15:03,420 --> 00:15:06,420
 was trained initially in cocoa and applied now

300
00:15:06,420 --> 00:15:08,340
 to your system.

301
00:15:08,340 --> 00:15:12,460
 So that's an important pipeline.

302
00:15:12,460 --> 00:15:16,420
 It's surprising that it works, but it's relatively simple

303
00:15:16,420 --> 00:15:17,860
 to understand.

304
00:15:17,860 --> 00:15:20,940
 So I've got a big multi-layer network.

305
00:15:20,940 --> 00:15:25,420
 It's got my original big ImageNet or cocoa trained model

306
00:15:25,420 --> 00:15:27,620
 that I download from the web because someone else spent

307
00:15:27,620 --> 00:15:33,380
 a lot of GPU cloud resources to train it from scratch.

308
00:15:33,380 --> 00:15:36,740
 And if I want to now apply it to the objects

309
00:15:36,740 --> 00:15:38,940
 I actually want to manipulate, not elephants,

310
00:15:38,940 --> 00:15:42,900
 but I want to have a mustard bottle detector,

311
00:15:42,900 --> 00:15:46,180
 then the approach is effectively to chop off the head.

312
00:15:46,180 --> 00:15:53,300
 You take the last layer or layers out of the network,

313
00:15:53,300 --> 00:15:56,620
 put a new one in of whatever number outputs

314
00:15:56,620 --> 00:16:00,340
 you care about with some random initialization,

315
00:16:00,340 --> 00:16:05,100
 but you keep all the initial layers of the network.

316
00:16:05,100 --> 00:16:06,660
 And then you just train.

317
00:16:06,660 --> 00:16:08,740
 You don't have to retrain on the original data set.

318
00:16:08,740 --> 00:16:10,660
 You just train now on your smaller data

319
00:16:10,660 --> 00:16:13,540
 set that you've generated that's narrow and curated

320
00:16:13,540 --> 00:16:15,660
 for your task.

321
00:16:15,660 --> 00:16:20,180
 And somehow, magically, the training

322
00:16:20,180 --> 00:16:22,260
 on the ImageNet and cocoa data sets

323
00:16:22,260 --> 00:16:28,060
 seem to capture enough general understanding about images,

324
00:16:28,060 --> 00:16:32,300
 about objects, that a relatively small amount of training

325
00:16:32,300 --> 00:16:36,860
 on the narrow data set seems to transfer.

326
00:16:36,860 --> 00:16:41,220
 I think it's one of the biggest success stories that

327
00:16:41,220 --> 00:16:44,020
 made all this stuff go.

328
00:16:44,020 --> 00:16:45,500
 It's very surprising in my mind.

329
00:16:45,500 --> 00:16:48,220
 And I think it took a data set as big as ImageNet

330
00:16:48,220 --> 00:16:53,980
 to start seeing that quality of generalization.

331
00:16:53,980 --> 00:16:58,660
 So I know a lot of you know this very well, but yeah,

332
00:16:58,660 --> 00:16:59,540
 please ask questions.

333
00:16:59,540 --> 00:17:02,260
 How do you determine whether or not just a good candidate

334
00:17:02,260 --> 00:17:04,700
 or not for this sort of a transfer?

335
00:17:04,700 --> 00:17:08,420
 So the question was--

336
00:17:08,420 --> 00:17:11,020
 so help me understand the question a little better.

337
00:17:11,020 --> 00:17:13,460
 So you're saying, how do you know if the task that you have

338
00:17:13,460 --> 00:17:16,700
 is a good candidate for transfer learning?

339
00:17:16,700 --> 00:17:19,900
 I mean, this is an active question,

340
00:17:19,900 --> 00:17:23,140
 is how far can you be from the original task

341
00:17:23,140 --> 00:17:26,940
 in order to still be successful when you transfer?

342
00:17:26,940 --> 00:17:28,780
 And we'll talk about--

343
00:17:28,780 --> 00:17:32,260
 so I think the big labeled ImageNet COCO data sets were

344
00:17:32,260 --> 00:17:34,340
 the first way that people did this.

345
00:17:34,340 --> 00:17:36,980
 Nowadays, people are trying to self-supervise and getting

346
00:17:36,980 --> 00:17:40,740
 broader initial networks from just snarping up the web.

347
00:17:40,740 --> 00:17:47,380
 And I think there's some intuition, I guess,

348
00:17:47,380 --> 00:17:51,620
 people have of that there's some distribution of the data that's

349
00:17:51,620 --> 00:17:53,060
 been applied here, and you've got

350
00:17:53,060 --> 00:17:54,620
 some different distribution.

351
00:17:54,620 --> 00:17:57,300
 And probably there's some similarity metric

352
00:17:57,300 --> 00:17:59,500
 that we don't understand completely of how far you

353
00:17:59,500 --> 00:18:02,100
 can go before that doesn't work.

354
00:18:02,100 --> 00:18:04,260
 But it's been surprisingly far.

355
00:18:04,260 --> 00:18:15,700
 So how do you generate that smaller amount of data?

356
00:18:15,700 --> 00:18:18,460
 Remember, I mentioned this early.

357
00:18:18,460 --> 00:18:22,140
 You can use iterative closest point, for instance,

358
00:18:22,140 --> 00:18:23,060
 to do this.

359
00:18:23,060 --> 00:18:27,100
 So when we first started trying to train deep perception

360
00:18:27,100 --> 00:18:29,460
 systems for the objects in our lab--

361
00:18:29,460 --> 00:18:33,060
 this is a bench in my lab with the--

362
00:18:33,060 --> 00:18:34,860
 it's kind of messy.

363
00:18:34,860 --> 00:18:37,180
 And there's a drill we wanted to pick up and manipulate.

364
00:18:37,180 --> 00:18:39,860
 And the drill wasn't in the COCO data set.

365
00:18:39,860 --> 00:18:40,980
 So what do you do?

366
00:18:40,980 --> 00:18:44,220
 You generate a bunch of depth data.

367
00:18:44,220 --> 00:18:46,180
 This is now departing from what the computer--

368
00:18:46,180 --> 00:18:49,540
 I mean, none of the big computer vision data sets have depth.

369
00:18:49,540 --> 00:18:50,820
 That's just starting to change.

370
00:18:50,820 --> 00:18:52,420
 People are releasing COCO-sized data

371
00:18:52,420 --> 00:18:56,140
 sets that have depth very recently.

372
00:18:56,140 --> 00:18:59,100
 So take my depth camera around.

373
00:18:59,100 --> 00:19:00,300
 Even if I'm going to train--

374
00:19:00,300 --> 00:19:03,340
 let me stop that because it's distracting here.

375
00:19:03,340 --> 00:19:07,420
 Even if I'm just going to train an RGB-based network,

376
00:19:07,420 --> 00:19:11,620
 if you have the luxury of depth, then what we can do

377
00:19:11,620 --> 00:19:14,460
 is we can do a dense reconstruction.

378
00:19:14,460 --> 00:19:15,700
 So we basically build a--

379
00:19:15,700 --> 00:19:18,380
 very quickly build an approximate CAD model

380
00:19:18,380 --> 00:19:23,540
 or a fused point cloud of all of my scans

381
00:19:23,540 --> 00:19:26,300
 so I can get the 3D view of it.

382
00:19:26,300 --> 00:19:29,860
 And then I can do ICP on that 3D view.

383
00:19:29,860 --> 00:19:33,220
 So I can find the drill in that point cloud.

384
00:19:33,220 --> 00:19:36,860
 And then I can go back in all of my images,

385
00:19:36,860 --> 00:19:39,260
 and just automatically provide a label

386
00:19:39,260 --> 00:19:42,140
 saying that these pixels, pixel by pixel,

387
00:19:42,140 --> 00:19:46,100
 that were the projected version of my ICP fit

388
00:19:46,100 --> 00:19:49,340
 are labeled to be the correct thing.

389
00:19:49,340 --> 00:19:50,540
 Did I say that well enough?

390
00:19:50,540 --> 00:19:54,180
 So by having a point cloud from all different sides,

391
00:19:54,180 --> 00:19:56,780
 you can make a nice point cloud, a nice point cloud which ICP

392
00:19:56,780 --> 00:19:58,940
 could work on.

393
00:19:58,940 --> 00:20:01,060
 Now I can re-render the image effectively

394
00:20:01,060 --> 00:20:05,260
 with just the green drill there labeled in all the pixels.

395
00:20:05,260 --> 00:20:06,660
 And I have a lot of training data

396
00:20:06,660 --> 00:20:10,140
 now that's just labeled drill or not drill.

397
00:20:10,140 --> 00:20:10,900
 Super effective.

398
00:20:10,900 --> 00:20:12,020
 I mean, there's a lot of--

399
00:20:12,020 --> 00:20:14,460
 I've seen lots of companies, lots of labs

400
00:20:14,460 --> 00:20:15,980
 have their own versions of that.

401
00:20:15,980 --> 00:20:19,260
 That was our version we called Label Fusion.

402
00:20:19,260 --> 00:20:20,820
 But that's a very effective pipeline.

403
00:20:20,820 --> 00:20:21,320
 Yeah?

404
00:20:21,320 --> 00:20:24,820
 Does that work as in like a generalized pipeline

405
00:20:24,820 --> 00:20:27,980
 drill for new rooms?

406
00:20:27,980 --> 00:20:31,020
 So that just leans on the generalization power

407
00:20:31,020 --> 00:20:34,100
 of the deep network, which has been impressive.

408
00:20:34,100 --> 00:20:36,340
 So I mean, I think you'd like to have

409
00:20:36,340 --> 00:20:38,820
 scenes with multiple rooms if you're

410
00:20:38,820 --> 00:20:42,420
 going to try to generalize in that way in your data set.

411
00:20:42,420 --> 00:20:45,140
 I mean, maybe roughly the idea is

412
00:20:45,140 --> 00:20:49,180
 that in distribution generalization

413
00:20:49,180 --> 00:20:49,980
 is extremely good.

414
00:20:49,980 --> 00:20:52,340
 If you try to ask for something it's never seen before,

415
00:20:52,340 --> 00:20:54,220
 then your mileage may vary.

416
00:20:54,220 --> 00:20:56,780
 So if I can get drills in a handful of rooms

417
00:20:56,780 --> 00:20:58,540
 and you put me in a new room, I would expect

418
00:20:58,540 --> 00:20:59,540
 it to work fairly well.

419
00:20:59,540 --> 00:21:00,040
 Yeah.

420
00:21:00,040 --> 00:21:04,820
 So my question was more, you generate 1,000 ways of vision

421
00:21:04,820 --> 00:21:08,220
 just by putting the drill in one part of the room.

422
00:21:08,220 --> 00:21:10,260
 And that sounds like very large data set.

423
00:21:10,260 --> 00:21:10,760
 I see.

424
00:21:10,760 --> 00:21:12,680
 But in some way it's small because you only

425
00:21:12,680 --> 00:21:14,600
 have to drill in one spot in one room.

426
00:21:14,600 --> 00:21:15,100
 OK.

427
00:21:15,100 --> 00:21:15,940
 That's a really good point.

428
00:21:15,940 --> 00:21:19,260
 So let me say it again for the camera.

429
00:21:19,260 --> 00:21:22,220
 So I say you can generate lots of data very fast.

430
00:21:22,220 --> 00:21:24,420
 But all of those images are very similar.

431
00:21:24,420 --> 00:21:27,140
 They don't cover a lot of different backgrounds,

432
00:21:27,140 --> 00:21:29,500
 basically, and there are lots of different scenarios.

433
00:21:29,500 --> 00:21:31,020
 So in some sense, even though there's

434
00:21:31,020 --> 00:21:32,980
 a large number of images, it represents

435
00:21:32,980 --> 00:21:38,540
 a small fraction of the scenes you want to care about.

436
00:21:38,540 --> 00:21:39,260
 I totally agree.

437
00:21:39,260 --> 00:21:42,860
 So the number there is artificially high

438
00:21:42,860 --> 00:21:44,020
 in terms of what you'd want.

439
00:21:44,020 --> 00:21:46,320
 In fact, you could probably down sample it dramatically

440
00:21:46,320 --> 00:21:48,060
 and do just as well.

441
00:21:48,060 --> 00:21:50,360
 So you still want to get some diverse scenes in order

442
00:21:50,360 --> 00:21:51,080
 to train.

443
00:21:51,080 --> 00:21:51,920
 Good point.

444
00:21:51,920 --> 00:22:01,240
 People use that a lot.

445
00:22:01,240 --> 00:22:05,000
 So the hope when we started that project

446
00:22:05,000 --> 00:22:08,920
 was that we could write strong enough global point cloud

447
00:22:08,920 --> 00:22:12,560
 registration algorithms, the global versions of ICP,

448
00:22:12,560 --> 00:22:14,880
 so that we could have it completely automated.

449
00:22:14,880 --> 00:22:18,760
 If I just let the robot walk around and do its collection,

450
00:22:18,760 --> 00:22:19,920
 it would build the model.

451
00:22:19,920 --> 00:22:23,280
 It would find the drill in the model, and we'd be good.

452
00:22:23,280 --> 00:22:26,680
 We'd be generating data like crazy.

453
00:22:26,680 --> 00:22:31,040
 OK, but you guys have now had some experience with ICP.

454
00:22:31,040 --> 00:22:33,320
 It's not that robust, unfortunately.

455
00:22:33,320 --> 00:22:36,280
 So in practice, in order to make that actually work,

456
00:22:36,280 --> 00:22:40,360
 we have a user interface where after we take these scans,

457
00:22:40,360 --> 00:22:43,800
 we have a human say, look here for the drill.

458
00:22:43,800 --> 00:22:46,760
 Three clicks to say roughly a pose,

459
00:22:46,760 --> 00:22:50,440
 and then ICP takes those very approximate rapid clicks

460
00:22:50,440 --> 00:22:52,900
 because we have the GUI so they can just go through all

461
00:22:52,900 --> 00:22:54,480
 your data set really fast.

462
00:22:54,480 --> 00:22:57,160
 It'll find the ICP, generate all the images.

463
00:22:57,160 --> 00:22:59,000
 So that's what worked for us at the time.

464
00:22:59,000 --> 00:23:01,160
 I still think there might be a future where

465
00:23:01,160 --> 00:23:05,520
 the global registration is good enough for that,

466
00:23:05,520 --> 00:23:07,480
 or the more global registration.

467
00:23:07,480 --> 00:23:10,680
 But that's where we were there.

468
00:23:10,680 --> 00:23:18,240
 So it's interesting, though, that even though that

469
00:23:18,240 --> 00:23:22,640
 is an effective pipeline and generates lots of data,

470
00:23:22,640 --> 00:23:27,040
 even those imperfections that of just the ICP

471
00:23:27,040 --> 00:23:29,720
 converge to something close but not quite or whatever,

472
00:23:29,720 --> 00:23:35,400
 those imperfections are a nuisance in your training.

473
00:23:35,400 --> 00:23:38,600
 If you contrast that with our other super powerful tool,

474
00:23:38,600 --> 00:23:43,080
 which is simulation, where we can make absolutely pristine,

475
00:23:43,080 --> 00:23:46,760
 perfect labels from our simulator,

476
00:23:46,760 --> 00:23:51,560
 the difference between errors in human labeled data

477
00:23:51,560 --> 00:23:56,600
 versus simulation perfect data has made this interesting trend

478
00:23:56,600 --> 00:24:00,680
 where you would think that their natural data would always

479
00:24:00,680 --> 00:24:02,480
 be better than simulated data.

480
00:24:02,480 --> 00:24:03,600
 But it's not true.

481
00:24:03,600 --> 00:24:07,120
 At some point, because the simulation quality is better,

482
00:24:07,120 --> 00:24:09,080
 the pixel by pixel quality is better,

483
00:24:09,080 --> 00:24:12,000
 even if the scenes are less diverse,

484
00:24:12,000 --> 00:24:14,600
 people have switched their pipeline slowly

485
00:24:14,600 --> 00:24:18,240
 from a purely real world data to a lot of people

486
00:24:18,240 --> 00:24:21,560
 are using purely simulated data to generate these training

487
00:24:21,560 --> 00:24:23,520
 sets now.

488
00:24:23,520 --> 00:24:27,840
 Or maybe most common still is mostly simulation data,

489
00:24:27,840 --> 00:24:29,800
 sprinkle in a little bit of real world data just

490
00:24:29,800 --> 00:24:32,560
 for good measure.

491
00:24:32,560 --> 00:24:37,520
 But I can tell you, it's annoying

492
00:24:37,520 --> 00:24:39,320
 to do the real world data.

493
00:24:39,320 --> 00:24:41,000
 The sim data works really, really well.

494
00:24:41,000 --> 00:24:42,680
 And in less and less often, I see

495
00:24:42,680 --> 00:24:44,760
 we're actually going to the real world data,

496
00:24:44,760 --> 00:24:47,240
 only if you really have to.

497
00:24:47,240 --> 00:24:49,400
 So what does the simulation pipeline look like?

498
00:24:49,400 --> 00:24:50,480
 So we already had it.

499
00:24:50,480 --> 00:24:55,040
 This is how it looks like in Drake.

500
00:24:55,040 --> 00:24:58,560
 If you have your RGB sensor that was kicking out the color

501
00:24:58,560 --> 00:25:00,320
 images that we were drawing like this,

502
00:25:00,320 --> 00:25:02,280
 it's kicking out the depth images.

503
00:25:02,280 --> 00:25:06,280
 It's also-- there's an output port for the label image.

504
00:25:06,280 --> 00:25:08,240
 So it spits out an image that's the same size

505
00:25:08,240 --> 00:25:10,280
 as what the camera was.

506
00:25:10,280 --> 00:25:15,320
 But every pixel is just assigned an integer value.

507
00:25:15,320 --> 00:25:17,640
 The integer value is mapped with a hash

508
00:25:17,640 --> 00:25:19,920
 to what type of object it is.

509
00:25:19,920 --> 00:25:22,680
 So this is an instant segmentation.

510
00:25:22,680 --> 00:25:25,480
 I changed-- if you were to just take the label image

511
00:25:25,480 --> 00:25:28,760
 and just say, plot the label image,

512
00:25:28,760 --> 00:25:30,520
 you would see something that looked black.

513
00:25:30,520 --> 00:25:34,400
 But I just-- because it uses like 0, and then 1, and then 2,

514
00:25:34,400 --> 00:25:36,800
 and then 3.

515
00:25:36,800 --> 00:25:40,200
 So I just scrambled those up into our favorite matplotlib

516
00:25:40,200 --> 00:25:41,600
 colors in order to make this plot.

517
00:25:41,600 --> 00:25:43,860
 But otherwise, it's almost exactly what you'd get out

518
00:25:43,860 --> 00:25:46,760
 of the label image port.

519
00:25:46,760 --> 00:25:50,120
 You get unique identifiers, pixel by pixel perfect,

520
00:25:50,120 --> 00:25:54,200
 based on the raycast in the renderer.

521
00:25:54,200 --> 00:25:54,700
 Yes?

522
00:25:54,700 --> 00:25:56,500
 So it doesn't have to be the normal object?

523
00:25:56,500 --> 00:25:59,460
 You don't have to get any of the [INAUDIBLE]??

524
00:25:59,460 --> 00:26:03,200
 So let's-- so the question is, is it only for known objects?

525
00:26:03,200 --> 00:26:06,000
 So I think it's helpful to think about this in the known object

526
00:26:06,000 --> 00:26:07,120
 case first.

527
00:26:07,120 --> 00:26:11,840
 Like in the-- the simulator knows the objects.

528
00:26:11,840 --> 00:26:17,040
 It's like, I've put in something from mustard.sdf, right?

529
00:26:17,040 --> 00:26:20,240
 And I can label it, this is mustard because of that.

530
00:26:20,240 --> 00:26:20,720
 OK?

531
00:26:20,720 --> 00:26:22,600
 So the simulator has no problem with that.

532
00:26:22,600 --> 00:26:25,200
 In order to have put the data in,

533
00:26:25,200 --> 00:26:28,960
 you have somehow a knowledge of what the objects are.

534
00:26:28,960 --> 00:26:31,560
 Whether you choose to use the known object

535
00:26:31,560 --> 00:26:34,720
 pipeline on the deep network side or not

536
00:26:34,720 --> 00:26:35,960
 is a second question.

537
00:26:35,960 --> 00:26:38,800
 But I think for the generating the labeled images,

538
00:26:38,800 --> 00:26:40,160
 that data is available.

539
00:26:40,160 --> 00:26:48,840
 OK, and this is really surprisingly potent.

540
00:26:48,840 --> 00:26:52,640
 So-- and I've seen--

541
00:26:52,640 --> 00:26:56,440
 when this trend started, it was because video games

542
00:26:56,440 --> 00:26:57,600
 got really good.

543
00:26:57,600 --> 00:27:02,440
 Flat out, Unreal Engine looks awesome, right?

544
00:27:02,440 --> 00:27:04,080
 Unreal Engine 4 looks even better.

545
00:27:04,080 --> 00:27:07,320
 Like, there's-- and all--

546
00:27:07,320 --> 00:27:08,320
 Unity looks amazing.

547
00:27:08,320 --> 00:27:10,920
 There's a bunch of-- there's like this--

548
00:27:10,920 --> 00:27:12,680
 it seems that most people roll their own.

549
00:27:12,680 --> 00:27:14,040
 Most of the game engines roll their own.

550
00:27:14,040 --> 00:27:15,280
 But they've all reached this--

551
00:27:15,280 --> 00:27:17,040
 somehow there's this, like, I don't know,

552
00:27:17,040 --> 00:27:18,920
 enough employees have shuffled back and forth or whatever.

553
00:27:18,920 --> 00:27:20,300
 There's like this common knowledge

554
00:27:20,300 --> 00:27:22,840
 where everybody's kicking out these incredible graphics

555
00:27:22,840 --> 00:27:23,440
 engines, right?

556
00:27:23,440 --> 00:27:25,920
 And they took us above some threshold

557
00:27:25,920 --> 00:27:31,760
 and making near photorealistic renders, OK?

558
00:27:31,760 --> 00:27:37,720
 But as people have leaned more and more on simulation,

559
00:27:37,720 --> 00:27:39,640
 where we first started off worrying

560
00:27:39,640 --> 00:27:41,560
 a lot about the render quality, like, I

561
00:27:41,560 --> 00:27:43,280
 will do whatever it takes to put the best

562
00:27:43,280 --> 00:27:45,080
 renderer on this as possible.

563
00:27:45,080 --> 00:27:46,880
 I've seen now increasingly people are like,

564
00:27:46,880 --> 00:27:49,360
 you know what, it actually kind of works fine with OpenGL.

565
00:27:49,360 --> 00:27:50,860
 You know, you don't even really need

566
00:27:50,860 --> 00:27:52,840
 to get the shadows perfectly right.

567
00:27:52,840 --> 00:27:54,400
 It still seems to transfer.

568
00:27:54,400 --> 00:27:57,920
 And it's been this interesting trend where the concern

569
00:27:57,920 --> 00:27:59,600
 about the render quality has gone down.

570
00:27:59,600 --> 00:28:00,560
 Now, if you really--

571
00:28:00,560 --> 00:28:02,100
 you will start seeing some artifacts.

572
00:28:02,100 --> 00:28:04,640
 So if you're really trying to build a high-end system,

573
00:28:04,640 --> 00:28:06,100
 you want your shadows to look right.

574
00:28:06,100 --> 00:28:07,160
 You want those details.

575
00:28:07,160 --> 00:28:08,280
 But you can get pretty far.

576
00:28:08,280 --> 00:28:10,320
 If you just want to pick some stuff up,

577
00:28:10,320 --> 00:28:11,400
 OpenGL is going to be fine.

578
00:28:11,400 --> 00:28:20,960
 OK, so I really don't want to talk too much

579
00:28:20,960 --> 00:28:22,580
 about architecture, but I want people

580
00:28:22,580 --> 00:28:26,520
 to understand just some of the very basics about why

581
00:28:26,520 --> 00:28:30,480
 I can ask questions like, how many objects are in the scene?

582
00:28:30,480 --> 00:28:33,160
 That's very distracting.

583
00:28:33,160 --> 00:28:35,760
 So let me just sort of step through the basic,

584
00:28:35,760 --> 00:28:39,320
 how do I go from an object recognition system,

585
00:28:39,320 --> 00:28:41,240
 you know, an image recognition system,

586
00:28:41,240 --> 00:28:43,200
 to my object detection system?

587
00:28:43,200 --> 00:28:48,680
 How do I go from image recognition

588
00:28:48,680 --> 00:28:51,040
 to object detection?

589
00:28:51,040 --> 00:28:54,560
 And the idea is, like, you saw it on that slide.

590
00:28:54,560 --> 00:28:58,440
 Basically, object detection, in its simplest form,

591
00:28:58,440 --> 00:29:01,480
 is image recognition, where I just try--

592
00:29:01,480 --> 00:29:05,360
 I move a sliding window across my image,

593
00:29:05,360 --> 00:29:12,120
 and I ask the image recognition question for each box.

594
00:29:12,120 --> 00:29:14,720
 If I just do image recognition on those boxes,

595
00:29:14,720 --> 00:29:16,600
 and I threshold and only keep the boxes that

596
00:29:16,600 --> 00:29:18,420
 have a high recognition score, then I

597
00:29:18,420 --> 00:29:22,440
 can imagine right there getting a box that's really--

598
00:29:22,440 --> 00:29:26,960
 that gives my object detection.

599
00:29:26,960 --> 00:29:28,580
 Now, that's not what people do anymore.

600
00:29:28,580 --> 00:29:31,200
 That's somehow some of the first things people do.

601
00:29:31,200 --> 00:29:34,000
 That's a poor representation of exactly everything

602
00:29:34,000 --> 00:29:37,640
 that's in RCNN, but that's the basic idea, right?

603
00:29:37,640 --> 00:29:40,920
 You have a lot of different images of different sizes

604
00:29:40,920 --> 00:29:42,480
 and different shapes, whatever.

605
00:29:42,480 --> 00:29:44,120
 For each one of them, you're basically

606
00:29:44,120 --> 00:29:47,580
 running that through a convolutional network

607
00:29:47,580 --> 00:29:51,560
 and asking the basic yes/no questions.

608
00:29:51,560 --> 00:29:52,680
 And that was how the first--

609
00:29:52,680 --> 00:29:59,640
 the mainstream detectors, object detection, started working.

610
00:29:59,640 --> 00:30:05,200
 Nowadays, people do more advanced versions of that,

611
00:30:05,200 --> 00:30:07,400
 but actually not crazy more advanced.

612
00:30:07,400 --> 00:30:10,560
 So nowadays, you don't just try--

613
00:30:10,560 --> 00:30:14,800
 so typically, it was expensive to try every possible region.

614
00:30:14,800 --> 00:30:17,640
 So we used some standard computer vision

615
00:30:17,640 --> 00:30:22,720
 from before 2014, ideas for what regions would

616
00:30:22,720 --> 00:30:24,080
 be good regions to evaluate.

617
00:30:24,080 --> 00:30:26,520
 So you could try to make it fast.

618
00:30:26,520 --> 00:30:30,600
 The fast RCNN was trying to down select some images.

619
00:30:30,600 --> 00:30:34,880
 And then the faster RCNN was this series of papers, right?

620
00:30:34,880 --> 00:30:36,680
 The faster RCNN said, you know what?

621
00:30:36,680 --> 00:30:39,840
 Let's use a neural network to predict where the regions are.

622
00:30:39,840 --> 00:30:41,880
 That's the region proposal network.

623
00:30:41,880 --> 00:30:47,080
 And then inside those regions, we'll do our image recognition.

624
00:30:47,080 --> 00:30:49,440
 And this is roughly the story how these things go,

625
00:30:49,440 --> 00:30:51,040
 is that we're like, oh, there's a little piece that

626
00:30:51,040 --> 00:30:51,800
 was hand-engineered there.

627
00:30:51,800 --> 00:30:53,720
 Let's replace that part with a neural network.

628
00:30:53,720 --> 00:30:58,200
 And at some point, we've got a completely deep end-to-end

629
00:30:58,200 --> 00:31:00,040
 thing.

630
00:31:00,040 --> 00:31:03,720
 The other thing that happened in this pipeline that

631
00:31:03,720 --> 00:31:07,380
 is important is that you have a region proposal network

632
00:31:07,380 --> 00:31:10,200
 or your initial guesses of where the regions are.

633
00:31:10,200 --> 00:31:13,960
 And typically, you will ask not only is there an object in here,

634
00:31:13,960 --> 00:31:17,240
 but if you were to do a small crop of that image,

635
00:31:17,240 --> 00:31:18,600
 then what would--

636
00:31:18,600 --> 00:31:20,800
 you try to predict also a crop, basically,

637
00:31:20,800 --> 00:31:23,760
 so that you get tighter images, tighter bounding boxes

638
00:31:23,760 --> 00:31:27,680
 by having the network also put out a crop value, basically,

639
00:31:27,680 --> 00:31:29,720
 the new bounding box regression.

640
00:31:29,720 --> 00:31:36,720
 So I mean, it is interesting to just stop and think

641
00:31:36,720 --> 00:31:40,180
 and appreciate, I guess, what's happening, right?

642
00:31:40,180 --> 00:31:43,160
 Is that, first of all, I remember--

643
00:31:43,160 --> 00:31:44,680
 I mean, I'm old, right?

644
00:31:44,680 --> 00:31:47,920
 So I remember when we were doing neural networks,

645
00:31:47,920 --> 00:31:51,920
 and it was like we were training the sheep, the dog, the cat

646
00:31:51,920 --> 00:31:52,440
 detector.

647
00:31:52,440 --> 00:31:54,360
 And it was kind of like, yeah, that works.

648
00:31:54,360 --> 00:31:56,040
 But I'm never going to train--

649
00:31:56,040 --> 00:31:57,800
 it doesn't really scale to have an output

650
00:31:57,800 --> 00:31:59,760
 vector that has 2,000 classes.

651
00:31:59,760 --> 00:32:01,560
 No one would ever do that.

652
00:32:01,560 --> 00:32:03,520
 People do that.

653
00:32:03,520 --> 00:32:04,760
 They do it all the time, right?

654
00:32:04,760 --> 00:32:06,400
 And then to think, oh, we're going

655
00:32:06,400 --> 00:32:08,920
 to apply a neural network to every single one of those boxes

656
00:32:08,920 --> 00:32:09,620
 is just--

657
00:32:09,620 --> 00:32:11,220
 yeah, that works too, right?

658
00:32:11,220 --> 00:32:14,620
 And yeah, sometimes the number of outputs

659
00:32:14,620 --> 00:32:16,460
 that people are predicting with these networks

660
00:32:16,460 --> 00:32:18,780
 is just mind boggling.

661
00:32:18,780 --> 00:32:20,940
 Like, I just would never have predicted

662
00:32:20,940 --> 00:32:24,020
 that we would want to do that.

663
00:32:24,020 --> 00:32:28,500
 But things have scaled up with GPUs, and TPUs, and the like

664
00:32:28,500 --> 00:32:30,240
 have really scaled beautifully.

665
00:32:30,240 --> 00:32:38,100
 I mean, even the fact that--

666
00:32:38,100 --> 00:32:40,300
 sorry, one other point I want to make there.

667
00:32:40,300 --> 00:32:42,240
 It used to be that we think, OK, well, we've

668
00:32:42,240 --> 00:32:45,140
 got just a fixed length vector coming out

669
00:32:45,140 --> 00:32:46,380
 of our neural network, right?

670
00:32:46,380 --> 00:32:48,340
 So the probability of it being a sheep,

671
00:32:48,340 --> 00:32:50,140
 the probability of it being a dog,

672
00:32:50,140 --> 00:32:54,340
 maybe the xy value of my new bounding box, OK?

673
00:32:54,340 --> 00:32:56,900
 But it's always like a fixed size vector in

674
00:32:56,900 --> 00:32:59,380
 and a fixed size vector out.

675
00:32:59,380 --> 00:33:03,020
 And so you ask, like, if I want to say I have some previously

676
00:33:03,020 --> 00:33:05,340
 unknown number of objects in my scene,

677
00:33:05,340 --> 00:33:09,940
 how do you architect the network to do that, right?

678
00:33:09,940 --> 00:33:11,820
 How do you have a variable number of outputs?

679
00:33:11,820 --> 00:33:14,420
 We have multiple answers to that these days.

680
00:33:14,420 --> 00:33:16,740
 But the initial way that that worked

681
00:33:16,740 --> 00:33:18,500
 was these region proposal networks,

682
00:33:18,500 --> 00:33:20,220
 or just any threshold of regions.

683
00:33:20,220 --> 00:33:21,540
 Was that you're going to evaluate the network many,

684
00:33:21,540 --> 00:33:22,980
 many times at threshold.

685
00:33:22,980 --> 00:33:25,940
 And that suddenly gave this reality to the idea

686
00:33:25,940 --> 00:33:29,620
 that you could have a variable number of object recognitions

687
00:33:29,620 --> 00:33:31,480
 or object detections in a single image.

688
00:33:34,340 --> 00:33:36,740
 OK, and you can sort of imagine that this would work, too,

689
00:33:36,740 --> 00:33:40,620
 if I had a neural network that had an image coming in,

690
00:33:40,620 --> 00:33:44,460
 and I had the pixel by pixel segmentation coming out,

691
00:33:44,460 --> 00:33:47,740
 whether it's semantic or instance level, OK?

692
00:33:47,740 --> 00:33:50,660
 You could imagine trying to learn that function, too.

693
00:33:50,660 --> 00:33:52,620
 And if you put all those together,

694
00:33:52,620 --> 00:33:57,540
 then you get mask R-CNN, which is a little bit old now,

695
00:33:57,540 --> 00:33:59,580
 but it's still extremely good.

696
00:33:59,580 --> 00:34:06,220
 There's a mask R-CNN 2, which is in Detectron 2,

697
00:34:06,220 --> 00:34:10,420
 but it's still incredibly good, OK?

698
00:34:10,420 --> 00:34:14,660
 And it's relatively simple, but I

699
00:34:14,660 --> 00:34:19,660
 did provide all of the example that you can open up and run

700
00:34:19,660 --> 00:34:23,380
 that does all of the data generation, the training,

701
00:34:23,380 --> 00:34:26,700
 and fine tuning of a mask R-CNN network,

702
00:34:26,700 --> 00:34:30,060
 so you can play around with it and see how it works

703
00:34:30,060 --> 00:34:31,900
 for our cluttered data set.

704
00:34:31,900 --> 00:34:35,580
 I knew that was going to happen.

705
00:34:35,580 --> 00:34:38,900
 But let me just show you kind of how that works.

706
00:34:38,900 --> 00:34:41,060
 If you were to grab this from the--

707
00:34:41,060 --> 00:34:45,500
 oh, good, it cached it.

708
00:34:45,500 --> 00:34:51,980
 And when you start doing this, those of you

709
00:34:51,980 --> 00:34:54,460
 that have spent time training deep models,

710
00:34:54,460 --> 00:34:56,860
 you know that basically 90% of the work

711
00:34:56,860 --> 00:35:00,860
 is like writing the data loader, OK?

712
00:35:00,860 --> 00:35:04,620
 So someone gave me an image, and it's in some sort of format,

713
00:35:04,620 --> 00:35:07,780
 and its values are either 0 to 55 or 0 to 1 or whatever.

714
00:35:07,780 --> 00:35:09,940
 And you just have to--

715
00:35:09,940 --> 00:35:11,860
 you spend a lot of time getting that right,

716
00:35:11,860 --> 00:35:14,860
 especially because if you get it wrong,

717
00:35:14,860 --> 00:35:16,380
 the network still learns something,

718
00:35:16,380 --> 00:35:17,820
 but it doesn't learn quite the right thing.

719
00:35:17,820 --> 00:35:19,900
 And it's just really hard to realize that you made

720
00:35:19,900 --> 00:35:21,900
 a mistake in the data loader.

721
00:35:21,900 --> 00:35:23,540
 So I did that for you.

722
00:35:23,540 --> 00:35:26,660
 You don't have to do that.

723
00:35:26,660 --> 00:35:28,060
 But you can play with it, right?

724
00:35:28,060 --> 00:35:37,380
 So the things that come out-- this is my training image.

725
00:35:37,380 --> 00:35:41,060
 This is my mask that comes out of MaskR-CNN,

726
00:35:41,060 --> 00:35:45,700
 where I just asked for the mustard mask, OK?

727
00:35:45,700 --> 00:35:48,180
 It found a mustard bottle in the image for what--

728
00:35:48,180 --> 00:35:50,660
 this is an instance-level detector, right?

729
00:35:50,660 --> 00:35:53,340
 So it found one of the instances.

730
00:35:53,340 --> 00:35:56,740
 I could change the indices that I've asked for,

731
00:35:56,740 --> 00:35:58,900
 and it'll show me the other mustard bottle, I think.

732
00:35:58,900 --> 00:36:06,540
 And then you can see all of the detections it found

733
00:36:06,540 --> 00:36:07,540
 in this image.

734
00:36:07,540 --> 00:36:09,780
 It flat out missed the big Cheez-It box.

735
00:36:09,780 --> 00:36:15,100
 It's the most easy, you would think, scene.

736
00:36:15,100 --> 00:36:18,180
 But admittedly, I trained this network hours

737
00:36:18,180 --> 00:36:21,220
 before lecture last year.

738
00:36:21,220 --> 00:36:26,580
 So probably it could have done better with a little bit more.

739
00:36:26,580 --> 00:36:29,540
 Yeah, but it did a pretty darn good job of finding the Jello.

740
00:36:29,540 --> 00:36:33,180
 And you can see it's got the gelatin box and the gelatin box

741
00:36:33,180 --> 00:36:33,820
 that SDF.

742
00:36:33,820 --> 00:36:39,620
 It's got these-- it's pretty incredible how well it worked.

743
00:36:39,620 --> 00:36:44,460
 I generated a reasonably big data set.

744
00:36:44,460 --> 00:36:46,780
 I think I could have generated a much smaller data set

745
00:36:46,780 --> 00:36:48,060
 and had it work well.

746
00:36:48,060 --> 00:36:52,620
 I just didn't want to have nothing for lecture.

747
00:36:52,620 --> 00:36:55,220
 OK, and you can go in there, and you can start playing with,

748
00:36:55,220 --> 00:36:56,580
 what are the region proposals?

749
00:36:56,580 --> 00:36:59,220
 I was actually surprised at how bad a lot of the region

750
00:36:59,220 --> 00:37:00,220
 proposals were.

751
00:37:00,220 --> 00:37:02,780
 I mean, it happened to get some good ones that, when cropped

752
00:37:02,780 --> 00:37:05,340
 down, did the job.

753
00:37:05,340 --> 00:37:07,220
 But if you only look at the final output,

754
00:37:07,220 --> 00:37:09,620
 then maybe you don't see all the messiness that's inside.

755
00:37:09,620 --> 00:37:12,060
 And it still does--

756
00:37:12,060 --> 00:37:14,540
 it considers pretty random proposals in the middle.

757
00:37:15,540 --> 00:37:19,540
 So that's a pipeline that, like I said,

758
00:37:19,540 --> 00:37:22,140
 if we just wanted to use our antipodal grasping,

759
00:37:22,140 --> 00:37:23,260
 that's going to do the job.

760
00:37:23,260 --> 00:37:26,020
 It said, if I want to, say, pick up a mustard bottle,

761
00:37:26,020 --> 00:37:28,700
 I could actually iterate through all the mustard bottles.

762
00:37:28,700 --> 00:37:35,020
 But I can just say, these pixels from that RGB camera

763
00:37:35,020 --> 00:37:37,700
 are mustard, are a mustard bottle.

764
00:37:37,700 --> 00:37:39,500
 If I project that back onto the Jello,

765
00:37:39,500 --> 00:37:40,740
 I can actually see the Jello.

766
00:37:40,740 --> 00:37:41,780
 And I can see the Jello.

767
00:37:41,780 --> 00:37:43,140
 And I can see the mustard bottle.

768
00:37:43,140 --> 00:37:45,700
 If I project that back onto the point cloud,

769
00:37:45,700 --> 00:37:48,020
 I could use that as my segmentation for my point cloud.

770
00:37:48,020 --> 00:37:49,940
 So just remove all of the points that

771
00:37:49,940 --> 00:37:53,140
 aren't associated with those pixels.

772
00:37:53,140 --> 00:37:56,060
 And then I could find the antipodal grasp on that.

773
00:37:56,060 --> 00:37:58,360
 And I picked mustard bottles instead of random things.

774
00:37:58,360 --> 00:38:03,380
 It's very cool.

775
00:38:03,380 --> 00:38:05,300
 So I hope you play with it.

776
00:38:05,300 --> 00:38:08,300
 If you're not familiar, then you can see the whole pipeline.

777
00:38:08,300 --> 00:38:10,880
 There's a different notebook for generating the training data,

778
00:38:10,880 --> 00:38:17,540
 and one for training, and then one for testing.

779
00:38:17,540 --> 00:38:19,580
 So the training takes hours, many hours.

780
00:38:19,580 --> 00:38:28,220
 I wrote it so that basically, if you walked away

781
00:38:28,220 --> 00:38:30,780
 and Google Colab shut down, just before it shut down,

782
00:38:30,780 --> 00:38:33,180
 it tried to save the file to your laptop.

783
00:38:33,180 --> 00:38:35,960
 And there might be some lessons there

784
00:38:35,960 --> 00:38:38,740
 if you're trying to use Colab in the future, too.

785
00:38:39,740 --> 00:38:41,280
 Any questions on that before we go

786
00:38:41,280 --> 00:38:43,880
 into some of the more manipulation-specific

787
00:38:43,880 --> 00:38:44,840
 representations?

788
00:38:44,840 --> 00:38:51,280
 It's a super powerful pipeline.

789
00:38:51,280 --> 00:38:52,000
 It's really good.

790
00:38:52,000 --> 00:38:59,240
 OK.

791
00:38:59,240 --> 00:39:03,240
 So let's just think about the fact

792
00:39:03,240 --> 00:39:07,240
 that ICP doesn't work globally.

793
00:39:07,240 --> 00:39:15,000
 And we even said back then that it's

794
00:39:15,000 --> 00:39:18,760
 hard to beat the geometric perception for really

795
00:39:18,760 --> 00:39:21,280
 fine estimation of the pose.

796
00:39:21,280 --> 00:39:22,820
 But in terms of going from an image

797
00:39:22,820 --> 00:39:24,820
 and having an initial guess, something

798
00:39:24,820 --> 00:39:28,360
 sort of in the spirit of just trying to find

799
00:39:28,360 --> 00:39:31,620
 the needle in the haystack, I think deep learning pose

800
00:39:31,620 --> 00:39:37,780
 estimation is a very nice approach to that.

801
00:39:37,780 --> 00:39:42,300
 Caveat-- this is deep perception part one.

802
00:39:42,300 --> 00:39:44,460
 Deep perception part two, I'm roughly going to say,

803
00:39:44,460 --> 00:39:46,220
 don't do pose estimation.

804
00:39:46,220 --> 00:39:49,500
 Because I think once you have a deep network at work,

805
00:39:49,500 --> 00:39:51,380
 you can ask richer questions that

806
00:39:51,380 --> 00:39:55,260
 are less specific to single object representations.

807
00:39:55,260 --> 00:39:58,180
 There's more interesting object representations.

808
00:39:58,180 --> 00:40:01,100
 But I think it's very helpful to take the stuff we've already

809
00:40:01,100 --> 00:40:05,500
 seen and try to map it into the deep learning context.

810
00:40:05,500 --> 00:40:07,260
 And it's the beginning of what we'll see--

811
00:40:07,260 --> 00:40:08,740
 I guess I can just erase that.

812
00:40:08,740 --> 00:40:10,280
 It's the beginning of what we'll see

813
00:40:10,280 --> 00:40:14,020
 as sort of a nice marriage between the ideas from geometry

814
00:40:14,020 --> 00:40:15,900
 and the ideas from deep learning.

815
00:40:23,780 --> 00:40:31,100
 So I think the big question is, if I do send images in--

816
00:40:31,100 --> 00:40:35,460
 I guess I could have just left what I had there.

817
00:40:35,460 --> 00:40:36,900
 And I have poses coming out.

818
00:40:36,900 --> 00:40:42,500
 I want you to think about this first as one choice

819
00:40:42,500 --> 00:40:44,380
 for object representation.

820
00:40:44,380 --> 00:40:44,880
 OK.

821
00:40:44,880 --> 00:40:58,780
 So this is sort of our--

822
00:40:58,780 --> 00:41:07,860
 we've got a mesh model plus a pose as our representation.

823
00:41:07,860 --> 00:41:10,620
 So implicitly, in order to generate the training data

824
00:41:10,620 --> 00:41:12,460
 or whatever, I had a model, which I generated

825
00:41:12,460 --> 00:41:14,100
 a bunch of simulated images.

826
00:41:14,100 --> 00:41:17,100
 My mustard bottle, that SDF, right?

827
00:41:17,100 --> 00:41:21,300
 And somehow, that model, once I have it,

828
00:41:21,300 --> 00:41:23,340
 if I can just say what the pose is,

829
00:41:23,340 --> 00:41:25,300
 then this will tell me everything

830
00:41:25,300 --> 00:41:26,780
 I need to do to start manipulating

831
00:41:26,780 --> 00:41:28,300
 that object in the world.

832
00:41:28,300 --> 00:41:30,980
 We're going to look at other alternatives here,

833
00:41:30,980 --> 00:41:33,580
 and even slightly richer versions of this.

834
00:41:33,580 --> 00:41:36,100
 We'll talk about learning sign distance functions,

835
00:41:36,100 --> 00:41:38,580
 or some of you think about learning

836
00:41:38,580 --> 00:41:42,060
 NERF or other kind of representations

837
00:41:42,060 --> 00:41:44,460
 that maybe don't require the mesh model.

838
00:41:44,460 --> 00:41:46,820
 But the one that snaps right in and we

839
00:41:46,820 --> 00:41:49,540
 can compare to our understanding of geometric perception

840
00:41:49,540 --> 00:41:53,260
 is if we try to predict the poses.

841
00:41:53,260 --> 00:41:55,340
 So the question I want to ask you here

842
00:41:55,340 --> 00:41:57,340
 is, we talked about all the different ways

843
00:41:57,340 --> 00:41:58,820
 to represent pose.

844
00:41:58,820 --> 00:42:03,460
 How do I represent-- which representation should I use?

845
00:42:03,460 --> 00:42:08,580
 OK.

846
00:42:08,580 --> 00:42:12,540
 So I mean, having an x, y, z value,

847
00:42:12,540 --> 00:42:14,700
 that's totally not a big deal.

848
00:42:14,700 --> 00:42:15,380
 That's fine.

849
00:42:15,380 --> 00:42:16,940
 I don't think there's a better choice.

850
00:42:16,940 --> 00:42:18,660
 But what about for orientation?

851
00:42:18,660 --> 00:42:28,980
 What do you think would be a good orientation

852
00:42:28,980 --> 00:42:30,940
 representation for a deep network to kick out?

853
00:42:30,940 --> 00:42:31,440
 Yeah.

854
00:42:31,440 --> 00:42:32,900
 [INAUDIBLE]

855
00:42:32,900 --> 00:42:33,400
 OK.

856
00:42:33,520 --> 00:42:34,020
 So.

857
00:42:34,020 --> 00:42:42,440
 So that's, I think, a lot of-- well,

858
00:42:42,440 --> 00:42:44,860
 that's not the first one that I saw a lot of papers about,

859
00:42:44,860 --> 00:42:47,200
 but that's definitely something that people try.

860
00:42:47,200 --> 00:42:51,640
 And it doesn't work well at all, because there's

861
00:42:51,640 --> 00:42:54,320
 singularities in that transform.

862
00:42:54,320 --> 00:42:57,200
 And so you tend to--

863
00:42:57,200 --> 00:42:59,120
 I think the networks actually have trouble

864
00:42:59,120 --> 00:43:01,200
 learning real pitchy off.

865
00:43:01,200 --> 00:43:03,520
 But that's a great starting point.

866
00:43:03,520 --> 00:43:04,040
 What else?

867
00:43:04,040 --> 00:43:04,540
 Yeah.

868
00:43:04,540 --> 00:43:05,520
 [INAUDIBLE]

869
00:43:05,520 --> 00:43:06,020
 OK.

870
00:43:06,020 --> 00:43:06,640
 Quaternions.

871
00:43:06,640 --> 00:43:17,600
 So if the quaternions are coming out,

872
00:43:17,600 --> 00:43:19,640
 and we just think of them as a 4 by 1,

873
00:43:19,640 --> 00:43:24,760
 if you think about it as just as an element of R4,

874
00:43:24,760 --> 00:43:27,240
 I have four real valued outputs.

875
00:43:27,240 --> 00:43:28,200
 That's part of it.

876
00:43:28,200 --> 00:43:31,680
 But you also need to make it a unit quaternion, right?

877
00:43:31,680 --> 00:43:45,200
 So plus some normalization, which people would typically

878
00:43:45,200 --> 00:43:48,960
 do now by having a last layer of the network that

879
00:43:48,960 --> 00:43:51,280
 does the normalization.

880
00:43:51,280 --> 00:43:52,680
 And you can just take derivatives

881
00:43:52,680 --> 00:43:55,440
 through that normalization function, right?

882
00:43:55,440 --> 00:43:59,920
 But you need to include that to have unit quaternions.

883
00:43:59,920 --> 00:44:02,400
 So that's a good proposal.

884
00:44:02,400 --> 00:44:07,120
 But actually, we've seen people understanding more deeply now

885
00:44:07,120 --> 00:44:11,800
 that that can have trouble for neural networks too.

886
00:44:11,800 --> 00:44:14,880
 It's a little bit of folklore.

887
00:44:14,880 --> 00:44:21,900
 But I think the progress in learning 3D vision,

888
00:44:21,900 --> 00:44:24,360
 I think that people would say that more

889
00:44:24,360 --> 00:44:26,240
 continuous representations, that this actually

890
00:44:26,240 --> 00:44:27,240
 can have some discontinuities.

891
00:44:27,240 --> 00:44:28,440
 We'll talk about it.

892
00:44:28,440 --> 00:44:31,560
 And that there are better representations still.

893
00:44:31,560 --> 00:44:32,060
 Yeah?

894
00:44:32,060 --> 00:44:33,440
 AUDIENCE: Axis angle.

895
00:44:33,440 --> 00:44:34,520
 PROFESSOR: Axis angle.

896
00:44:34,520 --> 00:44:40,640
 So the axis angle is going to be very similar to the unit

897
00:44:40,640 --> 00:44:41,680
 quaternions.

898
00:44:41,680 --> 00:44:44,840
 But it doesn't need the normalization.

899
00:44:44,840 --> 00:44:47,720
 But it turns out it's going to be plagued by the same stuff.

900
00:44:47,720 --> 00:44:49,240
 I mean, axis angles and quaternions

901
00:44:49,240 --> 00:44:50,600
 are very, very closely related.

902
00:44:50,600 --> 00:44:51,100
 Yeah?

903
00:44:51,100 --> 00:44:57,760
 So let's try to understand--

904
00:44:57,760 --> 00:44:59,480
 I mean, the other ones we've talked about,

905
00:44:59,480 --> 00:45:02,160
 we've talked about the rotation matrix as an output.

906
00:45:02,160 --> 00:45:06,080
 But again, you need to somehow make sure you get a real rotation

907
00:45:06,080 --> 00:45:07,320
 matrix out.

908
00:45:07,320 --> 00:45:08,560
 So did you want to recommend?

909
00:45:08,560 --> 00:45:09,060
 Yeah?

910
00:45:09,060 --> 00:45:10,000
 AUDIENCE: [INAUDIBLE]

911
00:45:10,000 --> 00:45:18,920
 PROFESSOR: OK, so the proposal is x, y, z of corners of a cube.

912
00:45:18,920 --> 00:45:22,640
 So that's not a crazy idea.

913
00:45:22,640 --> 00:45:24,240
 OK, so let me understand what you said.

914
00:45:24,240 --> 00:45:30,160
 So to some extent, if I'm trying to learn a pose,

915
00:45:30,160 --> 00:45:34,760
 if I've learned this x, y, z, and you're saying basically

916
00:45:34,760 --> 00:45:46,920
 also learn this x1, y1, z1, maybe this, and all three,

917
00:45:46,920 --> 00:45:52,800
 something like that, you could think about how minimal you

918
00:45:52,800 --> 00:45:57,640
 could make it and just learn all of those as 3D points,

919
00:45:57,640 --> 00:46:01,000
 let's say, or 3D vectors, rather,

920
00:46:01,000 --> 00:46:02,760
 and then try to reconstruct.

921
00:46:02,760 --> 00:46:04,480
 People have had some success with that.

922
00:46:04,480 --> 00:46:12,840
 I think I would call that sort of a key point based version.

923
00:46:12,840 --> 00:46:16,200
 We'll talk a lot about key points on Thursday.

924
00:46:16,200 --> 00:46:19,240
 But that's a reasonable representation, too.

925
00:46:19,240 --> 00:46:21,400
 You have to make sure you still have

926
00:46:21,400 --> 00:46:24,000
 to enforce something about the scaling or whatever

927
00:46:24,000 --> 00:46:25,120
 when you reconstruct.

928
00:46:25,120 --> 00:46:29,840
 But you could imagine trying to fit the closest

929
00:46:29,840 --> 00:46:31,640
 rotation to those points.

930
00:46:31,640 --> 00:46:36,960
 You could just try to do the rotation matrix directly,

931
00:46:36,960 --> 00:46:38,840
 which actually is almost that.

932
00:46:38,840 --> 00:46:46,160
 I mean, if you're trying to fit a rotation matrix

933
00:46:46,160 --> 00:46:51,640
 as a 3 by 3 matrix, that's basically

934
00:46:51,640 --> 00:46:56,880
 parameterizing the x, y, z vectors of the transform, which

935
00:46:56,880 --> 00:46:58,560
 is really basically doing that.

936
00:46:58,560 --> 00:47:03,360
 But you need to-- so the same way

937
00:47:03,360 --> 00:47:05,160
 that you'd need to be careful on this,

938
00:47:05,160 --> 00:47:08,280
 you would have to somehow add the rotation matrix

939
00:47:08,280 --> 00:47:08,780
 constraint.

940
00:47:08,780 --> 00:47:15,060
 So R R transpose equals I and determine it.

941
00:47:15,060 --> 00:47:33,820
 So there's a series of work that has been arguing that somehow--

942
00:47:33,820 --> 00:47:36,980
 again, it's not a super watertight argument.

943
00:47:36,980 --> 00:47:40,060
 But the argument roughly goes like this.

944
00:47:40,060 --> 00:47:43,900
 Deep networks learn continuous functions.

945
00:47:43,900 --> 00:47:46,420
 You'd like the function that you're trying to learn

946
00:47:46,420 --> 00:47:47,300
 to be continuous.

947
00:47:47,300 --> 00:47:49,340
 And all things considered, a function

948
00:47:49,340 --> 00:47:52,500
 that is a continuous function, as I change angles,

949
00:47:52,500 --> 00:47:54,900
 for instance, if I change poses of my object,

950
00:47:54,900 --> 00:47:57,100
 the output changes continuously, is

951
00:47:57,100 --> 00:47:59,580
 going to be better for the neural network to learn.

952
00:47:59,580 --> 00:48:02,380
 There's a series of papers about on the continuity of rotation

953
00:48:02,380 --> 00:48:05,340
 representations for deep learning, which

954
00:48:05,340 --> 00:48:06,780
 I will cite properly in the notes.

955
00:48:06,780 --> 00:48:17,940
 But try to learn more continuous representations.

956
00:48:17,940 --> 00:48:21,140
 And the discontinuities in our representations

957
00:48:21,140 --> 00:48:22,300
 are a little sneaky.

958
00:48:22,300 --> 00:48:24,380
 You have to kind of think about it.

959
00:48:24,380 --> 00:48:29,220
 So if you think about in 2D, imagine

960
00:48:29,220 --> 00:48:31,900
 I just wanted to use theta, for instance.

961
00:48:31,900 --> 00:48:35,460
 So if I just want to parameterize

962
00:48:35,460 --> 00:48:45,940
 the total angle of my pose, maybe theta's in 0 to 2 pi.

963
00:48:45,940 --> 00:48:48,700
 I'd like to somehow have the output of my network

964
00:48:48,700 --> 00:48:51,420
 be between 0 and 2 pi.

965
00:48:51,420 --> 00:48:53,820
 Well, then there's two rotations.

966
00:48:53,820 --> 00:49:04,580
 The rotations at epsilon and 2 pi minus epsilon

967
00:49:04,580 --> 00:49:05,700
 should be close.

968
00:49:05,700 --> 00:49:07,980
 The wrapping gives you a discontinuity.

969
00:49:07,980 --> 00:49:12,340
 Those should be close, but aren't in this representation.

970
00:49:12,340 --> 00:49:18,180
 You say, OK, well, maybe I just do 0 to 4 pi

971
00:49:18,180 --> 00:49:19,140
 or something like that.

972
00:49:19,140 --> 00:49:20,780
 But what is your training data?

973
00:49:20,780 --> 00:49:22,580
 When you've got your training data,

974
00:49:22,580 --> 00:49:25,820
 and you say this object is at this orientation,

975
00:49:25,820 --> 00:49:26,820
 you've got to pick.

976
00:49:26,820 --> 00:49:29,700
 You've got to somehow pick your unwrapping.

977
00:49:29,700 --> 00:49:33,180
 And for whatever function you've tried to pick,

978
00:49:33,180 --> 00:49:36,460
 you somehow have picked a domain for this.

979
00:49:36,460 --> 00:49:39,380
 And it's not easy to unwrap perfectly.

980
00:49:39,380 --> 00:49:43,820
 Well, there are things you can do to repair these problems.

981
00:49:43,820 --> 00:49:45,740
 But this is the fundamental problem,

982
00:49:45,740 --> 00:49:49,300
 is that you think that's good, right?

983
00:49:49,300 --> 00:49:52,340
 Because the rotation matrix has a function of theta.

984
00:49:52,340 --> 00:49:55,900
 And 2D is this sort of smooth function.

985
00:49:55,900 --> 00:50:07,740
 I mean, going from theta to the rotation, the full rotation

986
00:50:07,740 --> 00:50:08,620
 matrix, that's good.

987
00:50:08,620 --> 00:50:11,660
 But going backwards involves some unwrapping,

988
00:50:11,660 --> 00:50:13,820
 some decision.

989
00:50:13,820 --> 00:50:17,340
 So the argument is roughly, the network

990
00:50:17,340 --> 00:50:23,540
 needs you to be unambiguous in your choice of orientation.

991
00:50:23,540 --> 00:50:25,060
 And that's kind of fundamentally what

992
00:50:25,060 --> 00:50:29,020
 happens to quaternions or Euler angles and other things, too.

993
00:50:29,020 --> 00:50:35,740
 Even quaternions are ambiguous because negative

994
00:50:35,740 --> 00:50:37,460
 of the quaternion is the same rotation

995
00:50:37,460 --> 00:50:39,380
 as the positive of the quaternion.

996
00:50:39,380 --> 00:50:50,060
 So if I have for quaternions, I have a quaternion q

997
00:50:50,060 --> 00:50:55,700
 and quaternion negative q are the same rotation.

998
00:50:55,700 --> 00:51:01,660
 It was interesting to sort of see,

999
00:51:01,660 --> 00:51:04,140
 if you go to the website of the paper

1000
00:51:04,140 --> 00:51:07,340
 on this continuity of learning, you

1001
00:51:07,340 --> 00:51:11,580
 can see that they clearly got attacked.

1002
00:51:11,580 --> 00:51:13,900
 Like, what do you mean quaternions aren't smooth

1003
00:51:13,900 --> 00:51:14,420
 or whatever?

1004
00:51:14,420 --> 00:51:15,900
 And they have this, like, fact.

1005
00:51:15,900 --> 00:51:19,260
 This is what we meant by quaternions not being smooth.

1006
00:51:19,260 --> 00:51:21,060
 But at some point, let's say you could just

1007
00:51:21,060 --> 00:51:24,780
 say your training data had to pick a q or a minus q,

1008
00:51:24,780 --> 00:51:25,820
 for instance.

1009
00:51:25,820 --> 00:51:28,620
 And they have some theorems in the follow-up paper,

1010
00:51:28,620 --> 00:51:33,580
 for instance, saying that basically you cannot pick

1011
00:51:33,580 --> 00:51:37,180
 a transformation from the rotation matrices to q that

1012
00:51:37,180 --> 00:51:39,660
 doesn't have a singularity.

1013
00:51:39,660 --> 00:51:41,060
 They always have a singularity.

1014
00:51:41,060 --> 00:51:45,060
 In fact, there's a topological argument

1015
00:51:45,060 --> 00:51:48,860
 that basically you can't map the rotation

1016
00:51:48,860 --> 00:51:54,180
 matrices into a representation in four numbers that

1017
00:51:54,180 --> 00:51:57,780
 is absolutely no discontinuities.

1018
00:51:57,780 --> 00:52:03,100
 So they have a series of proposals for ways to do that,

1019
00:52:03,100 --> 00:52:04,500
 ways to get around it.

1020
00:52:04,500 --> 00:52:08,540
 So one of them-- it's actually--

1021
00:52:08,540 --> 00:52:10,380
 it wasn't what I expected.

1022
00:52:10,380 --> 00:52:11,460
 They call it an ensemble.

1023
00:52:11,460 --> 00:52:15,780
 But they basically wrote four different functions

1024
00:52:15,780 --> 00:52:20,380
 that are all choices of q or q inverse differently.

1025
00:52:20,380 --> 00:52:23,020
 So you could basically write four different rotation matrix

1026
00:52:23,020 --> 00:52:25,300
 to quaternion maps.

1027
00:52:25,300 --> 00:52:27,340
 And you could, like, combine all of their outputs.

1028
00:52:27,340 --> 00:52:29,140
 And you can get a singularity-free output.

1029
00:52:29,140 --> 00:52:31,460
 That's one way to do it.

1030
00:52:31,460 --> 00:52:41,620
 So you can repair this with more dimensions.

1031
00:52:41,620 --> 00:52:52,300
 In fact, one of them would be, I think,

1032
00:52:52,300 --> 00:52:56,140
 a fairly reasonable one that people use

1033
00:52:56,140 --> 00:52:58,220
 would be to actually parameterize the rotation

1034
00:52:58,220 --> 00:53:02,100
 matrix in 3 by 3.

1035
00:53:02,100 --> 00:53:05,700
 And then the same way we added a normalization,

1036
00:53:05,700 --> 00:53:07,740
 add basically the singular value decomposition

1037
00:53:07,740 --> 00:53:08,900
 as the last layer.

1038
00:53:08,900 --> 00:53:12,700
 So you're projecting back to the closest rotation matrix.

1039
00:53:12,700 --> 00:53:38,500
 And that's something that seems to work fairly well, too.

1040
00:53:38,500 --> 00:53:39,500
 Yes.

1041
00:53:39,500 --> 00:53:40,500
 So for the last thing that you wrote there, I get how having-- like, the fact that the

1042
00:53:40,500 --> 00:53:41,500
 quaternions are not unique, that's like a challenge in terms of trying to make these

1043
00:53:41,500 --> 00:53:42,500
 work.

1044
00:53:42,500 --> 00:53:43,500
 And I get how adding more dimensions can help you get around, like, the times that you

1045
00:53:43,500 --> 00:53:44,500
 need to be normalized.

1046
00:53:44,500 --> 00:53:45,500
 But I feel like as you add more dimensions, wouldn't it become more redundant?

1047
00:53:45,500 --> 00:53:46,500
 Like, wouldn't there be more, like, it's not necessarily a question of, well, what

1048
00:53:46,500 --> 00:53:47,500
 does this mean?

1049
00:53:47,500 --> 00:53:48,500
 But I guess I'm not sure that that's the answer.

1050
00:53:48,500 --> 00:53:49,500
 So I guess I'm not sure that's the answer.

1051
00:53:49,500 --> 00:53:50,500
 But I guess I'm not sure that that's the answer.

1052
00:53:50,500 --> 00:53:51,500
 So I guess I'm not sure that's the answer.

1053
00:53:51,500 --> 00:53:52,500
 So I guess I'm not sure that's the answer.

1054
00:53:52,500 --> 00:53:53,500
 So I guess I'm not sure that's the answer.

1055
00:53:53,500 --> 00:54:12,220
 So I guess I'm not sure that's the answer.

1056
00:54:12,220 --> 00:54:16,060
 So the question is, if you add more dimensions, are you adding more redundancy?

1057
00:54:16,060 --> 00:54:19,980
 I mean, that's not always true.

1058
00:54:19,980 --> 00:54:23,700
 But I like the question.

1059
00:54:23,700 --> 00:54:34,460
 So I can answer it in 2D.

1060
00:54:34,460 --> 00:54:35,460
 4D is hard.

1061
00:54:35,460 --> 00:54:40,420
 Quaternions are hard to think about.

1062
00:54:40,420 --> 00:54:53,300
 So let's contrast a rotation, which is theta, versus the polar coordinates.

1063
00:54:53,300 --> 00:55:00,340
 If I were to instead do, let's say-- well, I could do r-- let's just even do cosine theta

1064
00:55:00,340 --> 00:55:06,020
 and sine theta, which is actually my first two elements of my rotation matrix, which

1065
00:55:06,020 --> 00:55:10,260
 I-- if I just think about it basically parametrizing

1066
00:55:10,260 --> 00:55:11,260
 those.

1067
00:55:11,260 --> 00:55:19,260
 If I were to ask the network to learn cosine theta and sine theta, then for every rotation,

1068
00:55:19,260 --> 00:55:23,780
 there is a unique answer in two numbers.

1069
00:55:23,780 --> 00:55:27,780
 And maybe I have to project to make sure cosine squared plus sine squared equals 1 at the

1070
00:55:27,780 --> 00:55:28,940
 end.

1071
00:55:28,940 --> 00:55:31,020
 And I guess that constraint is what's saving us.

1072
00:55:31,020 --> 00:55:33,860
 Maybe that's in the counting game that you're trying.

1073
00:55:33,860 --> 00:55:36,860
 I think it's the constraint that would then save us.

1074
00:55:36,860 --> 00:55:40,460
 Maybe that's the simplest answer.

1075
00:55:40,460 --> 00:55:43,660
 But this is an example where I think you'd prefer, even though it's more dimensions,

1076
00:55:43,660 --> 00:55:47,380
 you'd prefer to ask it to learn something like cosine theta, sine theta, because you

1077
00:55:47,380 --> 00:55:49,580
 can give it a unique answer.

1078
00:55:49,580 --> 00:55:55,460
 That's a great, great question.

1079
00:55:55,460 --> 00:56:01,340
 The representations in 5 and 6D, I don't have a picture in my head.

1080
00:56:01,340 --> 00:56:07,540
 One could, but I haven't studied it enough to know.

1081
00:56:07,540 --> 00:56:11,840
 Because even in 4D, there's a unit quaternion constraint to keep it-- so there's really

1082
00:56:11,840 --> 00:56:20,340
 natively three variables.

1083
00:56:20,340 --> 00:56:25,540
 But there's still actually-- so we've listed a lot of good ones here from the key points

1084
00:56:25,540 --> 00:56:27,940
 and the rotation matrices and all these quaternions.

1085
00:56:27,940 --> 00:56:35,020
 And then I think the rotation matrices plus SVD is actually a good choice.

1086
00:56:35,020 --> 00:56:44,140
 It doesn't completely-- so even this picture, it resolves the ambiguity if my object is

1087
00:56:44,140 --> 00:56:49,740
 rotationally unique or something, if it doesn't have any rotational symmetries.

1088
00:56:49,740 --> 00:56:55,100
 But in practice, there's all sorts of ambiguity in rotations.

1089
00:56:55,100 --> 00:57:00,220
 So there are shapes that are perfectly symmetric around rotations.

1090
00:57:00,220 --> 00:57:03,580
 There are things that are just ambiguous because of partial views.

1091
00:57:03,580 --> 00:57:11,300
 So I brought some pictures from the kitchen sink.

1092
00:57:11,300 --> 00:57:14,260
 This is Kunimatsu's work.

1093
00:57:14,260 --> 00:57:17,780
 And he used something more like what Charles recommended, actually, where you tried to

1094
00:57:17,780 --> 00:57:24,500
 learn the center point, but you'd also learn some points on the boundary that would construct

1095
00:57:24,500 --> 00:57:26,620
 the axis.

1096
00:57:26,620 --> 00:57:32,020
 But he also tried to learn an uncertainty representation.

1097
00:57:32,020 --> 00:57:37,660
 Because mugs, my gosh, if you can't see the handle, then there's a whole bunch of angles

1098
00:57:37,660 --> 00:57:39,180
 they could be at.

1099
00:57:39,180 --> 00:57:40,620
 They're not perfectly rotationally symmetric.

1100
00:57:40,620 --> 00:57:43,420
 They're partially rotationally symmetric.

1101
00:57:43,420 --> 00:57:47,260
 You could imagine a handle-less mug that's perfectly rotationally symmetric.

1102
00:57:47,260 --> 00:57:50,100
 Our plates are always completely symmetric.

1103
00:57:50,100 --> 00:57:53,940
 But the sort of interesting case is the mug, where if you can see the handle, you'd like

1104
00:57:53,940 --> 00:57:57,780
 to have high confidence at your label.

1105
00:57:57,780 --> 00:58:06,300
 But if you can't see the handle, then you'd like to somehow-- well, you have a problem.

1106
00:58:06,300 --> 00:58:11,340
 If you're trying to ask the network to output a particular pose, then how do you even label

1107
00:58:11,340 --> 00:58:19,020
 that training set and expect it to give potentially-- if some of my perfectly generated CAD models

1108
00:58:19,020 --> 00:58:23,100
 are in this orientation, they're in this orientation, they're in this orientation, you could give

1109
00:58:23,100 --> 00:58:29,580
 identical inputs to the network and ask it to be outputting different answers.

1110
00:58:29,580 --> 00:58:33,300
 It's not a function.

1111
00:58:33,300 --> 00:58:38,060
 So networks don't like doing that in general.

1112
00:58:38,060 --> 00:58:45,300
 And I think the fundamental answer for that is to try to output not just one of these,

1113
00:58:45,300 --> 00:58:50,620
 but to generalize it to outputting a distribution over poses.

1114
00:58:50,620 --> 00:59:13,460
 That's, I think, the only fundamental way to get around that.

1115
00:59:13,460 --> 00:59:24,100
 So in the COSNET paper, we had a particular way to-- it was kind of thinking a lot about

1116
00:59:24,100 --> 00:59:29,380
 rotational symmetries in one axis, which is that there's a lot of objects that are particularly

1117
00:59:29,380 --> 00:59:30,500
 symmetric in one axis.

1118
00:59:30,500 --> 00:59:35,220
 So it focused its distribution representation on each axis independently.

1119
00:59:35,220 --> 00:59:38,500
 And that was a pretty useful way to do it.

1120
00:59:38,500 --> 00:59:43,020
 And then you could imagine outputting, let's say, a Gaussian distribution, a mean and a

1121
00:59:43,020 --> 00:59:52,300
 covariance for if you did Euler angles for each Euler angle, for instance.

1122
00:59:52,300 --> 00:59:54,500
 And that works fairly well for mugs.

1123
00:59:54,500 --> 00:59:57,780
 It's a little bit hard for me to talk you through this.

1124
00:59:57,780 --> 01:00:01,620
 But basically, there's views here where you can just barely see a handle.

1125
01:00:01,620 --> 01:00:03,780
 And you're supposed to see-- when you see the handle, you're supposed to see pretty

1126
01:00:03,780 --> 01:00:06,020
 narrow uncertainty.

1127
01:00:06,020 --> 01:00:08,480
 And when you can't see the handle, the uncertainty gets broader.

1128
01:00:08,480 --> 01:00:12,140
 So the network's doing a good job.

1129
01:00:12,140 --> 01:00:17,020
 But actually generating the label for that is tough.

1130
01:00:17,020 --> 01:00:22,980
 How do you know from your CAD models that all of these poses-- that I should have a

1131
01:00:22,980 --> 01:00:26,140
 wide distribution here and a narrow distribution?

1132
01:00:26,140 --> 01:00:28,900
 In general, for mugs, you can imagine hacking it.

1133
01:00:28,900 --> 01:00:35,180
 In fact, we did have a way that worked using numerical differencing of images, a way that

1134
01:00:35,180 --> 01:00:36,340
 worked for that paper.

1135
01:00:36,340 --> 01:00:41,060
 But that's a hard problem more generally.

1136
01:00:41,060 --> 01:00:46,860
 So typically, you have to change your loss function to do better.

1137
01:00:46,860 --> 01:00:52,860
 And basically, instead of outputting-- so if your output of your network is a distribution

1138
01:00:52,860 --> 01:01:01,700
 over poses, you have to do-- you just have to accept that if it's producing the maximum

1139
01:01:01,700 --> 01:01:08,220
 likelihood of this distribution, has high probability in the true training set, then

1140
01:01:08,220 --> 01:01:13,660
 you're happy, instead of trying to match the entire distribution to your training set,

1141
01:01:13,660 --> 01:01:14,660
 because you can't produce that.

1142
01:01:14,660 --> 01:01:36,740
 So you have to change your loss to a maximum likelihood formulation.

1143
01:01:36,740 --> 01:01:40,940
 Actually in each of these, the loss function is interesting.

1144
01:01:40,940 --> 01:01:46,540
 Like the quaternions, you don't want to do loss, the L2 loss.

1145
01:01:46,540 --> 01:01:51,380
 You want to do a geodesic distance as a loss function.

1146
01:01:51,380 --> 01:01:57,180
 And each of these representations, you can think about what the right loss function is.

1147
01:01:57,180 --> 01:02:03,480
 But the interesting idea here is just that you can train a distribution over poses by

1148
01:02:03,480 --> 01:02:07,180
 only getting samples from the distribution, instead of getting labels that are the entire

1149
01:02:07,180 --> 01:02:08,180
 distribution.

1150
01:02:08,180 --> 01:02:17,980
 And I think it's really interesting to ask, what are the right ways to write distributions

1151
01:02:17,980 --> 01:02:20,620
 over orientations?

1152
01:02:20,620 --> 01:02:22,860
 And there's an answer that people mostly like.

1153
01:02:22,860 --> 01:02:25,140
 Anybody know the distribution?

1154
01:02:25,140 --> 01:02:32,340
 It's gotten more popular, but it's maybe not mainstream yet.

1155
01:02:32,340 --> 01:02:38,100
 It happens to be named the same name.

1156
01:02:38,100 --> 01:02:43,060
 It's not-- I think the author was not the person who basically destroyed the Incas.

1157
01:02:43,060 --> 01:02:46,180
 But there was an explorer who wasn't so good for the Incas.

1158
01:02:46,180 --> 01:02:48,180
 Anybody get it?

1159
01:02:48,180 --> 01:02:49,180
 No, OK.

1160
01:02:49,180 --> 01:02:50,180
 What is it?

1161
01:02:50,180 --> 01:02:51,180
 It wasn't Pizarro.

1162
01:02:51,180 --> 01:02:52,180
 Bingham.

1163
01:02:52,180 --> 01:02:54,860
 I think maybe it's spelled differently.

1164
01:02:54,860 --> 01:03:00,500
 But I went to Peru last year, so I learned a lot about Bingham.

1165
01:03:00,500 --> 01:03:04,500
 So there's this famous distribution, the Bingham distribution, which is sort of the right way

1166
01:03:04,500 --> 01:03:08,300
 to-- maybe the right way.

1167
01:03:08,300 --> 01:03:13,840
 It's a reasonable way to write distributions over the unit quaternions.

1168
01:03:13,840 --> 01:03:17,980
 And I think the picture is very, very nice and clean.

1169
01:03:17,980 --> 01:03:23,500
 Jared Glover was a student with Leslie and Tomas that worked on this.

1170
01:03:23,500 --> 01:03:26,820
 He did it in the context of a robot playing ping pong.

1171
01:03:26,820 --> 01:03:29,380
 But it was more general than that.

1172
01:03:29,380 --> 01:03:34,140
 And to this day, I think his pictures in his thesis are the best-- they give me the best

1173
01:03:34,140 --> 01:03:38,620
 mental representation of how to think about this.

1174
01:03:38,620 --> 01:03:43,260
 So the fundamental question is, how do you put a distribution over-- for quaternions,

1175
01:03:43,260 --> 01:03:51,460
 you'd like a distribution over the unit sphere in four dimensions that is also antipodal.

1176
01:03:51,460 --> 01:03:53,740
 So that's the question.

1177
01:03:53,740 --> 01:03:56,460
 Gaussians don't do that very well out of the box.

1178
01:03:56,460 --> 01:04:00,980
 How do you write a distribution that's antipodal?

1179
01:04:00,980 --> 01:04:03,300
 But it turns out the trick is not that bad.

1180
01:04:03,300 --> 01:04:08,980
 It's hard for me to see the circle underneath there, but maybe you get the idea.

1181
01:04:08,980 --> 01:04:25,940
 So in 2D-- so I want to write a distribution over the circles.

1182
01:04:25,940 --> 01:04:30,100
 And I'd like to have-- what you see on the right is some Gaussian-like thing that happens

1183
01:04:30,100 --> 01:04:36,420
 to be sort of centered here and centered over here.

1184
01:04:36,420 --> 01:04:42,420
 Then the way to do it, or a way to do it, the Bingham way to do it, is to make a Gaussian

1185
01:04:42,420 --> 01:04:49,120
 in the higher dimensional space in 2D, and then just apply the constraint after the fact

1186
01:04:49,120 --> 01:04:53,940
 that I'm only going to-- I'm going to renormalize the Gaussian so that it's only sampled on

1187
01:04:53,940 --> 01:05:01,660
 the unit circle.

1188
01:05:01,660 --> 01:05:02,660
 That's the Bingham distribution.

1189
01:05:02,660 --> 01:05:03,660
 Yes?

1190
01:05:03,660 --> 01:05:04,660
 [INAUDIBLE]

1191
01:05:04,660 --> 01:05:08,940
 It is the word people use.

1192
01:05:08,940 --> 01:05:12,300
 I didn't make it up myself, but it's a little made up, maybe.

1193
01:05:12,300 --> 01:05:19,140
 I would like the probability of picking q is equal to the probability of picking minus

1194
01:05:19,140 --> 01:05:20,140
 q.

1195
01:05:20,140 --> 01:05:21,140
 [INAUDIBLE]

1196
01:05:21,140 --> 01:05:24,740
 Yeah.

1197
01:05:24,740 --> 01:05:28,180
 I mean, you could say that.

1198
01:05:28,180 --> 01:05:29,180
 That's fine.

1199
01:05:29,180 --> 01:05:30,180
 Yeah.

1200
01:05:30,180 --> 01:05:37,940
 I'm happy with that.

1201
01:05:37,940 --> 01:05:43,220
 So yeah, I mean, I think that is the right picture for the Bingham distribution.

1202
01:05:43,220 --> 01:05:46,100
 But it gets more beautiful in high dimensions.

1203
01:05:46,100 --> 01:05:51,820
 So you can change-- just like if I change the covariance of this Gaussian, that parameterizes

1204
01:05:51,820 --> 01:05:57,380
 the covariance of the distribution on the circle.

1205
01:05:57,380 --> 01:06:00,260
 And like I said, it gets more beautiful.

1206
01:06:00,260 --> 01:06:02,140
 And Jared did a great job of illustrating it.

1207
01:06:02,140 --> 01:06:07,700
 So if you have a narrow uncertainty on the unit circle, you get these nice little peaks

1208
01:06:07,700 --> 01:06:10,000
 on the circle.

1209
01:06:10,000 --> 01:06:13,060
 If you have a broader distribution in one axis, you get things like this.

1210
01:06:13,060 --> 01:06:21,300
 And the limit where you've got a 0 eigenvalue there, then you get the ring.

1211
01:06:21,300 --> 01:06:27,820
 So if your object was perfectly symmetric in one axis, you might actually get this.

1212
01:06:27,820 --> 01:06:34,140
 If you're pretty sure about it in some orientations, but it could be anywhere in rotation, then

1213
01:06:34,140 --> 01:06:39,980
 you'd get a distribution that looks out like that.

1214
01:06:39,980 --> 01:06:42,100
 The normalization is a pain.

1215
01:06:42,100 --> 01:06:47,300
 It's like a big, ugly function integrating on the sphere.

1216
01:06:47,300 --> 01:06:49,860
 So you try not to do that.

1217
01:06:49,860 --> 01:06:53,420
 If you don't actually need a true probability, and you're OK having an unnormalized probability,

1218
01:06:53,420 --> 01:06:57,700
 then you'll be happier with Bingham's.

1219
01:06:57,700 --> 01:07:01,780
 But in general, we know how to compute that.

1220
01:07:01,780 --> 01:07:08,660
 Is Jared estimating the point as a [INAUDIBLE] ball?

1221
01:07:08,660 --> 01:07:13,300
 The best application of Jared's using of Bingham's was actually he was trying to estimate the

1222
01:07:13,300 --> 01:07:17,060
 spin while the ball was flying.

1223
01:07:17,060 --> 01:07:21,740
 And he picked ping pong balls with big labels so you could have some sense.

1224
01:07:21,740 --> 01:07:23,620
 But then he still had uncertainty.

1225
01:07:23,620 --> 01:07:33,700
 And he was trying to do fancy ping pong with spin.

1226
01:07:33,700 --> 01:07:41,620
 So maybe unsurprisingly, there's now deep Bingham networks.

1227
01:07:41,620 --> 01:07:45,940
 And actually, I feel there were papers that predated this that came pretty close to it.

1228
01:07:45,940 --> 01:07:53,820
 But I guess the people that picked deep Bingham networks picked the right title.

1229
01:07:53,820 --> 01:08:01,940
 So I think this is applied quite effectively to registering point cloud data.

1230
01:08:01,940 --> 01:08:04,500
 And I think it's a good representation of choice.

1231
01:08:04,500 --> 01:08:10,300
 They had a really nice paragraph in their introduction.

1232
01:08:10,300 --> 01:08:11,660
 I don't normally read things.

1233
01:08:11,660 --> 01:08:15,940
 But let me just read this or paraphrase this for you.

1234
01:08:15,940 --> 01:08:17,700
 This is a Leo Griebus paper.

1235
01:08:17,700 --> 01:08:20,980
 And there's a number of authors.

1236
01:08:20,980 --> 01:08:25,340
 So they say, a myriad of papers have worked on finding the unique solution to the pose

1237
01:08:25,340 --> 01:08:26,340
 estimation problem.

1238
01:08:26,340 --> 01:08:29,780
 They cited a lot of papers.

1239
01:08:29,780 --> 01:08:32,020
 A pose per view scan.

1240
01:08:32,020 --> 01:08:35,540
 However, this trend is now witnessing a fundamental challenge.

1241
01:08:35,540 --> 01:08:40,420
 A recent school of thought has begun to point out that for our highly complex and ambiguous

1242
01:08:40,420 --> 01:08:48,660
 environments, obtaining a single solution, the correct pose, is simply not sufficient.

1243
01:08:48,660 --> 01:08:52,700
 Instead of estimating a single solution, methods now propose to predict a range of solutions,

1244
01:08:52,700 --> 01:08:57,700
 providing multiple pose hypotheses and solutions that can associate uncertainties to their

1245
01:08:57,700 --> 01:09:02,300
 predictions or even solutions in the form of full probability distributions.

1246
01:09:02,300 --> 01:09:05,540
 I do think that's a trend that is happening.

1247
01:09:05,540 --> 01:09:14,300
 And I think it's required for more sophisticated manipulation pipelines.

1248
01:09:14,300 --> 01:09:20,540
 So they did the Bingham distribution as the output where they would train basically the

1249
01:09:20,540 --> 01:09:26,580
 covariance matrix of that Gaussian is the thing that's coming out of the network.

1250
01:09:26,580 --> 01:09:31,220
 And you can parameterize even that more or less cleverly.

1251
01:09:31,220 --> 01:09:37,420
 And for richer uncertainty distributions where you really have multimodal, you can just do

1252
01:09:37,420 --> 01:09:43,220
 a mixture of Binghams.

1253
01:09:43,220 --> 01:09:46,060
 So just sum over a handful of Binghams.

1254
01:09:46,060 --> 01:09:49,940
 And again, you have to do the loss function the right way with the maximum likelihood

1255
01:09:49,940 --> 01:09:55,500
 loss so that you don't pretend that all of your-- all the ones that you basically would

1256
01:09:55,500 --> 01:09:59,020
 have-- they call it mixture death or something.

1257
01:09:59,020 --> 01:10:00,300
 What do they call it?

1258
01:10:00,300 --> 01:10:05,420
 When the ones that are not getting sampled could just sort of disappear, choosing a maximum

1259
01:10:05,420 --> 01:10:13,500
 likelihood objective function can protect you against that.

1260
01:10:13,500 --> 01:10:19,420
 So this whole notion of choosing the right output of your network for the poses makes

1261
01:10:19,420 --> 01:10:22,380
 a big difference.

1262
01:10:22,380 --> 01:10:25,260
 There's other alternatives.

1263
01:10:25,260 --> 01:10:26,780
 There's people that talk about implicit.

1264
01:10:26,780 --> 01:10:34,980
 I have a snapshot from this idea of trying to overcome orientations by training basically

1265
01:10:34,980 --> 01:10:39,220
 an implicit function where you're basically trying to render the image.

1266
01:10:39,220 --> 01:10:41,060
 You take a bunch of random samples.

1267
01:10:41,060 --> 01:10:42,300
 You run it through a renderer.

1268
01:10:42,300 --> 01:10:46,580
 And if the image looks the same, then you say that's-- you try to find an invariant

1269
01:10:46,580 --> 01:10:49,860
 representation to the rendering.

1270
01:10:49,860 --> 01:10:53,420
 I'll tell you more about that when we talk about the implicit representations on Thursday.

1271
01:10:53,420 --> 01:10:55,620
 But there's a handful of these ideas out there.

1272
01:10:55,620 --> 01:10:59,500
 And it really-- it can matter.

1273
01:10:59,500 --> 01:11:04,100
 Euler angles don't actually work.

1274
01:11:04,100 --> 01:11:07,700
 OK.

1275
01:11:07,700 --> 01:11:08,700
 Questions on deep pose at all?

1276
01:11:08,700 --> 01:11:09,700
 Yes.

1277
01:11:09,700 --> 01:11:10,700
 Do you think this continuity issue is actually like a big factor in like the final algorithm?

1278
01:11:10,700 --> 01:11:11,700
 Because if you just consider this as like a little bit of a-- like a little bit of a

1279
01:11:11,700 --> 01:11:29,180
 [INAUDIBLE]

1280
01:11:29,180 --> 01:11:33,260
 It's a good question about whether this really matters in practice.

1281
01:11:33,260 --> 01:11:37,660
 If you've got a bunch of data sets and every once in a while you get a-- you happen to

1282
01:11:37,660 --> 01:11:42,620
 sample right around the discontinuity, how big of an effect that is.

1283
01:11:42,620 --> 01:11:48,540
 The papers that started highlighting this talked about experimental observations that

1284
01:11:48,540 --> 01:11:55,180
 people had made where you'd see big errors that never went to zero in parts of your space.

1285
01:11:55,180 --> 01:11:59,980
 In various different representations, they talked about empirical observations that forced

1286
01:11:59,980 --> 01:12:03,740
 the investigation.

1287
01:12:03,740 --> 01:12:08,220
 I think-- yeah, it depends how varied your data set is.

1288
01:12:08,220 --> 01:12:13,140
 I think you could say the same thing about Euler angles and humanoid robots.

1289
01:12:13,140 --> 01:12:18,860
 As long as you're-- I mean, I can sort of choose the-- I could choose an Euler axis

1290
01:12:18,860 --> 01:12:21,480
 representation that works really well for Atlas.

1291
01:12:21,480 --> 01:12:25,460
 But if I ever end myself completely horizontal, I'd be in trouble.

1292
01:12:25,460 --> 01:12:26,620
 And I think the same sort of thing.

1293
01:12:26,620 --> 01:12:29,580
 If you can kind of guarantee that your application isn't going to see objects that are near the

1294
01:12:29,580 --> 01:12:34,100
 singularity and you don't have to worry about it, you could be fine with a simpler representation.

1295
01:12:34,100 --> 01:12:41,860
 But doing a little bit more work, I think you can just defend against it.

1296
01:12:41,860 --> 01:12:43,860
 It's more recent that these results have come out.

1297
01:12:43,860 --> 01:12:49,580
 We've been applying networks pretty effectively without all those tricks.

1298
01:12:49,580 --> 01:12:51,860
 Cool.

1299
01:12:51,860 --> 01:12:53,780
 OK.

1300
01:12:53,780 --> 01:13:00,980
 So just to call out, because you guys-- the 6 to 800 folks talked about CLIP in the first

1301
01:13:00,980 --> 01:13:02,820
 paper.

1302
01:13:02,820 --> 01:13:10,220
 But it's sort of an interesting-- if I return a little bit to the generating training data

1303
01:13:10,220 --> 01:13:16,300
 for manipulation, we talked about fine tuning as an approach to this, is that I'll use the

1304
01:13:16,300 --> 01:13:22,660
 ImageNet or COCO data set, or whatever the biggest, closest data set I have is.

1305
01:13:22,660 --> 01:13:27,100
 And then I'll lop the head off and I'll train it for my task.

1306
01:13:27,100 --> 01:13:31,020
 So a really important trend that I just kind of want to call out-- we'll talk about specific

1307
01:13:31,020 --> 01:13:34,580
 instances of contrastive learning when we get to it.

1308
01:13:34,580 --> 01:13:40,440
 But I think even here, it's worth knowing that that idea of fine tuning or retargeting

1309
01:13:40,440 --> 01:13:47,660
 of transfer has sort of enabled even a bigger idea, which is that I can train a different

1310
01:13:47,660 --> 01:13:49,860
 task potentially.

1311
01:13:49,860 --> 01:13:51,980
 It doesn't have to be an image recognition task.

1312
01:13:51,980 --> 01:13:55,180
 Maybe I can train a slightly different task, something that would be easier to generate

1313
01:13:55,180 --> 01:14:00,160
 training data for, lop the head off that, and hope that as long as that task was similar

1314
01:14:00,160 --> 01:14:05,820
 enough, then I can do fine tuning on my particular task.

1315
01:14:05,820 --> 01:14:10,080
 So that's just a question of how far can you go with your retargeting?

1316
01:14:10,080 --> 01:14:15,540
 So there's this wave of results of trying to find-- in self-supervised learning-- of

1317
01:14:15,540 --> 01:14:20,560
 trying to find surrogate tasks where you don't need a human to provide labels.

1318
01:14:20,560 --> 01:14:25,620
 You can answer some different query, possibly just by looking at the data.

1319
01:14:25,620 --> 01:14:32,340
 And the backbone from that network might be learning relevant enough features that you

1320
01:14:32,340 --> 01:14:36,220
 could transfer it with a small amount of data on your task.

1321
01:14:36,220 --> 01:14:40,220
 So an early example of this that I like very much is something that people do is they'll

1322
01:14:40,220 --> 01:14:45,060
 train monocular depth.

1323
01:14:45,060 --> 01:14:47,460
 So here's what-- you take two cameras.

1324
01:14:47,460 --> 01:14:49,420
 You have a stereo image pair.

1325
01:14:49,420 --> 01:14:52,780
 You put it on your autonomous car, whatever you're going to do.

1326
01:14:52,780 --> 01:14:55,660
 And you've got a lot of data with two cameras.

1327
01:14:55,660 --> 01:15:02,380
 And you just ask, can you make-- so you can produce the depth from the stereo data.

1328
01:15:02,380 --> 01:15:06,180
 And then you can just ask, could I have predicted the depth just from one camera?

1329
01:15:06,180 --> 01:15:08,900
 Just take away the second camera, predict the depth from one camera.

1330
01:15:08,900 --> 01:15:13,100
 That's the monocular depth estimation problem.

1331
01:15:13,100 --> 01:15:15,340
 That's easy to train in a lot of settings.

1332
01:15:15,340 --> 01:15:23,420
 It turns out training that task seems to learn a lot of geometric information about the world,

1333
01:15:23,420 --> 01:15:25,020
 maybe unsurprisingly.

1334
01:15:25,020 --> 01:15:31,700
 If you lop the head off that network, you might be able to apply it to your problem.

1335
01:15:31,700 --> 01:15:33,700
 It's pretty cool.

1336
01:15:33,700 --> 01:15:43,500
 The CLIP paper that some of you read for the 6800 was trying to-- if you think about overcoming

1337
01:15:43,500 --> 01:15:50,220
 the limitation of the however many labels in the COCO data set, the CLIP paper was mining

1338
01:15:50,220 --> 01:15:56,560
 the web just trying to find correspondences effectively between words on the web and images

1339
01:15:56,560 --> 01:16:00,260
 on the web.

1340
01:16:00,260 --> 01:16:03,880
 And that's something that nobody needs to supervise.

1341
01:16:03,880 --> 01:16:08,300
 You could argue that the web is supervising it, that every person who made a website was

1342
01:16:08,300 --> 01:16:09,840
 somehow the supervisor.

1343
01:16:09,840 --> 01:16:11,020
 But for me, it's free.

1344
01:16:11,020 --> 01:16:13,440
 Or for open AI, it's free.

1345
01:16:13,440 --> 01:16:19,940
 So you can just go out and mine existing data, have a loss function which just says, could

1346
01:16:19,940 --> 01:16:27,300
 I predict-- given a new image or a new sentence, can I predict which image it came from?

1347
01:16:27,300 --> 01:16:30,340
 That's a self-supervised type signal.

1348
01:16:30,340 --> 01:16:34,380
 And it could generate huge amounts of data without any labeling.

1349
01:16:34,380 --> 01:16:38,380
 And it might be that it's close enough to the task you care about that you can reuse

1350
01:16:38,380 --> 01:16:41,420
 that training for your task.

1351
01:16:41,420 --> 01:16:46,100
 I've even seen people-- so there's a version where you can just now-- if you have learned

1352
01:16:46,100 --> 01:16:51,980
 from watching from image captions to images, learned a bunch of stuff about the web, you

1353
01:16:51,980 --> 01:16:59,160
 can ask questions that will turn CLIP into basically an object detection algorithm.

1354
01:16:59,160 --> 01:17:05,280
 And people-- so Kevin, who played with this, just started putting it on random images and

1355
01:17:05,280 --> 01:17:07,100
 asked questions like, where's the microwave?

1356
01:17:07,100 --> 01:17:09,300
 And it just-- it was never trained.

1357
01:17:09,300 --> 01:17:11,140
 It was never fine-tuned on this data.

1358
01:17:11,140 --> 01:17:16,020
 It was just kind of like, out of the box, having looked at enough images on the web,

1359
01:17:16,020 --> 01:17:21,740
 can I use this without fine-tuning in the wild?

1360
01:17:21,740 --> 01:17:29,820
 Does it have enough classes to actually sort of provide a general purpose object detection

1361
01:17:29,820 --> 01:17:30,820
 system?

1362
01:17:30,820 --> 01:17:32,660
 And it's not perfect.

1363
01:17:32,660 --> 01:17:33,660
 It's pretty good.

1364
01:17:33,660 --> 01:17:34,660
 Pretty good.

1365
01:17:34,660 --> 01:17:37,100
 It's pretty cool.

1366
01:17:37,100 --> 01:17:40,260
 It's pretty funny, because sometimes you get better answers if you ask questions.

1367
01:17:40,260 --> 01:17:43,860
 Like, you have to think, how would someone have written a caption about this?

1368
01:17:43,860 --> 01:17:44,860
 My dog is really cute.

1369
01:17:44,860 --> 01:17:47,980
 It might work better than dog or something.

1370
01:17:47,980 --> 01:17:48,980
 I don't know.

1371
01:17:48,980 --> 01:17:49,980
 That's a bad example.

1372
01:17:49,980 --> 01:17:53,660
 But sometimes the queries I've seen people put into the system to get good results sometimes

1373
01:17:53,660 --> 01:17:55,060
 look totally ridiculous.

1374
01:17:55,060 --> 01:17:56,420
 And they're not the minimal thing.

1375
01:17:56,420 --> 01:18:00,260
 But that's what it correlated.

1376
01:18:00,260 --> 01:18:04,140
 OK, awesome.

1377
01:18:04,140 --> 01:18:11,060
 So hopefully we talked a bit about training data for manipulation.

1378
01:18:11,060 --> 01:18:14,140
 We talked about the recognition and segmentation pipeline.

1379
01:18:14,140 --> 01:18:17,500
 I've given you a notebook so you can play with MaskRCNN.

1380
01:18:17,500 --> 01:18:21,020
 And that alone can feed a pipeline.

1381
01:18:21,020 --> 01:18:24,500
 Pose estimation is definitely a thing you can do.

1382
01:18:24,500 --> 01:18:28,460
 You should think about your geometry understandings as well as your deep network understandings

1383
01:18:28,460 --> 01:18:30,220
 to do it well.

1384
01:18:30,220 --> 01:18:31,220
 And we'll keep going on Thursday.

1385
01:18:31,220 --> 01:18:31,260
 [END PLAYBACK]

1386
01:18:31,260 --> 01:18:32,100
 Thank you.

1387
01:18:32,100 --> 01:18:42,100
 [BLANK_AUDIO]

