 And then new control, that's what roughly model-based RL is.
 But I think it is more than that, right?
 So what is new about sort of the model-based RL of the world?
 I think things that have really not
 been addressed in previous incarnations of system
 identification.
 One of them, for sure, is learning for images.
 That's a new thing that's working incredibly well now.
 It is super exciting and good.
 And the other thing is that I think
 we've transitioned in an important way
 from thinking about we're going to learn
 the model of the plant to something much more.
 So learn the model of the plant for me
 means I've got an atlas, my humanoid robot,
 and I want to figure out the inertias
 of the links of the robot.
 I want to learn the plant, do parameter system
 identification of atlas.
 That's an important task.
 But I think the call to arms here in RL
 is something more than that.
 It's something like learn an open world model, right?
 Learn models that can be applied generally in a kitchen,
 but in any kitchen, that somehow generalize to new domains,
 similar environments that you haven't seen before.
 And that is characteristically very different.
 In particular, we'll talk a bit about state representations.
 It begs fundamental questions about what
 is the right representation to encode these sort of models.
 So I'd say we've gone from thinking about plant models
 towards-- I don't think we're all the way there yet.
 But our goals are certainly to do both model-based RL
 and intuitive theories, which seems a little bit more
 specific to me, but I'll try to limit it.
 OK, so I want to--
 my goal is I would love to spend eight lectures on this,
 but I wanted to at least give you a teaser
 and hopefully encourage you to learn more or ask more questions
 and read more papers about this.
 But I want to tell you a few important lessons
 from system ID.
 So we're going to throw neural networks at this problem.
 That's the hallmark of model-based RL
 is to use neural networks to accomplish
 some of these amazing tasks.
 But I find that there's some really important lessons
 from system ID that can transfer into the neural network case,
 and I'd like to sort of highlight some of those.
 One of the biggest questions, which I've alluded to a few
 times in the term, is this question of what's
 the right representation for state?
 So we'll talk a bit about that, and then I'll
 talk about intuitive physics at the end of the night.
 OK.
 [SIGHS]
 Let me actually start when we talk about these sort
 of lessons from system ID.
 Let me actually start by showing a few slides.
 We had a panel not too recently, not too long ago,
 from one of the robotics symposiums,
 talking about basically deep learning models
 versus physics-based models.
 And actually, the title of the panel, which I didn't like,
 was "Data-Driven Models Versus Physics Models."
 And I took offense to that distinction,
 and I'll just see that in my slides.
 But I've got four slides that I used
 to set up my part of that conversation.
 And maybe it'll set the tone here.
 So I think, first of all, it was actually very striking to me
 to watch the series of papers, as they happen,
 transition from people super excited about model-free RLs--
 so all these things we talked about before
 are roughly model-free RL, which is a funny name.
 But roughly, it would mean that you
 could apply those black-box optimizations.
 You could apply the Q-learning to a real robot.
 If that data was coming from the real robot, that would be OK.
 The reason it's subtle is that if you're using a simulator
 in order to train your RL algorithm,
 then you've got some sort of a model.
 But people would tend to call that still model-free RL.
 If you could have applied it-- even if you did have a model,
 you could have applied it on the real robot,
 we'd call those model-free.
 And they started working really well a handful of years ago,
 but there was this striking movement back,
 I'd say, to model-based RL.
 At the 2018 CoreL conference, it was just like,
 oh, everybody was saying model-based RL.
 Model-free RL isn't good enough.
 Model-based RL is going to save the day.
 We're somewhere in the middle, I think, of what's going to be.
 So we did this global panel on data-driven versus physics-based
 models.
 So let me just remind you what I mean by a dynamic model.
 So we have inputs coming in, outputs going out.
 These could be your robot actuator commands, right?
 These can be the camera images.
 You can do your joint sensors, whatever.
 But fundamentally, the system is a map
 from inputs to output streams.
 The notion of a state, something in the middle,
 is something that is not required to be a system model
 and is only really a construction
 that we use to make it, to give it likes.
 So state space is one version of that.
 I showed this stuff before.
 Where we've been writing them down as xn plus 1 is f of x.
 And y is some function of your output.
 It's not the only one.
 You can take a history of inputs and outputs
 to predict your next output.
 And those are the autoregressive models.
 And you should think about models from mechanics
 or models from LSTMs, recurrent networks,
 as being state space models.
 And you should think about, for instance,
 if you have a feedforward network that
 has a history of images coming in,
 as autoregressive models.
 OK.
 So what does it mean to learn a model?
 So models come in lots of forms.
 I would actually say a Q function is a model, too.
 But it's a model that's trying to predict only one output,
 which is the cost to go.
 So you could say anything--
 you could actually say Q learning
 is doing model learning.
 But typically, when we talk about model-based RL,
 we're trying to learn something much bigger
 about the dynamics of the world.
 Something that we generalize across tasks.
 I want to distinguish between what would be a model class
 and what would be a model instance.
 So let's take the state space form,
 where I've got x is f of x and y is g of x.
 Theta is my parameters.
 So the way that I write down f and g
 describes a class of models.
 The particular parameters that I've chosen to fit
 are an instance of the model.
 So I think people confuse these two.
 And I want you to--
 I want to distinguish them today.
 So when people talk about what's the difference between having
 a deep network model versus a physics-based model,
 really that's more about the model class.
 The question is, should I prefer writing f and g down
 using the equations of motion from the ground,
 or should I use a deep network?
 And in some sense, that's a--
 at first glance, that's just a representation question.
 We could put all kinds of different nonlinearities
 in our network models.
 ReLU, stanchions, LSTMs, transformers,
 these are all architecture decisions.
 And maybe throwing in multibody equations
 is not so different than throwing in an LSTM
 or something like that.
 I think that's a useful way to think about it.
 But I objected, like I said, to the idea
 that somehow that these exercises are so different.
 I would say that all of the classic physicists
 were actually very much empirical people.
 And what they did, basically, is they
 fit very simple models to very noisy data.
 And they gave us this rich class of parametric models
 that we could fit-- that we could get to fit the new data.
 And I think that's something incredibly striking about that.
 I've gone back, and I've read all the histories of all
 these folks.
 And it really-- there's something incredible about how
 they did what they did.
 The fact that Galileo's rolling spheres down eight fine planes,
 and he didn't have perfect stainless steel
 balls or near zero friction and rolling almost forever.
 These are pretty noisy.
 And his measurements were super coarse.
 And somehow he made this leap.
 And maybe the whole field at the time,
 somehow they made this leap to say
 that the right abstraction is to think
 of the zero friction case and the perfect sphere that's
 rolling.
 And somehow that was this magical leap
 that took us to the next conceptual level.
 And that required something more than just the data, I think.
 There was something structural there that Galileo did,
 and Newton did again, and all these folks did.
 But it was very much this question of motivated
 by empirical data and trying to come up
 with the right representations that generalized broadly.
 I kind of joke that what would happen
 if Newton, back in the day, had deep learning, right?
 And maybe he would have just been a function approximator,
 and he would have been done.
 And then I don't know.
 I think the world would look pretty different right now.
 I mean that in a fundamental way.
 I think we've built our world roughly around things
 that we've engineered ourselves into things
 that we can model very well.
 I actually think the world would look very different.
 We somehow engineer things that are very rigid,
 because we know how to simulate them.
 We know how to test them.
 We know these things.
 But that's a function of our mathematics, I think,
 to some extent.
 It would be very interesting to see what would happen
 if we had different governing equations that rule
 our engineering disciplines.
 But I do think this is a fundamental question is,
 what are the right representations?
 How do we redo what happened back then in the Cold War
 Age in the 1600s, 1700s?
 And the question of state representation
 is the one I've shown a bunch of times here.
 What is the right representation for a task like this?
 This is way harder than rolling a stainless steel ball
 down an incline plane.
 And I can't help but wonder what Galileo or people
 would have made of this.
 They probably would have had something much more clever
 to say than what I tend to come up with.
 And no, it's not very good.
 If you think like the total rigid body
 position and velocity of all the pieces,
 that seems like a non-starter.
 And the picture of the pile also sort of
 seems like a non-starter, although there's cases
 where it works fairly well.
 The cognitive psychologist would say, of course,
 it's just like it's a pile of stuff.
 We have this notion of a pile of stuff.
 But what does that mean?
 How do I codify that?
 And how do I learn that?
 I think this is the challenge of our day.
 You could come up with a million.
 Well, actually, that's not true.
 I have six that I always use.
 People laugh at me for using the same things.
 I always talk about buttoning my shirt.
 I talk about spreading peanut butter on toast.
 I talk about chopping onions.
 I found my handful of canonical examples.
 But this is not a hard task for a human to do.
 This should not be considered a hard manipulation task.
 But it completely breaks the stack
 of everything we've talked about in class so far.
 If I gave you a simulator for this right now,
 it would be very slow.
 I don't know how to write a good, super inaccurate version
 of this.
 Some of the particle simulator people write might get close.
 But somehow, I don't think we know the right simple models
 for that kind of a task.
 Yeah?
 With these examples, do you think
 that controlling humans or using them to interact with it
 is that complicated?
 Because it doesn't seem like--
 actually, what they're doing in the scenes
 doesn't seem terribly--
 there's no wild manipulation happening here.
 That's exactly my point.
 So the question is, so this looks easy.
 There's no wild manipulation.
 So is what humans do a fancy thing?
 This is taken as an example of something
 that we think of as very easy.
 But for all of the math we've written down so far,
 it would be very hard.
 I would tend to make a very complicated simulation.
 I would have an extremely hard optimization
 problem in front of me.
 I'd be running hours and hours of RL
 to come up with something that should be tricky.
 So I think the journey now has to be--
 I'll say it over and over again.
 Manipulation-- most of the things we do in manipulation
 are easy.
 They really should be easy.
 They should be easy for an optimization.
 They should be easy for learning, whatever.
 And I don't think we know how to write them down
 in a way that reveals how easy they are.
 That's the problem.
 So why is that easy?
 Is it easy because-- maybe you'll
 say it's easy because the force can pull.
 I mean, but even the perception of how well have we
 done so far, have we evenly spread the peanut butter?
 How would you write down a cost function for that?
 It's hard.
 It's hard.
 We have to change the way we're thinking about these things.
 It's not-- let me be super clear.
 It's not that we don't know how to do simulations.
 We know how to simulate anything.
 I mean, you guys have been-- some of you
 have been bashing your heads on even rigid body simulations.
 But my gosh, simulations are so good.
 This is my favorite example of so good simulation
 of actually doing thermomechanical models
 of baking.
 So it's-- they actually watch bread leaven,
 and then it's going to split in just the right way
 because it was scored.
 This is three simple examples of bread
 that were scored in different ways.
 And if you show me anything, and I go to my dynamics friends
 and say, can you simulate that, they will say yes.
 They will definitely say yes.
 It might take a long time, but they can totally--
 we can simulate almost anything.
 The question is, how do I go from the task that's
 in front of me now, leverage all of my experience,
 and somehow come up with something
 that is that simulation?
 It's the real, the sim that's hurting us,
 not just the ability to write a simulation
 and one that's scoping.
 So there's a bunch of really important lessons, I think,
 from system identification.
 But I think one of the most important ones
 is just stopping to think about what
 makes a model class useful.
 So it's not just fitting the data.
 There are more hidden requirements than that.
 And I think it depends on your use case.
 If you're going to use a model for simulation,
 then you just want to generate a bunch of synthetic training
 data for a perception system.
 Or if you want to somehow score how well your policy is
 going to do by running lots and lots of simulations
 or do offline policy optimization,
 that is sort of like a different--
 that's one use case.
 It's an important use case.
 And it might ask you to expect a certain number of things
 from your models.
 It might mean that the observations that you generate
 are always reasonable.
 You'd never like to see things that
 are ridiculous that you would have never
 seen in the real world.
 Every simulation has rendered output
 that looks like something you could
 have seen in the real world.
 I think that is one requirement.
 It's very hard to write that.
 But I think you can buy one.
 It could be coverage, right?
 That somehow-- that if it could have happened in the real world,
 then it could have happened in my simulation.
 Right?
 That for every possible real world rollout,
 I could have gotten that in my simulation.
 Maybe that's a requirement.
 It could mean that it's always accurate, right?
 That somehow the probability of seeing
 this particular set of images given this action
 was the same in simulation and real.
 That's a problem.
 That might be more than we actually need.
 But I think most of these require
 some amount of being able to go from a lot of data
 into calibrating our model class into a model instance
 by doing the system identification step.
 There's many other things you could list, right?
 So people want generalization, the proper data.
 You want it to somehow be fast to run on a computer,
 repeatable, interpretable, debuggable.
 All these things matter.
 For model-based RL, I think it takes you
 on a slightly different path.
 So you might expect it to be reasonable
 and happen in generalizing, right?
 But if you're really going to do online planning control,
 this is asking something slightly different.
 It's saying that I have a class of models.
 I'm looking now at my kitchen.
 I need to be able to efficiently parse the world
 into my kitchen model in my head.
 My kitchen is in my head.
 I need to be efficient so I can be planning on it at runtime.
 And probably-- this is maybe implied by the efficiency--
 is that it probably means that I need
 to be somehow dialed into my task a little bit,
 that predicting everything possible in my image
 might be more than I could expect from my model.
 It needs to be a little bit more task-cellular.
 So let me-- so when I'm in my kitchen, right,
 and I'm chopping an onion, maybe there's
 like a roll of paper towels off to the side.
 And the paper towels have incredibly rich dynamics.
 Maybe it's slightly flapping in the breeze or something.
 I don't know.
 I'm not paying attention to that at all.
 The task that I'm doing, I'm maybe
 predicting with some level of accuracy.
 But there's a bunch of stuff happening in the scene
 that I'm completely ignoring.
 And I think that feels essential in order
 to have efficient and observable models.
 So of the class of models, state-space models
 do tend to be more efficient and compact.
 But they require some notion of state estimation.
 I'll talk through these in more detail.
 So perhaps the biggest philosophical difference,
 I think, between what we do on Atlas
 and maybe what we've been trying to do--
 some of us have been trying to do more in manipulation,
 for instance, would be to bring physics models more
 to manipulation--
 versus the pure deep learning approach
 with universal approximators is, I think,
 that it's exactly the fact that the neural networks are
 universal approximators.
 So in physics, the functions that we can write down
 are not arbitrary.
 Mechanics gives us constraints, whether there's
 conservation of mass, conservation of energy,
 friction dissipates energy, all these things.
 OK?
 And I think these constraints that give our models
 their structure.
 So they're what gives us power from an algorithmic perspective.
 So this is one of the big questions.
 And I'll return to it at the end.
 How do we balance the generalization power
 that we get and the computational leverage
 that we get from making assumptions
 and saying something that don't happen, like conservation
 laws, versus the general trying to fit any data
 power of universal function approximators?
 It's a tension that I don't know how to resolve completely.
 So this was just, like I said, the intro to that session.
 I basically said, I want the next Newton to come around
 and work on non-uniformity and get a better
 algorithm, because those are the challenges of our days.
 To have someone who comes through, I think,
 helps us find-- or an algorithm that comes through and helps us
 find the magical structure that captures
 the simplicity of the zero friction
 cases of these tasks that are really simple for humans,
 but are unreasonably hard for algorithms right now.
 OK.
 That was a setup.
 But let me try to give you some specific lessons
 from system IP.
 So it's useful to think about a handful of model classes.
 You have the grounding mechanics,
 one super useful set of models.
 You've seen these written down plenty of times.
 There's linear systems.
 So obviously, important structure
 that we know a ton about.
 And I think there's some important lessons.
 Of course, those systems that we hear about now
 are not well-described by linear models in general.
 But you can think so clearly about the linear models
 that a lot of the lessons there should carry over
 into these models.
 All right.
 So I think that the power of deep learning is undeniable.
 But let's see if we can bring a few ideas from these--
 some observations here towards this path.
 OK.
 Grounding mechanics.
 There's so much we know about doing system identification
 and grounding mechanics.
 OK.
 If you wanted to, for instance, identify
 the parameters of the humanoid atlas,
 then we typically start with some really crippling
 assumptions for the pigeon scenario.
 But let's just lay out the assumptions
 that these tools tend to make.
 If we're given a kinematic tree,
 so if I know what objects are in the scene,
 what their relations are, are they
 connected by a revolute joint, free bodies,
 are they prismatic or other way?
 OK.
 Typically, we have to know the contact geometry.
 So we're going to do the estimation of contact.
 We can try to resolve--
 remove any of these assumptions a bit.
 But given these sort of basic assumptions,
 and then given full state observations--
 so the first thing I would do if I'm going to system
 identify a new atlas is spend a lot of time
 trying to get clean estimates out
 of my sensors of the joint angles,
 the orientation of the body, and what
 this is the first pre-processing step.
 Then we have super powerful algorithms.
 We know what they can do.
 We know what their limitations are.
 There are some systems that we just
 don't have enough information to estimate
 all of the parameters of your mechanical system.
 But we know exactly what the identifiable parameters
 are, the unidentifiable parameters.
 Many use least squares.
 We also have a notion of optimal experiment design.
 [INAUDIBLE]
 Because I can write down exactly the parameters
 that I'm trying to fit, I can describe the way
 that they contribute to the dynamics in a linear way.
 I can actually write trajectory optimization algorithms
 that will try to move my robot in a way that
 is maximally informative for my parameters.
 This is a powerful set of tools.
 We're missing it in the more narrow ways,
 but you should know that these exist.
 In particular, as an example of this heuristic,
 least squares probably always have a data matrix
 that you build up from all your state samples.
 You could do something like--
 I mean, if you're done linear system identification,
 you might have done frequency sweeps,
 the kind of things that would excite
 and give you sufficiently rich trajectories
 for system identification.
 In the manipulation or atlas case,
 you might try to design a trajectory that
 minimizes energy in fractions of the walls,
 but which maximized the smallest singular value of your data
 matrix.
 That's such a powerful way to think about these things,
 is to try to design trajectories that maximize your information.
 So you should know--
 I'm not presenting a poll, but you
 should know that these things exist.
 I mean, this rich literature of things
 you can do with mechanics.
 OK.
 In linear system identification, it's even more powerful,
 because the model class is more constrained
 and we can wrap it around our little finger
 and do all kinds of cool things with it.
 In linear system identification--
 oh, I forgot to show an example.
 I put it in for the folks that are doing the catching.
 So just as an example, I told you
 we could fit atlas as parameters.
 But we can also--
 in a manipulation-specific example--
 I've mentioned this once before, but you
 could do sort of amazing throwing and catching.
 So this is an old paper by John Jopp, '91.
 This was actually in NE43, the old AI building,
 just up on the ninth floor.
 It was weird.
 I left the audio up here.
 It's kind of low.
 Right?
 '91.
 And it talks.
 I don't like it.
 That's pretty silly.
 But I don't think that's the research.
 But computer vision was harder back then.
 And they sort of didn't solve it.
 It's a bright red ball.
 And it's two foveating cameras.
 How does that angle?
 It mounts up there.
 And basically, they just try to--
 the job of the perception system is
 to keep the red blob in the middle of the camera.
 So it just turns the camera to keep the red blob
 in the middle of the camera.
 And then the angle encoders of the camera
 tell you where the ball is, right?
 In triangle.
 Really, I mean, that's what you do.
 I don't like it.
 OK.
 So that's an old throwing and catching.
 But this is the one I wanted to--
 Are they supposed to be controlling the robot
 in that case?
 That's a WAM.
 So that was torque control.
 That's a various WAM.
 [INAUDIBLE]
 But for the contact part, are they trying to move their hand
 to the--
 for the right location?
 Yeah.
 [INAUDIBLE]
 Yeah, basically, they do subjective matching.
 That paper is a good read.
 So I'm pumped.
 OK.
 But so yeah, they basically try to trajectory speed
 match the ball, right?
 They predict the trajectory of the ball.
 And they go like this.
 And they close their hand.
 OK.
 But the more subtle thing that's happening there in the middle--
 which let me show that again here.
 So the throw is pretty good, right?
 It picks up an arbitrary ball.
 They pick it-- it goes back once,
 partly to open and close the hand to make
 sure it's in a known location.
 And then it throws.
 And it can pick up relatively arbitrary mass balls
 and throw them into a little hoop
 all the way across the room.
 OK.
 And the way it does that--
 [INAUDIBLE]
 It does a little re-grab to get in the known location.
 And then shoot a basket across the room.
 OK.
 They had a parameter estimation and a multi-body parameter
 estimation algorithm running online.
 In the course of just going back,
 they're able to estimate the mass and the center
 of mass of the ball accurately enough
 that they can release it and throw it into a hoop across the room.
 Right?
 That's good stuff.
 That's good stuff, right?
 I mean, of course, there's lots of restrictions and assumptions.
 But that's pretty good.
 And to do it on so little data--
 I mean, because it's a simple--
 to do it on so little data and be able to mail it,
 I think that's super great.
 And I think they never actually published that paper.
 I had to--
 I was talking to Alberto Rodriguez one time.
 And I was like, you know John Jock's little drawing stuff?
 No.
 And then I had to ask John Jock.
 And he showed me this video.
 And I said, what paper?
 And he said, we never published it.
 Right?
 But he found the video for me.
 Right?
 I took out the other-- one more.
 I don't care.
 I have one.
 Yeah.
 He also has Catch Me Airplane.
 He doesn't throw the airplane back.
 That would be really interesting.
 But--
 I think you can see the foveating vision system here.
 Bright red tag on the top of the airplane.
 And then all the cameras do is try
 to keep that in the middle of the view.
 And it's an amazing thing.
 OK.
 So there's so much wealth available here.
 And it's tempting.
 I mean, for me, it's very tempting to try to bring
 all this into the space of manipulation.
 But it doesn't work with carrots.
 It doesn't work with peanut butter.
 Somehow, that breaks the stack.
 Linear system has a bunch of other important lessons, too.
 So super well understood.
 In general, the linear system, I think, probably still not
 by best.
 But there are good algorithms.
 For instance, there's an algorithm called O-column.
 Same column, but algorithm for linear state-space
 identification.
 And these algorithms really do--
 they do put out what state-space models.
 So it's really from U to Y.
 And I think they don't capture all the--
 I mean, a lot of the systems that we care about
 for the information are inherently nonlinear.
 But there are still important lessons.
 So I actually made some examples just
 to try to convince people that it was relevant.
 So to take one of the classic R and L problems of a cart pole--
 this is just a physical version of a car pole.
 So you know what a car pole is.
 And I took the whole column algorithm.
 And I generated a bunch of data around the balancing regime.
 But I only used key points.
 So I didn't tell the position of the cart or the--
 well, I just gave U coming in with the force on the cart.
 [INAUDIBLE]
 Y coming out with the location of the key point.
 And I asked it to come up with a model that looked like this.
 Find the ABCD that describes the dynamic of that.
 And one of the things you have to do,
 which again is an important lesson here,
 is you have to pick the size of X.
 That is a design parameter that doesn't automatically
 discover the dimension of X. You have to pick the size of X.
 And you can do things like in the linear case,
 you can expect monotonic improvement in performance.
 So you can, for instance, choose X
 to be one dimension, two dimensions, three dimensions,
 and plot the quality of the fit.
 And at some point, you have diminishing returns.
 And so you say that's the dimension
 of my state realization.
 So I thought an interesting question here
 would be if I show only key points for the cart pole
 and run the standard linear input-output system
 identification algorithm, can it discover the fact
 that there are two degrees of freedom, positions
 and velocities of the system?
 And it does.
 It works beautifully, of course.
 So you get these beautiful impulse responses
 in the linear regime.
 You run the Hohkohmen algorithm.
 And the singular value drops off exactly after you'd expect.
 And it says, yes, you have two degrees of freedom,
 even though I've given high dimensional
 in the number of key points.
 But I didn't tell it.
 Some key points were attached to this body.
 Some key points-- it just discovered that from data.
 And that's super useful.
 So there's very powerful tools, again,
 from the linear system that are potentially growing.
 And there's some important lessons there.
 So I think that choosing the state dimension
 and exploring like that is an important lesson.
 But there's other lessons, too.
 So let me get myself a fresh board for this.
 [WRITING ON BOARD]
 One of the important lessons, which we see in multibody,
 but it's more visible, I'd say, in linear system
 identification, is the difference
 between what we'll call equation error versus simulation
 error.
 OK?
 So there's multiple ways that we could write a cost
 function for system identification.
 Let me write these two down.
 So I have my beta coming in, which is just--
 I'll call it un and yn.
 And if I do some--
 I'll do the simpler case where I've separated out
 that state estimation.
 But I'll-- this works in the real-common case
 and the general case.
 But I'll call x hat my estimated state.
 [WRITING ON BOARD]
 There's two natural cost functions.
 And you'll see these in the neural network case, too.
 They're harder to--
 I think they're easier to think about.
 So one would be to minimize over alpha,
 sum over my beta of--
 I'll write it in a slightly more general case--
 f of alpha x hat un minus x hat.
 [WRITING ON BOARD]
 Simulation error, on the other hand,
 needs to minimize--
 so two different possible metrics
 for system identification.
 You see the difference here.
 So if I have beta coming in-- let's say
 I process myself into a local state estimate.
 Then if I were to just say I'm going
 to do supervised learning on this problem,
 the natural thing to do would be say,
 I'm going to accumulate in output beta for this function
 f, and I'm going to minimize the least squared error.
 And that's what this does exactly.
 It's going to take x, u, x n plus 1, all this data,
 make up a big table, and then just do b squared.
 That's sort of the one-step identification procedure.
 Simulation error is different.
 It says you're not allowed to use the data
 intermediate in my rollout.
 I'm going to initialize my simulation at time 0
 with the data.
 But then my cost is actually on the long-term simulation.
 As I roll out my model forward into time,
 my long-term predictions have to match my data.
 OK?
 This is a fundamental difference.
 This tends to lead to easier optimization problems.
 A lot of times, like in the linear system IP case,
 this would be a least squares problem.
 This is even harder in the linear case.
 OK?
 But we see there's some beautiful lessons
 that we get, even in the linear case,
 of the differences between these two.
 The big lesson here is that you can make this small.
 You can take your training loss to basically 0.
 And this is still a big--
 so what do I mean by that?
 If I have a trajectory of data, lots of trajectories of data,
 and I'm just trying to-- basically, at every step,
 I reset my data, reset my model to the true data.
 I make a one-step prediction.
 And I try to make this incremental cost small.
 That's the equation error.
 The simulation error says I'm going
 to roll this whole thing out, and I'm
 going to penalize the distance of the rollout.
 OK?
 One of the painful lessons from system identification
 is that you can make this one-step error small.
 But if you start simulating it forward,
 you can get a very big long-term error.
 Yeah?
 So I'm curious how that changes with the stability
 of the system.
 And in particular, if you put it in close with the controller,
 you can make your disturbance to output gain very small.
 That's spot on.
 Good.
 Yeah, sorry.
 Finish your question.
 So it seems like having a small equation error,
 that's just a small disturbance.
 And feedback cancels that out all the time.
 Good.
 So the question was, how does this
 relate to the stability of the system?
 Can you use robust stabilization?
 Can you take your data from a closed-loop system
 instead of an open-loop system?
 All these questions.
 And that is exactly the right set of questions.
 And I think it's crystal clear.
 In fact, the case in linear system identification
 that is the one that people watch out for
 is that you almost always want your data to be generated
 from a stable system.
 If you're trying to fit an unstable system,
 that's maybe like a no-close problem.
 You kind of go on and go there.
 So almost always try to generate data from a stable system.
 If you just fit the best A, B, C, and D to that data
 in the equation error sense, you can drive the error low.
 But your fit model could be unstable.
 Right?
 So in the linear system world, the mitigation for this
 is to actually try to do--
 if you want to use the one-step error,
 you try to put additional stability constraints
 on your model parameters.
 That can bring the quality of a fit from equation error
 much closer to the fit from simulation error.
 So that's exactly what you're looking at.
 Now, the challenge here, this mitigation,
 I don't know how to bring to neural networks.
 I mean, there are heuristics that people try to bring.
 But in general, restricting my search in the linear systems
 case to stable models is tractable.
 And I don't know how to do it if you're fitting it
 back to the detail of the model.
 So that's a lesson, I think, that we should try to embrace.
 I think there's another important lesson here,
 which is the balanced realizations.
 So given a bunch of data, there are many A, B, C, and D
 matrices that fit the data.
 In fact, there's a whole family of A, B, C, and D matrices
 that fit the data equally well.
 I said, what makes a good model?
 One of the things that makes a good model
 is that it should be useful for control and estimation.
 So how do you capture what your models could be useful
 for control and estimation?
 And it turns out that we have good understanding
 of that in linear systems.
 We talk about controllability gramming.
 We talk about the observability gramming.
 And we can embed that as a choice in our optimization
 algorithms and try to choose models that are explicitly
 controllable and observable.
 And we try to balance the two, actually.
 They're essentially the same.
 But I think we can see this in a very simple example.
 So using the equations here, my claim
 is if I found some E, B, and C that fits the data,
 then there's a whole continuum of models
 that fit the data just as well.
 Why is that?
 So if you take any invertible matrix, T,
 then if A, and B, and C, and D fit the model well,
 then there's another model that fits that is exactly the same.
 [WRITING ON BOARD]
 These are the similarity transforms.
 So what does that mean?
 So here's some intuition for what
 the similarity transforms are.
 So if I have only input-output data, y,
 and I've made some choice of A, B, and C,
 then certainly nobody told me what x was.
 I had to just pick an x.
 Certainly, I could just-- if x is like five numbers,
 I could write those numbers in a different order.
 I could switch x1 and x2.
 Clearly, that would be the same model.
 There's nobody who's made a choice.
 This is happening in an LSTM also,
 is that somehow the units in a recurrent network
 have chosen to represent some states.
 But you could have switched that neuron 32
 was being used for what neuron 31 was, and vice versa.
 There's always this sort of symmetry inside there.
 But it's even worse than that.
 And I think it's exposed in the linear system.
 I could take x and make x-- in the scalar case,
 it's even simpler.
 So let's say I just have a single variable x.
 x could be under the domain 0 and 1.
 It could be over the domain 0 and 100.
 It could be in the domain 0 and a million.
 There's nothing about input-output data
 that says what scale my A and B may be.
 My x may be.
 Theta should be.
 In fact, if I were to just multiply--
 take my u's, multiply them by a big number,
 then I might get big x's around.
 And I would just divide by a big number on the way out.
 Or if I had a--
 if it was a very small number, I could
 multiply by a very big number.
 There's nothing telling me to make x nicely
 conditioned in the middle.
 We see all kinds of heuristics in deep learning about--
 make sure you normalize your inputs and outputs,
 and stuff like this.
 I think it's the same symptom as what we're seeing here.
 In linear systems, we have a beautiful way
 to address this, which is to try to choose a balanced
 realization.
 Balanced realizations, they let controllability
 and observability compete.
 And they choose a particular A and B
 in the whole column algorithm, for instance.
 The balanced realization is one that has--
 the details aren't as essential, but basically,
 the controllability gramian and the observability gramian
 are both diagonal and equal.
 And that's what balances the model's utility
 for observability and controllability.
,
 And I would love to somehow capture that same--
 generalize that same notion of somehow the models
 that I want to fit to data, if I've
 got a more powerful function class, model class,
 should somehow be models that I can observe,
 and somehow models that I can control.
 And I know these symmetries are all
 hiding inside the deep learning models,
 but I don't have the same tool chain to address them.
 So if you map these lessons to deep learning,
 you can find models that achieve effectively zero training
 error on your data.
 What people always complain about in model-based RL
 is that they make good short-term predictions,
 but they don't make good long-term predictions.
 And that ruling thing about the long-term
 becomes very inefficient.
 I think it's lost somewhere in these notions of you
 can get a small--
 you don't have stability in the models, you can get large--
 even if your training error is zero,
 if you perturb your data a little bit,
 you hope your training error is epsilon.
 That would be the machine-learning-bound way
 to think about these things, is that I'm
 in the epsilon error of my model sense.
 But if I close that in a feedback loop of simulation,
 these errors can grow very quickly,
 because the system may not be stable.
 So a couple, I think, big lessons from system
 identification that we'd like to use here.
 Let me address the question of state representations.
 So in the linear case, it's just I've got to pick an x.
 The whole Kalman algorithm picks an x.
 But more generally, there's a fundamental question of,
 what do you use as your state?
 What's the state of the unknown?
 [STUDENT STANDING]
 You can try to just train an input-output model,
 like an LSTM, for instance, on input-output data,
 and let it recover x.
 That's a perfectly reasonable thing to do.
 It's pretty hard.
 Maybe you say x is just an image.
 [STUDENT STANDING]
 That's a reasonable thing to do.
 But the more general notion of state
 is that somehow it should be a sufficient statistic.
 OK.
 I have some history.
 [STUDENT STANDING]
 Then knowing x at time n should be
 as good as knowing the whole history.
 [STUDENT STANDING]
 That's the ideal notion of state.
 So the examples of--
 you can just care about your e-lap,
 then our positions and velocities.
 [STUDENT STANDING]
 The e-lap are sufficient to simulate--
 no matter what the e-lap did before,
 if you tell me its position and velocity,
 I can simulate what the e-lap is going to do in the future.
 There is no information from the past
 that will change the way I would simulate in the future.
 If you tell me the positions and velocities,
 the grand genus state is a complete description.
 It's a sufficient statistic to predict anything.
 It's only the e, y, and the z.
 More generally, those of you that
 know about the Leib space plane and Pond-DPs and the like,
 more generally, the sufficient statistics
 are going to be some possibly distribution
 over possible states, some belief state or some information
 state.
 So if we're going to use ML to try to discover state
 representations, I think there's this question of,
 to what extent can ML produce sufficient--
 how do we encourage it to produce sufficient statistics?
 Are there heuristics that we can use to guide it?
 There's a lot of rich ideas in this case.
 One example, a simple example that you might think of,
 if we talk about key points in the dense object nets,
 somehow we need to go from a history of images
 into something that's sufficient to predict the future
 performance of our system.
 So a natural example, for instance,
 would be to try to just use key points.
 So we have a project Lucas and Yunzhu and V did
 of trying to take the dense object nets
 that we talked about before and just said, can we use states--
 use the key points as an effective state representation
 for manipulation.
 So this is something that is grounded in perception.
 We know how to go from RGB inputs into key points.
 But we can learn much more compact models
 of key point dynamics than we could from image dynamics.
 So that's a pretty reasonable--
 hold on.
 That's only going to be useful if the tasks are observable,
 if the key points are observable and the task is--
 a single set of key points will only
 work if the system is quasi-static.
 In the history of key points, there's
 a whole bunch of variations of that.
 But there's, I think, a lot of interesting work
 in general of trying to find these learned state
 representations or trying to design visual features that
 can be used in state representations.
 Now, if we compare this to what we showed you--
 what I showed you before, like using dense object nets
 to put hats on rack and do fairly complicated tasks,
 this is actually a relatively simple task.
 The good thing about this work was
 that the original task I told you was behavior cloning.
 So basically, we had to give a lot of imitations
 in order to figure out how to put a hat on a rack.
 We would train a policy directly that
 used the dense descriptors or the visual representation
 in order to train a policy to put a hat on the rack.
 And the reason that was painful is
 that if you changed the objective just a little bit,
 you couldn't reuse all that data.
 This system, you could reuse the data.
 And if you changed the objective,
 you wanted to just push the box to a different location,
 then you could reuse all the existing data
 and just replant on the fly, giving them all.
 The interesting thing, though, was that we couldn't,
 at the time--
 to be fair, before Lucas was trying to finish his thesis
 and COVID had just hit, and we couldn't get in the lab.
 So it's possible that in a normal year,
 we could have gotten a lot of experiments for the hat
 to work with this too.
 But in this work, basically, he was
 able to train networks to predict future viewpoint
 dynamics, basically getting almost zero prediction error
 in the short term.
 And the rollouts were good, and everything was good.
 But actually, the thing that was hard
 was that it was still hard, given a neural network
 model, with the long-term planning,
 actually it was harder to do planning
 on that neural network model.
 The equations in the neural network
 were too complicated and too fragile
 to do our stronger planning ideas on.
 He actually said it's interesting that, for once,
 planning-- perception doesn't feel like the bottleneck
 with control.
 And that's the experiment for rate control.
 So the control was almost always, in that case,
 an end-effector position or stiffness,
 but it was typically just position for those tasks
 that didn't have much contact dynamics.
 So it did come into contact, but that was fine.
 It was about too light compared to the aim.
 Action space was the end-effector position
 of the aim.
 And the task was specified, given a single demonstration,
 just that the key points should go to this location,
 for instance, and plan on the fly through the neural network
 model to get to that location.
 So the contact part was the fragile part?
 It was just, in the case where it didn't work,
 it was just not maintaining contact?
 Actually, no.
 So I think the planning algorithms
 that people use, once you have a P-P algorithm,
 are surprisingly weak, I would say.
 They tend to be more like the CMA we talked about,
 the black box optimization.
 People would be planning with e-coms.
 There's a relatively small group of people who use them.
 You would think everybody did gradient descent,
 but actually, it's not as common.
 A lot of people think with gradient descent,
 it's stuck in local minima too quickly.
 So people tend to do CEM, which is a cross-entry method,
 or MDPI, which is an integral.
 I won't write it all out.
 But it's very similar to CEM.
 It's an objective function.
 But these are more like the black box algorithms,
 with a little bit of extra work done to--
 important work done to make them use the temporal structure
 of the optimal control problem.
 But they are more like black box solution methods.
 And at some point, they seem not to be strong enough.
 Even if your network model is good,
 and your rollouts are accurate in that particular work,
 we felt the bottleneck was that the planning algorithm didn't
 find good plans.
 We did not find good paths, even though the models
 were the problems in the path.
 So I think there's a lot of good work
 that people should be doing on making these algorithms
 stronger.
 And a big question that I have for myself
 is, can you make stronger algorithms when the model class
 is so rich?
 Most of the time, we're seeing stronger algorithms apply.
 It's leveraging some structure in the model class.
 And so constraining the model class might help here.
 And does it handicap your ability to predict?
 We're going to have to find out.
, You have about 40 more examples in about four minutes.
 So those-- yeah.
 I just feel great.
 So what did they see?
 Would you try the most basic image dynamics for these tasks?
 What was the comparison?
 Like, what did you gain?
 So we did try against auto-encoder features.
 The default would be to try to--
 so one way that people might learn models--
 I do actually have a couple of examples of this--
 would be to try to say, the thing that you want to predict
 is the future images that you would see.
 So if you have images coming in, actions coming in,
 you should find a neural network model that
 can predict future images.
 And you like to think of that model as being a bottleneck.
 So you're trying to take a big image in.
 You bring it down to a smaller latent vector z
 and come back out.
 And in the simplest case, you can just take the actions off
 and just do an auto-encoder.
 So auto-encoder models of state representations
 are a reasonable thing to try.
 And they're kind of the baseline that we compare against.
 So in general, the key point style model,
 the dense descriptors, could train and generalize--
 they leveraged the additional structure
 of the self-supervision.
 And you could use less visual data
 to train features that generalize more interpreted.
 I think it's one of a class of these kind of representations
 that can leverage additional information about this thing.
 Yunzu in the lab, which I have his work that I won't get to,
 I'm afraid, but every time I say something like what I just
 said, he comes along and makes a neural network
 to predict future images in ways that blow my mind.
 And I will continue to be impressed, I think,
 by this.
 He's got a new work for Tariq.
 [INAUDIBLE]
 So Yunzu's got this recent work here
 of learning open-loop prediction,
 learning models that can predict--
 we've got a simulation based on flex,
 and it's doing very complicated forward dynamics.
 And he's predicting future images.
 And it's ridiculously good.
 And then he does model predictive control
 to try to do pouring, for instance,
 to have this pouring effect.
 I think the challenge of these models--
 I mean, you get little fuzzy images,
 but that's still unbelievably--
 I mean, you always compare it against--
 you always are just learning the difference
 between some sort of baseline image.
 But still, this is ridiculously good predictions
 of future images.
 The challenge in these kind of models, I think,
 is that they tend to be fairly narrow.
 And you have to play games like restricting your planning
 algorithm to search where the data was generated.
 So that's a common--
 Abhishek mentioned that too, right?
 He said, how do you restrict your search in the policy space
 to stay near the data?
 We see this over and over again in model-based RL,
 in particular, because if you learn a deep model
 and it has arbitrary representational power
 and narrow data, then the first thing that happens--
 I'm going to stop that.
 But the first thing that happens when you do planning
 is the planner will ultimately exploit--
 I mean, you're almost asking your planner
 to exploit your model.
 So if your model has small errors
 and those errors happen to imply to the model
 that it can do a good job at control,
 it'll walk arbitrarily far away from your data
 in order to accomplish the task.
 So having a model that simulates well
 is actually one requirement.
 Having a model that can live up to the scrutiny
 of an optimization algorithm attacking it
 is a higher request.
 So it's less of a problem-- optimization
 is less of a problem in the key point-based models?
 Generalization, I would say, is--
 so in the extreme case of taking multibody models,
 we don't worry much about generalization, right?
 Because if I fit the masses and moments of inertia
 in some place and I put the system
 in a very different state, I still
 expect the model to be reasonable.
 It generalizes very well over the state space.
 Full image-based things I expect to be very narrow.
 Key points, I think, are somewhere in the middle.
 They're only suitable for some tasks.
 But they did leverage some--
 I mean, they can capture rigid body approximation.
 Key points are sufficient to estimate a pose
 if you needed to do that.
 So they're somewhere in the middle.
 They can be exploited for sure, especially
 if it's a deep network predicting
 the forward roll-offs of a key point.
 But I do think it implies some structure that generalizes
 better.
 OK, I didn't get to talk about intuitive physics.
 I'm sorry that I didn't have it as well.
 But I would actually say that this
 is what's lighting me up right now, this question of,
 what are the right--
 I mean, things that are easy for humans--
 spreading peanut butter on toast.
 I did a couple of fun examples I have in the slides
 I think you want to look, right?
 But there are lots of tasks that humans are good at
 and our models find incredibly hard.
 One of the simple--
 a fun example is--
 hold up over here.
 Yeah, this one, right?
 So if I said a simple experiment,
 like a psychological experiment from intuitive physics,
 say, which of these matches--
 has the object on top drink from the cloth?
 If I wanted to put that in a simulator
 and do an accurate simulation or a predict,
 and then compare the error in the pixel space of some
 and all of that system, that's ridiculously hard
 for all the tools that I've been working on.
 And humans are super good at it.
 Babies are relatively good at this task.
 Babies are good at understanding that objects don't move
 until they talk.
 But six-year-olds are actually-- and probably I am not good at--
 if you ask me to predict the parabolic trajectory of a ball
 flying through the air, which is trivial in the sense
 of Newton's mechanics, we're actually really bad at that.
 So there's a fundamental disconnect
 at the way humans generalize, I think,
 and the way our physics-based models generalize.
 And it's a mystery.
 Terry was asking me earlier, he says,
 computers are good at a lot of things.
 Why would you make them worse, more like a human?
 And if humans are bad at some things,
 like predicting the trajectory of a ball,
 why would you ever handicap your algorithm to be worse?
 And I don't want to handicap the algorithm,
 but I do wonder if you have to give up something.
 I think it's a big question, right?
 What is the model class that allows
 us to generalize so well in the home,
 and do reasonable things, load the dishwasher,
 and fit the relatively little data,
 watch somebody do a task, and be able to do it ourselves?
 What is that model class?
 I don't know.
 I don't think it's a deep network.
 And I don't think it's the Lagrangian mechanics
 with the rigid body tree given to you.
 There's something else missing, right?
 And I think this is a big-- this is what's lighting up right
 now.
 I think I'm on the quest to try to understand that state
 representation question.
 That's the big one.
 OK.
 Please ask us all your questions on the floor.
