 [INTERPOSING VOICES]
 Hello.
 Welcome, everybody.
 Thank you for packing in.
 We've got a room that's not quite big enough.
 We're going to have to see how that goes.
 So I will make adjustments as necessary.
 And I encourage everybody to come to class.
 So don't let this dissuade you.
 And I'll do some hardware demos to try to keep you coming.
 So yeah, welcome to robotic manipulation.
 I had to call it robotic manipulation
 because I just thought manipulation in general
 without context could be anything.
 But I'm going to just talk about manipulation today
 and throughout the semester.
 I hope that by the end of the lecture,
 you'll understand a bit more about what
 I mean by manipulation and what are the exciting challenges
 that it brings.
 In particular, I mean, you guys are
 coming into the field at exactly the right time.
 My gosh, if robots are not just on this incredible rate
 of progress right now, the things that we can do today
 that we couldn't do last year, it's
 kind of a fun time to write course notes
 because the course notes I worked hard on a year ago
 are obsolete in great ways now.
 So you'll watch the course notes evolve during the semester.
 OK, so let me dive in.
 Let me just start with a little bit of the course info.
 First of all, I'll introduce the people.
 I didn't mean for that to be me to pop up,
 but Tommy's here working the video camera.
 We are going to try to record the lectures.
 There's actually a brand new video capture
 system in the back of the room that hopefully will do it
 automatically and hopefully will be beautiful and good.
 But just in case, we're doing it manually this time too.
 I can't guarantee that every lecture will be perfect online.
 We've had times where the audio was gone
 or the blackboard was too fuzzy or something like this.
 So coming to class is great.
 But we do try to put it online.
 Sorry, Tommy's working the camera here.
 Michael's right here.
 We have also Quincy.
 Is Quincy here?
 Maybe Quincy's not here yet.
 OK, Ethan I saw, definitely.
 Ethan's here.
 Sadhana, right there.
 Broken image, sorry for that.
 Pranav, is Pranav here?
 We've got another class.
 OK, yeah.
 And then we also have two communication instructors.
 So I'll say a few more words about that.
 But Elena's over here.
 Awesome.
 I think it's an extremely strong core staff.
 And I hope you'll work with us closely over the semester.
 Let me say a few words about the communications part.
 I get a lot of questions just as the semester starts about this.
 And I just want to say it as clearly as possible right now.
 So you can take the class as an undergraduate.
 You can take the class, the graduate version of the class,
 the 4212.
 If you're in the undergraduate, that
 comes with a CIM component, the communications intensive.
 So that's a 15 unit course.
 That adds recitations on Friday.
 If you're a grad student, you don't take the recitations.
 The requirements are hopefully very clear on the website.
 The differences, the grad students
 will have a few extra problems on the problem sets
 and different requirements for the project.
 Both groups will do projects.
 The technical expectations for the graduate students
 are the--
 you can be an undergrad who takes the grad class.
 If you take 4212, the expectation
 on the technical part of the presentation goes up a bit.
 But you lose the CIM.
 Now the CIM, some of you need it for graduation.
 Some of you don't.
 But I actually think it's awesome, whether you need it
 or don't need it.
 I have to say the projects are a big part of the class.
 And at the end of the semester, some of the best projects
 that have come out have been the ones that have been nurtured
 through the CIM process to be like super projects.
 So I really think it's excellent.
 There'll be journal clubs where you're reviewing manipulation
 relevant papers to begin with.
 And then it'll work into helping you through the project,
 turning your project from a project to a super project.
 But yeah, just so you-- those are your options.
 And there's the project-related CI assignments
 are hopefully clear on the website.
 And you can see the relevant project-related for the 4212.
 Put them side by side.
 Make your decision.
 Again, the recitations are only if you're in the CIM.
 OK, just quick logistics.
 So we're going to mostly use Piazza for the robotics
 portion of the semester.
 So please make sure you're on Piazza.
 Your MIT credentials should get you in.
 If you have any trouble, let us know.
 The CIM component only will try to push material to you
 through Canvas.
 So if you're an undergrad or if you're--
 I should just say undergrad.
 If you're an undergrad or if you're--
 I should just say if you're taking 4210,
 you should sign up for Canvas or log into Canvas.
 Everybody has credentials, but you don't need it
 if you're in 4212.
 The course guidelines are up on the website.
 The percentage distribution, the late policy, all these things
 are hopefully very clear on the website.
 I'm not going to give you a piece of paper handout.
 So please review them and just make
 sure you're happy with them.
 If you have any questions, ask.
 The lecture notes are on the website too.
 And they're meant to be interactive.
 And they're meant to be even a place where
 you can ask questions directly.
 You can highlight something in the lecture notes,
 say, what the heck did you mean by this?
 And I will answer.
 It's a good way to ask questions.
 OK, we have roughly weekly problem sets.
 They will taper off as the project tapers up, ramps up.
 So they're due on Wednesday.
 The first problem set will be released maybe today,
 but certainly by tomorrow.
 It's on the course calendar.
 And the final project is a big part of the course.
 You're going to be able to build some pretty awesome systems
 and most of them will be in simulation.
 Some of you might have some hardware you want to try.
 We have some hardware that if you convince me in simulation
 that it's ready for hardware, we can try to help you with that.
 OK, so all of the course website,
 all the details of assignments and late terms, late policies
 and everything are all on the website.
 And this is what the course notes look like.
 So you can see people ask questions.
 I have a question about which rendering engine is used, right?
 And I want to answer.
 And actually, sometimes people from all over the world
 ask questions.
 And I do my best to answer.
 Do you use OpenRave?
 Do we use OpenRave?
 No, no, no, don't use OpenRave.
 You're talking about for this?
 This is actually-- it's called a hypothesis.
 It's a little HTML JavaScript plugin.
 OK, yeah, you'll also find both in the lectures
 and in the notes things that you can interact with.
 The network seems to be a little slow here.
 That's what it says when it's loading too, just so you know.
 But we'll see how-- there we go.
 OK, so you'll see we have this web-based visualizer.
 That's pretty awesome.
 You can interact with it.
 If you're watching the slides and you've pulled up the slides,
 you can interact with it right now.
 This is just our saved render.
 But this is also what you get when you're
 doing work in the class.
 And yeah, it's very interactive.
 So this is our spot from Boston Dynamics simulation,
 just saved a little recording of it getting up and looking
 around a little bit.
 You could sort of understand what's happening here.
 So those green lines, it looks like lasers shooting out
 of the feet or something like that.
 What are those?
 Contact forces, contact forces.
 So I can actually turn those on and off, the contact forces.
 That makes it a little cleaner.
 It's actually kind of cool too.
 If you want to see what the inertial properties of the robot
 are, you can visualize its inertial ellipses.
 If you want to see-- a lot of times the collision geometry
 that the physics engine uses is different
 than the original geometry.
 So you can see what the collision geometries of spot
 looks like.
 Those are the things that are going
 to cause contact forces between the robot and the world.
 But this is an interactive part.
 And it should work on every computer.
 It's just a browser-based thing.
 So no installation.
 Everything should just work.
 Please use the notes, interact with the notes,
 give me feedback on the notes.
 OK.
 We did the logistics.
 So the main goal for today is to tell you
 a little bit about what I mean by manipulation, which
 might not be what some other professors mean about--
 who do robotics mean by manipulation.
 I'll tell you my slant on it and my biases for it,
 give you some examples.
 I come from a bit more of a controls background.
 So I have a goal in this class to bring
 some of the rigorous thinking of control theory
 into the sort of wild west of robotic manipulation.
 So I'm going to tell you a little bit why
 I think that's important, the systems theory perspective,
 because that helps me tell you a little bit
 about the spectrum of things we're
 going to cover in the class.
 And then we'll just talk about some of the pieces,
 the core components that you'll have in most modern manipulation
 systems, and finish up with some of the broader
 goals for the class.
 So what is manipulation?
 So one of the important figures--
 sorry to walk on you there--
 one of the important figures in manipulation research
 was Matt Mason from Carnegie Mellon.
 He's got a beautiful review of robotic manipulation.
 It's a great thing to read if you're interested.
 He was very thoughtful in thinking
 about all the different ways you might define manipulation.
 So his first definition was manipulation just
 means activities performed by the hands.
 That could be tool use.
 That can be specifically picking up objects.
 It can be potentially very rich.
 And he goes through a series of different possible definitions.
 But maybe the most operative one for the class
 here, which is definition five, by the way,
 is that manipulation refers to the agent's control
 of the environment through selective contact.
 So that's kind of a nice way to think about it right now.
 My goal is to affect a change in the environment.
 I've got people, objects, whatever in the world.
 I want to apply forces in order to affect change.
 And that task is manipulation.
 So that's almost what I mean by manipulation.
 So the only thing I don't like about that
 is that kind of gives you this view of there's a robot
 and there's an object.
 In this case, it's holding a little red foam brick.
 And I just want to change the position of the brick.
 And that is good.
 That is correct.
 That is under the umbrella of manipulation.
 But it means way more than that when I think about manipulation.
 So this is manipulation.
 And this is way more than most robots
 can do with manipulation.
 This is what we teach our kids, to tie their shoelaces.
 But if you think about applying selective contact to the world
 in order to accomplish this change in state
 in the environment, that's tough.
 That's super rich dynamics and control playing out right
 there.
 And we want to embrace that.
 We want to sort of dig into some of the details of how
 would you build a manipulation system towards that.
 I can't actually offer a solution to that yet.
 Maybe next year.
 But even more so--
 so that's kind of digging into how rich maybe the dynamics
 and control could be.
 But even more so is that we want to talk about open world
 manipulation and with autonomy.
 So I don't want someone using a joystick
 to accomplish manipulation.
 I want the robot to be making its own decisions
 and understanding the world.
 So Matt's definition refers to an agent's control
 of the environment through selective contact.
 All true, but it's broader when you're in an open world.
 Open world is a term from video games.
 So basically, you don't know what the objects are
 going to be in the world.
 You keep walking.
 You keep getting more objects spawned in front of you.
 Or you just walk into the next room,
 and there's some objects you've never seen before.
 How do you make a manipulation system
 that's capable of reasoning about anything
 that could be in somebody's home or somebody's factory
 or whatever?
 And that requires a lot more than just dynamics and control
 the way we think of it.
 That requires a very rich perception
 and understanding of the environment,
 some sort of common sense understanding
 of what objects are.
 If I've seen an object--
 there's an object I've never seen before,
 but it's similar enough to things I've seen,
 I have a lot of intuition about how
 it's going to act when I start applying forces.
 How do you bring that into the robotic system?
 And then the ability to make long-term task-level plans
 and combine them all the way down to sort
 of fine joint-level motions.
 So if I need to make a bowl of cereal for my kid,
 I need to go to the closet and pull out the cereal box.
 I need to go to the fridge, move the pickles out of the way
 to get to the milk.
 I mean, this is like a complicated sequence
 of actions that are trying to achieve
 this physical goal of moving things around
 in the environment.
 You can get all the way to full intelligence just thinking
 about manipulation.
 And maybe it's the best way to think about intelligence,
 but that's my bias.
 So let me give you a couple examples now,
 I think, just of systems that maybe capture that.
 Often, these are going to come from the Toyota Research
 Institute.
 I have a second job over there.
 And the folks at TRI can build robot systems
 at a level of maturity, putting together
 all these components in ways that it's
 hard to do in academia.
 So when I try to show a full stack robot of things,
 I'll often turn to some of the videos
 that are coming out of TRI.
 So here's an example of a robot that's just tasked
 with loading the dishwasher.
 So it goes something like this.
 So Siwan here dumps random junk into the sink.
 There's also plates and mugs and spoons.
 And the robot's job is to do the dishes, roughly.
 So it's got to open the dishwasher.
 It's got to put the mugs in the top shelf,
 plates in the bottom shelf, the silverware
 in the silverware rack.
 Anything that's not a dish, it has
 to understand it was not a dish and throw it in the refuse pile
 over on the side.
 If you think about all of the pieces that
 have to go into making something like that work,
 that's, I think, closer to what I
 mean by the problem of manipulation.
 There's a lot going on there.
 Even at the low level--
 I'll show you some of the slightly nuanced things
 that are happening there.
 But this system used to run all day and all day, all night,
 and just do its best to load the dishes until someone
 dumped more dishes in, and then just keep going.
 Got to a very high level of maturity.
 So just zoom in on a few of the things
 that are happening that are actually super interesting.
 So first of all, you had to open the dishwasher door.
 That's non-trivial.
 If you get that wrong, you can jam your robot.
 Picking up silverware sometimes requires nudging it out
 of the corner because your big robot hand doesn't fit
 in the corner of the sink.
 Picking up plates is one of my favorite.
 You have to get your fingers in just right
 and understand what's a plate and not a plate in subtle ways.
 Here's the zoom in on the plates.
 This is a simulation, of course, on the left
 and the real robot on the right.
 It's been a lot of work on trying to make simulations
 match reality.
 That's partly what makes teaching a class like this
 really possible, the scale.
 And lots of you interact with lots
 of pretty complicated manipulation systems.
 Yeah, please.
 [INAUDIBLE]
 Yeah, that's great.
 So it depends on which version of the system you've seen.
 So the first versions we did were actually
 we would train perception systems that
 worked for certain plates, certain mugs, certain spoons.
 And then we started randomizing those and covering a big swath.
 But you could probably go to the Disney store in the mall
 and come home with a mug that I couldn't pick up
 with that perception system.
 So there's limits to how much diversity
 we could handle in that.
 And the deep learning revolution has brought us
 new capabilities in that space.
 But still, generalization is an open challenge.
 So that's always a great question to ask,
 sort of how general is this?
 Does this work for exactly one type of plate or every plate?
 And the answer is always something in between.
 This is a simulation of a more dexterous hand
 picking up those plates.
 And by the way, simulation like this
 didn't really work a few years ago.
 It's only very recently that computer game engine quality
 rendering has gotten good enough to train state
 of the art perception systems.
 And some of the physics engines have gotten good enough
 that you can actually do research in simulation
 that you expect to have work on the real robot.
 And then the Boston Dynamics guys, they kick the robot.
 It's hard to kick a dishwasher.
 But that's kind of what we were going for there.
 So even if someone closes the dishwasher rack
 and the robot was working on a mug,
 it's like, all right, I'll put the mug down
 and I'll open the dishwasher rack again.
 You can almost sense the robot being annoyed.
 But it'll go pick it up.
 But it'll get the job done most of the time.
 So really, this quest of understanding manipulation
 goes from low level control all the way up
 to sort of scene level understanding, task level
 planning.
 There's really a broad level of intelligence, if you will.
 You can go from the hind brain to the cortex or whatever.
 And we'll explore the swath, the different rungs of the ladder.
 Maybe that analogy is going a little too far
 throughout the semester.
 And for your project, you'll pick some level
 and try to dive in a bit deeper and focus in on those.
 The world gets even more open.
 So one of my reflections on teaching the class last year
 was that people did awesome projects.
 But they were all a robot bolted to a table
 and a hand doing certain objects.
 Because that's all I had given people to work with.
 So that limits your thinking, I think, a little bit.
 Because the world gets even more open
 if you can walk into the next room and see brand new objects.
 So one of my goals for the term is
 to bring more mobile manipulation into the discussion
 and into your projects.
 So this is an example, again, of a full system
 from TRI, the Toyota Research Institute.
 And it's actually-- they have a partnership
 with a local grocery store.
 And at night, they just take hundreds of items,
 make a random shopping list, and go get the groceries.
 And they drop groceries every once in a while.
 They log their failures.
 And they just get better and better and better
 as they work on this more and more and more,
 getting to solving real systems level tasks with a robot.
 But going into an arbitrary grocery store
 and dealing with the diversity of ad inventory,
 that's bigger than the kitchen sink.
 And you could have a new stocked item
 that you've never seen before.
 And if you go out of the grocery store,
 it might be even more complicated still.
 So again, Spot is a robot from Boston Dynamics.
 It's got an arm on top of it often.
 And like I said, one of our goals
 this year is to do more mobile manipulation.
 And so just for grins, I brought Spot.
 It's hiding back here behind the door.
 We'll do a little mobile manipulation challenge
 real quick before the batteries run out.
 Yeah.
 Just got to wait for the motors to power on real quick.
 But this is Spot.
 I'll do my best to do live demos in the class.
 It's a lot of work, and I'm far away from my buildings.
 But this one can walk over, so that was pretty good.
 All right.
 I brought one of Spot's toys here
 just to show a very basic demo.
 But robot dogs love robot plush toys.
 So why not?
 We'll just walk up and ask him to pick up.
 Now, depending on how I do this, it may or may not
 show a failure mode.
 But let's see.
 All right.
 Success, huh?
 Go back and carry it back.
 [VIDEO PLAYBACK]
 All right.
 Good job, Spot.
 Yeah.
 [APPLAUSE]
 Now, because I want Spot to walk back home,
 I'm going to power it down now.
 OK?
 Otherwise, the batteries are going to die on me.
 You can carry it if you want to.
 But I'm recruiting you if--
 I don't know exactly the weight.
 70 pounds, something like that?
 It's a lot of battery weight.
 It's also loud, so I have to fully power it down.
 Cool.
 Maybe I should have put it somewhere less
 in front of my board.
 [LAUGHTER]
 Over here, Spot.
 All right.
 [LAUGHTER]
 OK.
 So some of you have taken Underactuated with me.
 It's the other class I teach.
 It's a graduate class.
 If you haven't, maybe you'll take it next.
 Or some of you might just know that I've done
 sort of this controls background.
 So let me tell you sort of how I came into manipulation
 and a little bit what's the dynamics and controls
 perspective.
 I already showed you the shoelaces.
 That's like awesome dynamics and control.
 But there's some really important things
 that happen which make manipulation a challenge,
 like the thing I want to focus my work on controls on.
 So I've done work on humanoid robots, on walking robots.
 This is the early version of the Atlas robot
 from Boston Dynamics.
 You've seen the new version doing parkour
 in awesome fashion.
 And that's a real thing that really works beautifully well.
 This was a DARPA robotics challenge a handful of years
 ago where we had to program the whole stack to make it
 cross stairs, open doors, turn valves, and things like this.
 That was really the first time that we
 had to do some amount-- that I really started thinking
 about some amount of manipulation.
 And it was a relatively closed world.
 We had to turn various valves.
 But they were all in a small family of valves.
 And we would make a valve detector.
 This was actually right as the deep learning thing
 was happening.
 So most people were not using--
 I think nobody was really using deep learning that year.
 Next year, everybody was using deep learning.
 But OK, so-- and the way you would program a control system
 like that is awesome.
 I mean, you can make these balancing controllers that
 are super robust, even if people are jumping on the vehicle
 while you're trying to get out with one foot.
 And that stuff works really well.
 And it's built on this premise of you
 kind of go through the world.
 And you understand the world in a sort of geometry sense
 for walking.
 You have to sort of understand where you're allowed to step,
 what you shouldn't run into.
 But you can do a lot in locomotion
 with a relatively limited understanding of the world.
 You don't have to understand--
 I mean, if you're walking on some obstacle course,
 then it's more.
 But if you've just got to understand, yeah,
 I'm allowed to step there or I'm not allowed to step there,
 that's a relatively limited amount of understanding
 you have to do for the world.
 When you look in the kitchen sink,
 the natural extension of that is to try to say,
 I'm going to write a mug detector.
 I'm going to figure out where the mugs are,
 what's the pose of the mugs.
 Those highlights are key points of an output
 from our pose detector of the mug.
 And then you can start building a control system that's
 trying to work a lot like Atlas when it's balancing on the car.
 But now you're trying to regulate
 the positions of the mugs.
 And it's hard, but you can grow in that direction.
 But then you start saying, OK, what
 are all the mugs in the world that I haven't seen before?
 And how do you write a control system that
 accomplishes work on all the mugs in the world?
 And that starts to challenge my views of control
 from a few years ago.
 So the standard thing of modeling the world,
 having a perfect physics model, building a control system
 to stabilize that model, doesn't really
 map directly to any mug.
 So we had to start changing the way we thought
 about state, about representation.
 I'll make those all more precise as we go.
 Because the control problem in manipulation,
 the control problem for Atlas doing parkour
 is really about Atlas.
 The robot needs to understand its dynamics very well.
 And you can sort of master your dynamics
 and do incredible things.
 But the control problem for manipulation
 is not just controlling the robot.
 It's also controlling the objects in the world.
 And that means you have to understand potentially
 everything.
 And it's weird that tying shoelaces or even picking up
 objects in some ways is harder than doing backflips.
 But they just exercise different parts of your brain
 and of the robot's brain, I guess.
 So the state of the robot is part of the problem.
 But the stabilizing the state of the environment,
 however you choose to represent that,
 is another part of the problem.
 And when you get to interesting manipulation tasks,
 you think, how am I going to represent that?
 If I read a piece of onion detector,
 and then the number of onions is changing every time,
 and this sort of just takes my classical understanding--
 not even that classical-- my modern understanding
 of control and challenges it in fundamental ways.
 I don't know how to build world models of that that
 are trying to estimate the shape and the pose of all the pieces
 that would scale to a simple problem,
 even though that should be not that hard of a problem.
 So that's how I came to be super excited about manipulation,
 is that I think it breaks--
 I mean, we can do amazing things with control.
 We can do backflips.
 We can do these things.
 But we can't do that.
 Why can't we do that?
 Humans do make it look easy.
 It should be easy.
 So that's my primary interest.
 Now, the world is making super fast progress
 in that dimension.
 And one of the ways that people are doing it
 is by using deep learning.
 But most essentially, they will have potentially big perception
 networks, so you can go directly from an image
 to make your decisions.
 But I think the most essential thing is that they
 have intermediate layers that are
 learning implicit state representations of the world.
 So instead of having a mug detector
 or having an onion piece detector,
 the intermediate layers of the neural network
 are able to learn some more fundamental representation that
 does scale and maybe doesn't have
 to track every piece of the onion
 if it's not task relevant.
 And these notions of new learned state representations,
 I think, are fundamentally important
 in how we have to think about the control problem.
 Now, we'll talk a lot about visual motor policies.
 That's the buzzword for it.
 And we'll talk about different approaches to it
 when we get there.
 But that really does make a difference.
 And so we started to see control systems that could really
 go directly from RGB images, solve dynamics and control
 problems that were non-trivial--
 flexible hats hanging on a rack.
 These are visually cluttered scenes.
 Any shoe.
 And solving them with some of the robustness
 we associate with humanoid balancing, for instance,
 but doing it in this more open world setting.
 So that's where the research world is just
 exploding right now.
 And of course, large language models
 are coming in to add that whole component too.
 So I think this is going to change the way we think
 about control for everything.
 And that's why I'm passionate about pursuing it.
 Just another zoom in of that example,
 just to do a non-trivial but sort of starts
 in the land of we can understand what a plate is.
 We can understand the dynamics of a plate.
 We can write a control system that
 sort of thinks about the plate.
 But when you start saying that the primary sensor is a camera
 and you start saying we need to deal with all
 the visual diversity and all the different plates
 and all these things, then it gets really rich.
 And that's where deep learning has opened things up for us.
 It doesn't mean, though, that you
 have to throw away fundamental basic understanding.
 And I will still advocate for making darn sure
 when you build your big complicated systems
 that you understand how it works in a simple model that
 is based on just physics and removes that complexity.
 If you don't understand how the big complicated thing boils
 down and works in the simple cases,
 then I don't think you fully understand what's happening.
 So that's just a simplified version of that same task.
 Having said that, we can now do things
 that would have defeated state representations
 that we had before.
 So how do you design a dough detector?
 Or how do you roll the pizza dough?
 But these tasks are relatively easy for some
 of the deep learning approaches.
 And we're trying to figure out how to put all this together
 and to make really complicated systems.
 It's actually surprisingly robust.
 You can mess with the dough.
 It'll just get it done.
 Just keep rolling until it's happy.
 You can perturb the dough.
 It'll keep going.
 You can spread sauce.
 That's another one.
 We had a list of things that I didn't know how to do,
 of what's the state of the sauce.
 I don't know what the state of the sauce is.
 But we can now--
 even though the task looks simple, it should be simple,
 from a controls perspective, this
 would have been extremely hard.
 And the fact that we're starting to be able to do this
 is just awesome.
 That's kind of a high level motivation.
 Any questions about that?
 I want the class to be interactive.
 I know it's big, which is great.
 But feel free to ask questions.
 Yeah?
 [INAUDIBLE]
 Yeah.
 [INAUDIBLE]
 Yeah, that's great.
 So it's a big question.
 So how exactly are the deep networks figuring out
 the state representation?
 And oftentimes, maybe a version of that question
 is, is there even an explicit notion of state?
 Is there a point in the network where you point and say,
 that's the state?
 And I'd say, in some approaches, yes.
 There's an explicit, pre-trained something
 that tries to come out with an explicit latent vector
 that I would say, that's the state.
 In other cases, you'd train the system end
 to end, where the output is really motor actions
 or long-term actions.
 And then the notion of state is only
 implicit somewhere in the layers of the network.
 And you don't typically even go and find it.
 So that's a choice that you can make.
 [INAUDIBLE]
 I don't do it.
 The neural network does it.
 So the question was--
 [INAUDIBLE]
 Yeah, so the question is, how would you do a liquid?
 What's the state of a liquid?
 I mean, the fluid dynamicists have an answer.
 You could do Eulerian, or you could do Lagrangian.
 You can track particles.
 You can track whatever.
 And that's not practical if you just
 want to pour spread sauce.
 So in that particular example, there
 were a lot of demonstrations of humans teleoperating
 the robot to spread the sauce.
 And we said, when you see this picture,
 you want to take similar actions.
 And somewhere in the middle, it's
 deciding a representation for sauce.
 And I think if we want to generalize that,
 maybe that can go the distance.
 Or maybe we have to go in and understand that representation
 and apply it in a more general way.
 Yes?
 [INAUDIBLE]
 That's a super good question.
 So the question is, would you--
 I'll just repeat it for the folks watching.
 Yeah, so the question is, if we know a lot of things
 about what liquids can do from physics,
 do we incorporate those in the neural representations?
 Or is the neural network just ignoring that?
 And I think the simplest answer is
 we're almost always ignoring it.
 But there's a very active field of research
 of trying to do physics-inspired neural networks
 and trying to bring in biases from, hopefully,
 some of our knowledge about mechanics, for instance,
 or in other fields where you have some other knowledge
 into the neural network.
 Now, the trade-off there, Rich Sutton
 would say, any time you inject your understanding of the world,
 then you've corrupted the system and you've
 limited what it's capable of learning.
 But you probably also made it more data efficient.
 So there's a trade-off there.
 Awesome.
 Great question.
 So I want to start digging in, unpacking
 what are the different pieces of a manipulation system
 and how we're going to think about them in this class
 because I think about things through the lens of control,
 even perception is just a control problem.
 So how many people know Ross?
 You don't have to know Ross.
 Don't feel bad if you're not putting your arms up.
 I'm just trying to understand.
 I don't assume people know robotics.
 But if you've done robotics, then you've probably
 touched Ross, the robot operating system.
 You pick.
 You pick your threshold of whether you
 have your knowledge of Ross.
 But if you've been exposed to Ross,
 then you have a sense for what the anatomy of a manipulation
 system could look like.
 So what is Ross?
 Ross is, I think, one of the best things
 that happened to robotics.
 They call it an operating system.
 That's kind of a misnomer.
 It's really-- well, it's a way of packaging
 different components of a manipulation system
 that run on a proper operating system.
 And they pass messages back and forth.
 And it allowed us to sort of break up the manipulation
 problem into a more modular approach.
 So you might write a Ross node that could be my camera
 driver, for instance.
 So I buy a camera from Intel, let's say.
 It has some low-level software that talks to it.
 And somewhere, it gives you an image.
 So I could make a little Ross node--
 in fact, people have done this before me now--
 make a little Ross node, which a Ross node is an executable,
 a process that you'd run.
 It could be a Python script or whatever.
 But it's somehow something that runs
 as a process on your computer and starts
 sending messages that have the information you
 need from that driver for the rest of the world
 to think about.
 So maybe from the driver, it's just
 going to spit out RGB images.
 In robotics, we often actually use depth cameras.
 So they have red, green, blue, but also depth coming out.
 And it just says, I'm going to send packets
 on a network protocol that contain these images.
 And I'll define exactly the spec in a general way.
 And then you write a different process, a different Ross node
 that might be my perception system.
 And the perception system is a different executable.
 You run it.
 It's kind of a pain.
 When you're working with Ross, you
 have to start lots of processes every time
 you want to run the robot.
 But that's how we go.
 So maybe I'll start a different process that starts listening
 for RGB images and puts out, for instance,
 what's the position or the pose of the mug.
 Like I said, that's probably not going
 to take us the whole way.
 But that's a simple example for now.
 And maybe the pose of the mug goes to some other planning
 system that listens for the state of the mug
 and maybe also the state of the robot, for instance,
 and tries to decide, how does my robot
 need to move through the world in order
 to manipulate the mug?
 And it'll pick out some joint trajectories, for instance.
 This is just one version of that.
 The deep learning version could look different, for instance.
 And then maybe I've got my robot controller here
 that thinks about commands of where the robot wants
 the hands to be or the arms to be
 and turns that into motor commands.
 And then there's some other motor driver
 that shipped from the company that you bought motors from.
 And what's important, the reason I wanted to just take a second
 to write that down, is that ROS really did an amazing thing.
 It helped us start to modularize and compartmentalize
 some of the complexity of building a big system like this.
 And in particular, it made it easier to share.
 It was an open source project, one
 of the great open source projects in robotics.
 And it started an entire ecosystem
 of people who could say, well, maybe I'll
 download a perception system from Carnegie Mellon.
 And then I'll get my robot controller
 from the German Space Agency.
 They're really good at that.
 And then I just want to write a planning system.
 So I'll focus my attention on writing a really good planning
 system.
 And then if I succeed and write a really good one,
 I'll put it up in ROS.
 And everybody else can download it and work on their perception
 systems or whatever.
 And this sharing that happened because of ROS
 was just fantastically good.
 But also, the modularity that happened with that
 was important and special.
 So the fact that these are running
 as different executables and the only connection
 was this message type was sort of fundamental.
 As long as I could compile your code,
 if you could have been using C++, I was using Java,
 you were whatever, somehow the ecosystem
 became much more friendly.
 In fact, if you wrote yours in Windows
 and I wrote mine in Linux, Linux is better.
 You should write it.
 But even if you wrote it in Windows,
 I could write a Docker image and just run my ROS process
 in Windows.
 And then I can put that all together and bundle it up.
 And maybe, let me say, you wrote yours in Ubuntu 14
 a long time ago, right?
 And I wrote mine in 22.
 And I don't want to run Ubuntu 14 anymore,
 but I still want to run your perception system.
 So I'll just put a Docker container around it.
 OK.
 So a little side note.
 I have an interesting challenge of talking
 to people that are at many levels of the spectrum here.
 Some of you are robotics experts.
 Some of you are never doing robotics yet.
 This is great to start.
 So I'll drop in some lingo somehow
 that maybe not everybody understands.
 I do that somewhat intentionally.
 I apologize if I do it too much.
 I hope that the folks that are experts
 get a little bit out of it.
 But I hope that it doesn't--
 I will try to be conscious to never drop language
 that you must know to understand the next concept.
 So if I say, if you don't know what versions of Ubuntu are,
 that's fine.
 That was just a fun thing to drop.
 OK.
 Good.
 So this is an extremely powerful way
 to think about the complexity of building a big modern
 manipulation system.
 And really, this is a canonical standard.
 We're going to build a separate perception system, because
 there's some research groups that
 are really good at perception, a separate planning
 system, a separate robot control.
 It's not clear that that's the right architecture.
 And it's easy to point to examples
 of that distinction is a limitation.
 And we'll challenge that later.
 But that's as a starting sketch.
 This is sort of the sense, plan, act paradigm
 in artificial intelligence.
 And we don't love it, but it's been useful.
 OK, so let me just contrast that with, let's say,
 this is the raw software engineering view of the world.
 So as long as you write a program that when I run,
 it starts listening to messages, it
 could be doing anything it wanted internally.
 It could be calling random number generators.
 It could be mining Bitcoin.
 As long as it's spitting out the pose of the mug at 10 hertz,
 I'm good.
 I don't have to know anything about what's inside.
 OK, let's contrast this with the control view
 of the world, which makes similar diagrams,
 but asks a little bit more about,
 I want to know what's inside your diagrams.
 So a standard approach in control,
 a control system theory is, if you've ever
 used Simulink or Modelico or a handful of these tools,
 LabVIEW even is a bit like this.
 There's something called model-based design,
 which says if you've got a big, complicated system
 and you want to think about it rigorously,
 then you should start by encapsulating that complexity
 in a block diagram.
 So maybe I have my actual robot here,
 and it's a thing that takes in motor commands
 and puts out sensor information.
 Maybe it's a simulator.
 Maybe it's the robot.
 But I have some sort of model of this.
 Now, Ross really pushed the notion--
 sensor-- Ross pushed the notion that these communication
 channels should be network message passing.
 But before, these were signals and systems
 in block diagrams in controls.
 And you could compose these signals and systems
 into more complicated block diagrams.
 And I think they're two sides of the same story.
 And let's just compare and contrast them.
 So if I'm in Simulink or something like this,
 then in order to describe my robot,
 I'm going to use the language of differential equations
 typically, or difference equations,
 in order to describe what's happening inside this box.
 It could be really complicated difference equations,
 but they're probably not mining Bitcoin.
 You could.
 I guess you could.
 So inside here, I'll say it's not an arbitrary executable.
 It's not anything I could write in software.
 It's going to be some sort of difference equation.
 So here, x in this example would be the state of the robot,
 for instance.
 It could be the positions and velocities of the joint.
 u is typically used in these frameworks as the input.
 Which is, in this case, the motor commands.
 So I would have a difference equation
 that takes the state of the robot.
 Potentially does some complicated physics,
 in this case, or perception in some other case.
 Tells me how the state of the robot evolves on the next step.
 And then I have some other function
 that tells me how to generate the sensors
 from the state and the input.
 This is the language of difference equations.
 And you've seen it maybe in 18.03
 if you were an MIT undergrad or are an MIT undergrad.
 You probably saw simpler versions of it.
 You might have seen it as--
 this is a state space difference equation.
 Maybe you saw it first as a linear difference equation
 with linear algebra involved.
 Maybe if you took an intro controls course,
 you would have seen the state space version that would have
 included control inputs.
 Something like that.
 This is just the nonlinear generalization
 where f can be an arbitrary function.
 And what's interesting is that I would
 claim that almost everything you could write in your ROS--
 that you would want to write in your ROS ecosystem
 for controlling a big complicated robot
 could be described carefully with difference equations,
 differential equations.
 And in particular, writing it this way
 says when we're combining systems together,
 we're not only going to agree on the message pass, the message
 type that's passed.
 We're going to say something-- this embeds a timing semantics
 pretty firmly.
 So we have to say something about how often this
 is getting updated.
 We have to declare our state.
 So it's not going to be arbitrary.
 It's going to-- we have--
 for instance, if I ever wanted to rewind my simulation
 or my controller or whatever and just play it back in time,
 you can do that if you've declared state.
 But if you're doing arbitrary computations,
 you can't do that.
 And these functions can be complicated.
 That's not really a limitation.
 But it does ask you to sort of not just
 agree on the message passing, but agree on somehow
 that we're going to use difference
 and differential equations to describe our systems.
 In some cases, like the camera model, in simulation,
 the thing that takes the state of the world
 and puts out RGBD images, that's a full-on game renderer.
 That's a photorealistic renderer that I'm
 writing in this function g.
 So that's not what you saw in 18.03.
 And it's complicated.
 And you might not want to--
 there might be some questions you
 wouldn't want to ask about that big, complicated g.
 But it still fits in the intellectual framework.
 And I'm going to advocate, both for research
 and for the pedagogical value of having things in class,
 that we--
 every time we build our systems, we do a little bit more work.
 And we use the difference and differential equation
 view of those systems.
 And that's going to allow better things
 to happen downstream.
 Like I said, you could rewind your simulation.
 You can do Monte Carlo testing, if you want.
 There's a lot of things that come just
 from having taken a little bit more work every time
 you build each component.
 I also don't want to help everybody install ROS.
 That's really not a fun way to teach a class.
 So we're going to just give one software thing that
 does this version of it.
 But it's modular in the same way that ROS is.
 But it's using the language of difference equations.
 These functions really do get complicated.
 And like I said, the fact that g is a renderer that's
 good enough to train a perception system.
 Or a neural network can fit in this framework, too.
 So one of these systems might be a neural network.
 They could take, for instance, RGB images in, or RGBD images,
 and spits out, let's say, a pose of the mug.
 But that's not the final story.
 So let's just think for a second.
 What if we put a big neural network in here?
 Is it useful to think about neural networks?
 I want to think about neural networks with PyTorch
 or something.
 I don't want to think about it as differential equations.
 That's not true.
 I want to think about it as differential equations.
 And I want to slowly convince you
 to think about it as differential equations.
 So if you're doing a feed-forward neural network,
 then you could write that as a simple function that
 has no state and just is a big--
 this is my neural network function
 that just takes inputs and outputs,
 the output of the network.
 If I have a recurrent network, then I do have a state.
 And I should tell my system about it.
 And that way, if I ever wanted to rewind the progress
 of my LSTM, for instance, or your favorite recurrent
 neural network, then I could declare that state.
 And I should declare that state if I'm
 trying to make more rigorous understanding
 of the complicated system.
 If you're a transformer, then maybe you just
 have really big input tapes, or you call this your buffer.
 But it all fits.
 Transformers fit fine into this framework.
 [AUDIO OUT]
 So I'll show a few examples of that here.
 But the software that we're going to use,
 because that's what I've been working on a lot,
 is called Drake.
 So a lot of people ask me what Drake is.
 And I think the simplest answer--
 how many people know Harry Potter?
 You know what a Horcrux is?
 I think Drake is kind of my Horcrux, probably.
 That's probably the most honest answer I can give you.
 It's kind of me trying to put my soul into a piece of software.
 And I've had some incredibly talented engineers
 that have been working on it and making the dynamics
 engine really good.
 And it's a passion for me, about trying
 to take some of this differential equation,
 difference equation view of the world,
 and make it solve extremely complicated physics
 and controls problems.
 And I think it's good enough to be used in the class.
 And it'll run in your browser with no installation
 and all that other stuff.
 So I won't dive too much into it today.
 But we'll try to work you through.
 I've gotten feedback over the years
 that actually a little bit more tutorials about Drake
 early on would help people with their homeworks
 and certainly help people with their projects.
 So we have a few problems that sort of
 ask you to explore the software a little bit, too.
 But it's really meant to build up three things.
 One of them is this modeling dynamical systems.
 So if you want to take this difference equation description
 and build really good robot models, really good
 neural network models, and this, whatever, then you can do that.
 And it has a block diagram language
 that you can say I'm going to add this system.
 I'm going to add this system.
 I'm going to connect the output port here to the input port
 here.
 And it's not only a toolbox for writing these,
 but it's a big collection of the systems
 that have already been written for you,
 so a big collection of different controllers
 and dynamical systems and the like.
 A lot of those dynamical systems in my world use optimization.
 So there's actually a really nice optimization library
 attached and built into Drake.
 So if you want to write a controller
 by solving a small convex optimization problem or whatever,
 that's very possible in Drake.
 That's kind of what it was built for, too.
 And then there's a lot of specific tools
 when you get into the physics engine of working
 with the dynamics of the system, solving kinematics problems,
 solving dynamics problems, asking
 what's the center of mass of this really complicated thing?
 What is the Jacobian of some strange quantity?
 It's really good at those kind of things.
 And it provides this level of abstraction.
 So the signals and system-- there's a lot of tutorials,
 by the way, online if you ever get stuck.
 A lot of people, I think, don't go back and look
 at the tutorials, but you can.
 And it's perfectly compatible with ROS.
 If you want to have your Drake diagrams live in your ROS
 ecosystem, that should just work, especially ROS 2.
 So let me give you an example here of Drake's version of this
 is you model these systems with difference or differential
 equations.
 And you can do that for very simple things,
 and you can do it for very complicated things.
 But if you get to the robot level,
 then we're going to see complicated systems here.
 So we have a particular system that you'll use a bunch.
 We're calling it the hardware station.
 Basically, every physical robot that you want to work with
 sort of have a digital twin, if you will,
 that's described in a YAML file and describes
 not only the physical elements of your robot
 and the objects it will interact with,
 but also the drivers that are on that robot.
 And that gets packaged up and provides encapsulation
 into one big system, which, for instance,
 if I have a KUKA IWA robot--
 we'll talk about our specific robots,
 but there's a particular type of robot called an IWA, I-I-W-A,
 and a particular type of gripper we'll use a bunch
 called the shunk WSG.
 So if you want to unpack these, if I
 take a particular hardware station
 and it has a IWA in it and a WSG,
 and the IWA driver is looking for a position command,
 possibly a torque command, feed forward torque,
 and the shunk driver is listening for a position
 command, maybe a force limit, those
 are the commands that the software
 we get from the manufacturer provide.
 And the Drake abstraction of this
 will give you input ports that are signals that
 are waiting for those commands and will send it
 to either a simulated version of the IWA and the shunk
 or the real hardware, depending on-- you just flip a switch
 and it'll change.
 And inside this, it'll do the message passing,
 just like in ROS.
 And on the other side, the IWA driver
 spits out a lot of different things.
 It says what position it got commanded, position measured,
 all these things.
 But this abstraction is exactly provided
 by the drivers and the real robot.
 That's the boundary layer.
 And by encapsulating it with the signals and systems
 of this block diagram, I can change the implementation
 underneath.
 But if you've written your control system just expecting
 to talk here, the same way in ROS,
 I have a very modular approach.
 I can do things like flip a switch
 and I've switched from simulation to the real robot.
 In particular, if you do want to convince me
 to run your code on the real robot at the end of the term,
 there's actually two steps we'll do.
 First, we'll make it run all in one simulation.
 And then we'll say, run on a remote robot.
 But I'm not going to turn on the robot.
 I'm going to turn on a simulator on another computer
 and make sure that it works on a remote simulator
 with all the message passing and nothing blows up.
 And then, if you're not going to break the robot,
 then we'll turn off that second simulator
 and turn on the robot and it'll all just work.
 OK.
 So in software engineering, you can sort of
 think of it as object-oriented programming.
 Classes provide levels, tools for encapsulation
 and abstraction.
 And when used properly, they can allow
 you to build incredibly complicated systems.
 In the dynamical systems world, which
 we are living in in the class here,
 it's signals and systems.
 Systems are what provide that similar level of encapsulation
 and abstraction.
 You tell me your inputs and your outputs,
 what's happening on the inside with differential equations,
 and we can do similar--
 build big, tall towers of complicated things.
 If you zoom inside that, then you'll actually
 see a lot of complexity.
 So when you're in simulator mode, for instance,
 that hardware station, abstraction
 is actually, in itself, a diagram.
 And if you look inside, what's happening with the EWA
 position, it's actually being fed
 to another system, which is an inverse dynamics
 controller, for instance, which then goes
 to our dynamics engine, which we call multibody plant.
 That's the physics engine.
 And it also goes to something we call the scene graph.
 You'll see you'll explore these things,
 but this is the rendering engine.
 And there's a variety of different systems
 that are inside here that ultimately puts out
 my ability to put a simulated camera image.
 And you can nest these diagrams and build up
 more and more complicated abstraction.
 I'll run a quick example here now.
 So this is the first intro notebook in the class.
 I hope you run it.
 I hope you play with it.
 You see that it'll always open up a browser.
 This is Meshcat, our browser.
 You don't have to install anything to run this.
 Even the notebook will run in the cloud.
 And you have a relatively simple script that describes--
 I'm going to add a model, which is the EWA.
 And it's described in this particular format.
 I'm going to add a different model that
 is the Shunk Gripper that's described in this place.
 And it's just a series of simple things.
 I'm going to add a brick to the world.
 And then I wire it all up.
 I get a simulator now that I can interact with and run and pick
 up the brick.
 The YAML is the description for that.
 And then it parses into a signals and systems thing.
 I hope you all just go home and run that and try it.
 Any questions about that?
 Yes?
 EWA is a strange name, all lowercase.
 I always want to capitalize it because it's a proper name.
 But no.
 The manufacturers call it EWA.
 And it's exactly that robot that was on the screen here.
 So KUKA is the manufacturer.
 And they build lots of robot arms.
 And one series is called the EWA, I-I-W-A.
 Thank you for asking this, by the way.
 And in particular, there's two that we'll use in class.
 There's an EWA-14 or an EWA-7, which refers to the payload
 that they have.
 And that's something you could buy.
 It's yours for only $85,000 or something like that.
 Or you can simulate it for free.
 And the drivers for that provide this abstraction
 that's to take position commands in or torque commands in
 and go through it.
 Our next lecture, we're going to talk through robot hardware.
 And you'll see how EWA compares to some of the other robots
 that are out there.
 Yes?
 [INAUDIBLE]
 We were going to put safety filters.
 He asked if he's worried about breaking my--
 or I'm glad you're worried about breaking my robot.
 We'll put the proper safety filters when we get there.
 Did you have a question, sir?
 [INAUDIBLE]
 Yeah, we don't want you to break the arm.
 Yeah.
 [INAUDIBLE]
 Yes.
 [INAUDIBLE]
 That's a really good question.
 So the abstraction should be exactly what you see
 if you were to turn on the robot.
 So the robot drivers output the EWA position and stuff
 like this.
 But if you have a camera in there,
 then they put out an RGB image.
 So inside here is the physics engine
 that is loaded with all of the objects.
 And the camera is in order to be able to render
 RGB images, which contain the mug or whatever.
 It has to be living in the physics engine
 here in order to provide the same abstraction which
 if I turn the robot on, yeah, it has there.
 Now, some of these orange ports, we call those cheat ports.
 If you use-- if you pull on the body poses,
 for development purposes, we give you
 access to the ground truth pose of the mug.
 But if you switch to hardware mode,
 that port is not going to be available.
 So when you're in simulation mode,
 you can get oracular information about the internal state
 of the world, which won't be available more generally.
 Yeah?
 [INAUDIBLE]
 Yeah, so ROS is an ecosystem where
 you make the different packages.
 Drake could be any one of-- you could write any one
 of your systems in Drake.
 Or you can try to write the entire thing in Drake
 and not use message passing.
 So ROS is really the communications layer,
 which enabled modular thinking.
 Drake similarly enables modular thinking because
 of the old controls lineage.
 [INAUDIBLE]
 One can use ROS without Drake.
 But you shouldn't.
 [INAUDIBLE]
 I'm just teasing.
 Yeah.
 [INAUDIBLE]
 That's not true either.
 So the question is, can you use Drake without ROS?
 You can run your entire simulation,
 and we will for most of the class, just to keep
 the complexity down, all in one process.
 And then if you want to run on hardware,
 and you've got a ROS driver, then you
 put one of the systems that you put inside here--
 so if I run it in hardware mode, if I just flip a switch,
 then this system--
 we call it the hardware station interface--
 takes the same inputs and outputs.
 But inside here is a ROS sender, ROS publisher.
 And ROS receivers, which handle the network messaging,
 provide the same input and output abstraction
 in the signals and systems world.
 And that's actually even right down here.
 That's the hardware station interface.
 It just won't have any of the cheat ports
 because the drivers can't give you that.
 Is that an answer?
 Yeah.
 [INAUDIBLE]
 Yeah.
 [INAUDIBLE]
 Yeah.
 [INAUDIBLE]
 Yeah.
 That's beautiful.
 So the question is about multiple robots use cases.
 If those robots need to interact on the physics level,
 then you actually make one hardware station that
 will have EWA1 position, EWA2 position, whatever.
 And then that way, the physics engine
 can have forces interacting and collisions and everything
 like that.
 Yeah.
 [INAUDIBLE]
 Yeah.
 [INAUDIBLE]
 Yeah.
 [INAUDIBLE]
 Yeah.
 [INAUDIBLE]
 Yeah.
 [INAUDIBLE]
 Yeah.
 [INAUDIBLE]
 Yeah.
 [INAUDIBLE]
 Yeah.
 [INAUDIBLE]
 Yeah.
 [INAUDIBLE]
 Yeah.
 [INAUDIBLE]
 Yeah.
 [INAUDIBLE]
 Yeah.
 [INAUDIBLE]
 Yeah.
 [INAUDIBLE]
 Yeah.
 [INAUDIBLE]
 Yeah.
 [INAUDIBLE]
 Yeah.
 [INAUDIBLE]
 Yeah.
 [INAUDIBLE]
 Yeah.
 [INAUDIBLE]
 Yeah.
 [INAUDIBLE]
 Yeah.
 [INAUDIBLE]
 Yeah.
 [INAUDIBLE]
 Yeah.
 [INAUDIBLE]
 Yeah.
 [INAUDIBLE]
 Yeah.
 [INAUDIBLE]
 Yeah.
 [INAUDIBLE]
 Yeah.
 [INAUDIBLE]
 Yeah.
 [INAUDIBLE]
 Yeah.
 [INAUDIBLE]
 Yeah.
 [INAUDIBLE]
 Yeah.
 [INAUDIBLE]
 Yeah.
 [INAUDIBLE]
 Yeah.
 [INAUDIBLE]
 Yeah.
 [INAUDIBLE]
 Yeah.
 [INAUDIBLE]
 Yeah.
 [INAUDIBLE]
 Yeah.
 [INAUDIBLE]
 Yeah.
 [INAUDIBLE]
 Yeah.
 [INAUDIBLE]
 So Drake tries to be actually-- so Drake started as a research
 project in my group.
 It grew when Toyota Research started.
 It became a professional software project.
 And now it's actually used by lots of companies also.
 Companies are much more conservative than academics
 about licenses and everything like this.
 So we use third-party tools when we can.
 But we're pretty strict about the licenses.
 So that does limit us to some extent.
 If someone GPLs their code, I won't use it in Drake,
 for instance.
 So we provide most everything you need,
 sometimes through third-party libraries
 that you shouldn't have to think about.
 And the course, all the perception planning and control
 stuff will live inside--
 will be available for you.
 You might find something that we can't do yet.
 But we can get through a lot of pretty cool stuff in the class
 with the provided functionality.
 PyTorch, for instance, you can make a thin wrapper
 around PyTorch, for instance.
 We didn't re-implement PyTorch.
 Right.
 [INAUDIBLE]
 Gazebo is the simulator component
 in the ROS ecosystem.
 It's the most famous one.
 Other things can plug into.
 Gazebo did a lot of important things.
 It helped us find different description formats
 and everything like that, and can import the scene
 and similarly send ROS messages.
 Drake could be used instead of Gazebo.
 We've been talking to those folks
 about possibly having Drake be the physics engine inside
 Gazebo.
 The relationship is-- they're solving
 similar parts of the problem.
 But you could just--
 we don't use Gazebo in the class.
 And we can do all the things that you would want to do.
 [INAUDIBLE]
 So for the class--
 so you can choose to download and install locally.
 It's actually probably a better experience in terms of like,
 you can use your local IDE, your software development
 environment.
 But for the class, we have it all using DeepNote.
 So it'll just run on a Python notebook in the sky,
 and you don't have to install anything.
 That way, no matter what people have for their computing
 devices, it'll work.
 But it'll typically-- a lot of things
 will run on a single core, unless you get fancier
 and run locally and turn on multithreading and stuff
 like that.
 There are limits to what we can do on DeepNote.
 Awesome questions.
 Thank you, guys.
 OK.
 Let me think about what I still have time for.
 OK, so I think that was a fair representation of the sort
 of why I'm trying to think about the complexity of manipulation
 through the signals and systems perspective.
 And I would say some people think
 that manipulation is too complex.
 You shouldn't bother trying to write differential equations
 to describe it.
 Like, what benefit are you going to have?
 It's just so complex.
 And I feel differently.
 I feel like it's so complex that we
 must be careful about writing the low-level systems.
 Otherwise, there's no chance we're
 going to understand the high-level systems.
 And I think as you get to greater levels of maturity,
 as companies--
 like, a lot of companies will start with ROS.
 I think-- so ROS 2 is trying to make a much harder case.
 But a lot of times, you can bring something up very quickly
 in ROS.
 But when you're trying to certify something or get
 to a higher level of maturity, then
 not being able to control message-passing rates
 or other thing in detail becomes a limitation.
 And if you get to a higher level of maturity,
 having all your state declared, knowing
 you're going to get exactly deterministic replays,
 that's a powerful feature that you just cannot get in ROS.
 So that's one thing--
 in Gazebo, in ROS, it's not a knock.
 It's just the nature of choosing that path
 is that you will probably never get the same simulation twice.
 Simulation should be repeatable, right?
 But if you put in the middle of it message-passing, which
 is depending on operating system threads coming in and out
 at certain timing, it is very hard.
 You can build wrappers around it to synchronize everything.
 It's a lot of work, but you can do that.
 But the fact is you can't run the same experiment twice.
 The simulation will be slightly different every time.
 So it makes it harder to debug, harder to certify.
 If you declare extra state and everything like this,
 then you can.
 OK, yeah, so the basic plan for the course is--
 I'll finish up here.
 Let me take one second to start the robot booting again
 so I don't have to wait too long.
 Takes too long to start.
 Cool, I'll be able to walk home in 10 minutes.
 OK, so the basic plan for the course
 is to go through that ladder of complexity.
 This is why I didn't leave it on the whole time.
 We're going to talk about perception systems,
 sometimes by themselves.
 But we'll try to break the distinction.
 We'll have some lectures on perception for sure.
 We'll do both the geometric perception,
 which can get you pretty far and gives you
 some of the core skills from kinematics and geometry
 that are relevant.
 Geometric perception is one way to think about it.
 And then deep learning based or data driven perception
 will certainly be a topic also.
 We'll do kinematics and dynamics and motion planning,
 for instance.
 We'll definitely talk about some dynamics and control.
 Spoiler alert, big robots touching small objects
 is pretty--
 gets pretty complicated.
 Contact mechanics is tough.
 We'll spend a bit diving into the contact mechanics world.
 And we'll do some higher level task planning.
 But my goal is not to just say the first quarter of the term
 is perception, the next quarter of the term
 is motion planning, and so on.
 What I'm going to try to do is build a manipulation system
 that can do a full stack task.
 Move all the objects in this bin over to this bin,
 even if you throw in random objects.
 That'll be one of the things we build up.
 And we'll build a basic competency
 of in perception, planning, dynamics, and control
 to accomplish that task.
 And then we'll spiral out.
 And we'll try to introduce the new concepts to do
 more complicated things if it makes our robot capable
 of doing something new.
 I'm going to try to put in more mobile manipulation this time,
 because I think it's awesome.
 You can do it on your project.
 And then at the end, we'll have a handful
 of lectures that are more like the research topics,
 the boutique lectures we call them.
 And I'll query you guys throughout the term
 about what you're interested in.
 But if we want to dive in and talk about what does it
 look like to think about belief space planning
 or manipulation?
 What does it look like if we're going to think about tactile
 sensors for a lecture?
 And we'll have a handful of options
 that we can pick from to do the last lectures
 while you're focused on your project
 and not listening to me so much.
 We'll talk about the more research threads.
 Cool?
 OK.
 That's the plan for the course.
 I hope you keep coming.
 Thank you for sitting in the crowded room.
 We'll see you next time.
 Make sure you sign on to Piazza, please.
 How's it going?
 It's good.
 I was wondering if you-- have you heard of RT2?
 I can't spell it.
 Of course, yeah.
 Do you think that's sort of the future of standard
 [INAUDIBLE]
 I think RT2 is good.
 But I think it's the beginning of a future.
 But I think we're still in the place where
 we're taking what's amazing about language models
 and kind of bolting on the robot.
 And I think there's going to be a future where the robot
 data plays a bigger role in the common sense understanding.
 So yeah, I think a lot of the world is seeing--
 I think that most of the good things that are good about RT2
 are good because of the language model.
 Yeah.
 And the robot hasn't added extra problems
 with information at some point.
