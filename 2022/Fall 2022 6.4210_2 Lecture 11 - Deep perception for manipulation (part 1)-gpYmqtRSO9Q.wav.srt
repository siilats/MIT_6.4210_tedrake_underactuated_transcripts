1
00:00:00,000 --> 00:00:03,480
 [SILENCE]

2
00:00:03,480 --> 00:00:06,960
 [SILENCE]

3
00:00:06,960 --> 00:00:10,440
 [SILENCE]

4
00:00:10,440 --> 00:00:13,440
 [SILENCE]

5
00:00:13,440 --> 00:00:16,440
 [SILENCE]

6
00:00:16,440 --> 00:00:20,440
 [SILENCE]

7
00:00:20,440 --> 00:00:24,440
 [SILENCE]

8
00:00:24,440 --> 00:00:28,440
 [SILENCE]

9
00:00:28,440 --> 00:00:32,440
 [SILENCE]

10
00:00:32,440 --> 00:00:36,440
 [SILENCE]

11
00:00:36,440 --> 00:00:40,440
 [SILENCE]

12
00:00:41,440 --> 00:00:44,440
 [SILENCE]

13
00:00:44,440 --> 00:00:48,440
 [SILENCE]

14
00:00:48,440 --> 00:00:52,440
 [SILENCE]

15
00:00:52,440 --> 00:00:56,440
 [SILENCE]

16
00:00:56,440 --> 00:01:00,440
 [SILENCE]

17
00:01:00,440 --> 00:01:04,440
 [SILENCE]

18
00:01:04,440 --> 00:01:08,440
 [SILENCE]

19
00:01:09,440 --> 00:01:12,440
 [SILENCE]

20
00:01:12,440 --> 00:01:16,440
 [SILENCE]

21
00:01:16,440 --> 00:01:20,440
 [SILENCE]

22
00:01:20,440 --> 00:01:24,440
 [SILENCE]

23
00:01:24,440 --> 00:01:28,440
 [SILENCE]

24
00:01:28,440 --> 00:01:32,440
 [SILENCE]

25
00:01:32,440 --> 00:01:36,440
 [SILENCE]

26
00:01:37,440 --> 00:01:40,440
 [SILENCE]

27
00:01:40,440 --> 00:01:44,440
 [SILENCE]

28
00:01:44,440 --> 00:01:48,440
 [SILENCE]

29
00:01:48,440 --> 00:01:52,440
 [SILENCE]

30
00:01:52,440 --> 00:01:56,440
 [SILENCE]

31
00:01:56,440 --> 00:02:00,440
 [SILENCE]

32
00:02:00,440 --> 00:02:04,440
 [SILENCE]

33
00:02:05,440 --> 00:02:08,440
 [SILENCE]

34
00:02:08,440 --> 00:02:12,440
 [SILENCE]

35
00:02:12,440 --> 00:02:16,440
 [SILENCE]

36
00:02:16,440 --> 00:02:20,440
 [SILENCE]

37
00:02:20,440 --> 00:02:24,440
 [SILENCE]

38
00:02:24,440 --> 00:02:28,440
 [SILENCE]

39
00:02:28,440 --> 00:02:32,440
 [SILENCE]

40
00:02:33,440 --> 00:02:36,440
 [SILENCE]

41
00:02:36,440 --> 00:02:40,440
 [SILENCE]

42
00:02:40,440 --> 00:02:44,440
 [SILENCE]

43
00:02:44,440 --> 00:02:48,440
 How are people feeling for their project proposals?

44
00:02:48,440 --> 00:02:53,440
 Good shape? Thumbs up? Thumbs down?

45
00:02:53,440 --> 00:02:57,440
 Still figuring it out? No idea? Thumbs down?

46
00:02:57,440 --> 00:03:01,440
 I'll stick around a little bit after lecture if anybody wants to chat right now,

47
00:03:02,440 --> 00:03:07,440
 but just reach out to us and we'll give you any feedback or ideas we can this week.

48
00:03:07,440 --> 00:03:13,440
 Okay, well, welcome back everybody.

49
00:03:13,440 --> 00:03:22,440
 We're going to talk today about the deep learning version of perception.

50
00:03:22,440 --> 00:03:26,440
 And actually, this is one of the harder lectures for me to give

51
00:03:27,440 --> 00:03:33,440
 because I think the variance of experience in the room is the highest of all the topics we cover.

52
00:03:33,440 --> 00:03:38,440
 I can sort of assume, it might be a few years since you've drawn a free body diagram,

53
00:03:38,440 --> 00:03:42,440
 but everybody's drawn free body diagrams at some point.

54
00:03:42,440 --> 00:03:47,440
 And here, I think some people have just heard, everybody's heard about it.

55
00:03:47,440 --> 00:03:53,440
 Some of you might be training deep networks right now on your laptop while we're in lecture,

56
00:03:54,440 --> 00:03:57,440
 some of you haven't dabbled yet, right?

57
00:03:57,440 --> 00:04:02,440
 And there are plenty of courses here that can teach you the details of deep learning,

58
00:04:02,440 --> 00:04:06,440
 which I would highly recommend. I'm not going to try to do any of that.

59
00:04:06,440 --> 00:04:09,440
 Even computer vision, right, there's huge, there's whole courses on computer vision.

60
00:04:09,440 --> 00:04:11,440
 I'm not going to try to cover that.

61
00:04:11,440 --> 00:04:16,440
 What I'm going to try to do is dial in a few of the key topics,

62
00:04:16,440 --> 00:04:20,440
 enough that if you haven't seen it, you can use it effectively.

63
00:04:21,440 --> 00:04:24,440
 If you have seen it, I hope to bring some ideas from manipulation

64
00:04:24,440 --> 00:04:30,440
 that you maybe haven't thought about the computer vision pipelines from this perspective.

65
00:04:30,440 --> 00:04:36,440
 More than any other lecture, I would say, I'm going to be reading you guys

66
00:04:36,440 --> 00:04:42,440
 and trying to speed up or slow down based on what you guys are feeling.

67
00:04:42,440 --> 00:04:47,440
 Feel free to ask questions, feel free to be like, within reason.

68
00:04:48,440 --> 00:04:56,440
 Okay, so I do think that the needs of manipulation for,

69
00:04:56,440 --> 00:04:59,440
 you know, put particular pressures on our deep learning pipelines

70
00:04:59,440 --> 00:05:02,440
 that are unique and interesting.

71
00:05:02,440 --> 00:05:06,440
 Let me just remind you sort of the motivation we talked about

72
00:05:06,440 --> 00:05:10,440
 is that we've done a lot of work with geometric perception

73
00:05:10,440 --> 00:05:14,440
 and we had a whole pipeline of clearing clutter out of the bins

74
00:05:14,440 --> 00:05:16,440
 that didn't use anything from deep learning, right?

75
00:05:17,440 --> 00:05:18,440
 And it's surprisingly good.

76
00:05:18,440 --> 00:05:21,440
 Like it can just pick up objects all day long,

77
00:05:21,440 --> 00:05:24,440
 you can throw any objects in the bin, it'll do its thing.

78
00:05:24,440 --> 00:05:28,440
 But it doesn't get done everything that we want, right?

79
00:05:28,440 --> 00:05:33,440
 It has flaws, even if your goal is object agnostic,

80
00:05:33,440 --> 00:05:36,440
 you don't care what objects are moved,

81
00:05:36,440 --> 00:05:41,440
 you might still have problems because it's not just the fact that it doesn't know about objects,

82
00:05:41,440 --> 00:05:45,440
 it can make silly decisions about where to put its fingers, right?

83
00:05:46,440 --> 00:05:49,440
 It might pick up the hammer from the corner and that's just a bad strategy

84
00:05:49,440 --> 00:05:52,440
 because there's a large wrench due to the gravitation

85
00:05:52,440 --> 00:05:55,440
 that would cause the grasp to be fragile, right?

86
00:05:55,440 --> 00:05:58,440
 So there's a lot, when you go to decide what to do,

87
00:05:58,440 --> 00:06:01,440
 where to pick things up, you're bringing a lot of background information

88
00:06:01,440 --> 00:06:06,440
 into the picture that geometry alone doesn't tell you.

89
00:06:06,440 --> 00:06:09,440
 Even physics alone doesn't tell you,

90
00:06:09,440 --> 00:06:12,440
 maybe don't touch the sharp part of the knife, for instance, right?

91
00:06:13,440 --> 00:06:14,440
 Okay.

92
00:06:14,440 --> 00:06:19,440
 In practice, in the particular tool chain that we gave you,

93
00:06:19,440 --> 00:06:22,440
 there are quirks like picking up two objects at once

94
00:06:22,440 --> 00:06:26,440
 because you don't even know where the extents of an object are, right?

95
00:06:26,440 --> 00:06:28,440
 And so that's a problem that these come in.

96
00:06:28,440 --> 00:06:35,440
 The geometric stuff can take its best effort with partial views.

97
00:06:35,440 --> 00:06:38,440
 If you have cameras only on one side,

98
00:06:38,440 --> 00:06:40,440
 there's a backside of the object that you can't see,

99
00:06:41,440 --> 00:06:44,440
 your point cloud reasoning is only going to get you so far.

100
00:06:44,440 --> 00:06:49,440
 At some point, inherently, the way to get farther

101
00:06:49,440 --> 00:06:53,440
 is to have previous experience which tells you what's on the other side of the object.

102
00:06:53,440 --> 00:06:58,440
 A data-driven method becomes a key approach, right?

103
00:06:58,440 --> 00:07:03,440
 And it happens that because of our sensors today,

104
00:07:03,440 --> 00:07:06,440
 it turns out that some of our geometric reasoning

105
00:07:06,440 --> 00:07:10,440
 falls down for transparent objects and other things like that.

106
00:07:10,440 --> 00:07:15,440
 But fundamentally, there are tasks that just require you to understand objects.

107
00:07:15,440 --> 00:07:19,440
 If you say, "I don't care about the cheese-it box, I don't care about the spam,

108
00:07:19,440 --> 00:07:23,440
 I want to move the mustard bottles over because someone just bought a mustard bottle

109
00:07:23,440 --> 00:07:25,440
 and I need to put it in the box and ship it," right?

110
00:07:25,440 --> 00:07:29,440
 Then that fundamentally requires knowledge of the objects.

111
00:07:29,440 --> 00:07:31,440
 Okay.

112
00:07:31,440 --> 00:07:37,440
 So, there's been a revolution in deep learning over the last few years.

113
00:07:38,440 --> 00:07:40,440
 It was powered mostly by data, right?

114
00:07:40,440 --> 00:07:45,440
 Among other things, and compute, and good ideas, and a lot of things.

115
00:07:45,440 --> 00:07:48,440
 But everybody talks about the big data.

116
00:07:48,440 --> 00:07:53,440
 So, one of the first topics I want to throw in and talk about here is,

117
00:07:53,440 --> 00:08:05,440
 how do we get to big data for manipulation?

118
00:08:05,440 --> 00:08:12,440
 [Writing on Board]

119
00:08:12,440 --> 00:08:20,440
 So, the watershed moments for deep learning for computer vision

120
00:08:20,440 --> 00:08:25,440
 ran through the ImageNet dataset and the ImageNet challenge,

121
00:08:26,440 --> 00:08:36,440
 where Fei-Fei and her team acquired labels of images, of like 128 million images.

122
00:08:36,440 --> 00:08:40,440
 A lot of images. I think it's 1.28 million.

123
00:08:40,440 --> 00:08:44,440
 I was off by a couple of orders of magnitude, but it's a big number. That's the point.

124
00:08:44,440 --> 00:08:52,440
 So, the go-ahead idea in ImageNet was to crowdsource image labels.

125
00:08:53,440 --> 00:08:59,440
 But it took a lot of acquiring images, cleaning images, and then labeling images

126
00:08:59,440 --> 00:09:03,440
 to make one big dataset which powered a lot of computer vision.

127
00:09:03,440 --> 00:09:12,440
 If I want to pick up mustard bottles, I don't want to start by labeling 1.28 million mustard bottles.

128
00:09:12,440 --> 00:09:14,440
 So, what are we to do?

129
00:09:14,440 --> 00:09:21,440
 So, I want to just talk that through, and how do we get there for manipulation?

130
00:09:22,440 --> 00:09:25,440
 To tell that story, let me just make sure I define our basic concepts.

131
00:09:25,440 --> 00:09:33,440
 When you're talking about the standard computer vision tasks in learning,

132
00:09:33,440 --> 00:09:37,440
 we have to distinguish between a couple of different categories.

133
00:09:37,440 --> 00:09:40,440
 The first one would be image recognition.

134
00:09:40,440 --> 00:09:45,440
 I just say, "Is there a sheep in the image? Is there a dog in the image?"

135
00:09:45,440 --> 00:09:50,440
 With what confidence would I say that there is a sheep in the image or a dog in the image?

136
00:09:51,440 --> 00:09:54,440
 That's the classic first task for computer vision.

137
00:09:54,440 --> 00:09:57,440
 That's what ImageNet had a lot of labels for initially.

138
00:09:57,440 --> 00:10:03,440
 ImageNet also got labels for object detection, which is to say not only that there is a sheep,

139
00:10:03,440 --> 00:10:05,440
 but here's a bounding box around the sheep.

140
00:10:05,440 --> 00:10:10,440
 So, the output would be two numbers, for instance, the two sets of four numbers,

141
00:10:10,440 --> 00:10:16,440
 the pixel location of the lower left and the pixel location of the upper right, for instance.

142
00:10:16,440 --> 00:10:20,440
 We'll talk about exactly how that's done, too.

143
00:10:20,440 --> 00:10:24,440
 And then there's two different types of segmentation, which is very common,

144
00:10:24,440 --> 00:10:27,440
 which would be the semantic segmentation.

145
00:10:27,440 --> 00:10:29,440
 I think the picture tells this very well.

146
00:10:29,440 --> 00:10:35,440
 Semantic segmentation says, "Make all of the pixels that are sheep pixels blue

147
00:10:35,440 --> 00:10:38,440
 and the dog pixels red," for instance, in this case,

148
00:10:38,440 --> 00:10:42,440
 versus instance-level segmentation, which is to say,

149
00:10:42,440 --> 00:10:45,440
 "For every different sheep, I want a different label."

150
00:10:45,440 --> 00:10:48,440
 So, that's the background for this.

151
00:10:49,440 --> 00:10:51,440
 Which of those do we want for our manipulation pipeline?

152
00:10:51,440 --> 00:10:54,440
 If we just want to pick the mustard bottles, for instance, out,

153
00:10:54,440 --> 00:10:56,440
 which of those is going to be the most useful?

154
00:10:56,440 --> 00:11:04,440
 Object detection is going to be super useful, right?

155
00:11:04,440 --> 00:11:09,440
 So, at least we could then go in and use our geometric reasoning

156
00:11:09,440 --> 00:11:12,440
 on the point cloud inside the bounding box, for instance.

157
00:11:12,440 --> 00:11:17,440
 If we can get pixel-wise segmentation, we could do even better, right?

158
00:11:18,440 --> 00:11:21,440
 If we can ignore, you know, we've talked about the limitations of ICP,

159
00:11:21,440 --> 00:11:23,440
 for instance, with outliers.

160
00:11:23,440 --> 00:11:26,440
 If you could really take away all of the points that are not associated

161
00:11:26,440 --> 00:11:29,440
 with the mustard bottle, then that's even the dream.

162
00:11:29,440 --> 00:11:33,440
 So, instance segmentation is actually the one that's proven to be

163
00:11:33,440 --> 00:11:36,440
 the most transferable, I would say, from the computer vision world

164
00:11:36,440 --> 00:11:39,440
 directly into manipulation context of this.

165
00:11:39,440 --> 00:11:41,440
 We're going to see where I'm going to spend most of the time on Thursday

166
00:11:41,440 --> 00:11:44,440
 talking about all of the things that computer vision people normally don't do

167
00:11:44,440 --> 00:11:47,440
 that help, that are more specific to manipulation.

168
00:11:47,440 --> 00:11:51,440
 But first, we'll try to understand how to get instance segmentation

169
00:11:51,440 --> 00:11:53,440
 into our manipulation pipeline.

170
00:11:53,440 --> 00:11:56,440
 Okay?

171
00:11:56,440 --> 00:11:59,440
 And there's actually two ways. We'll talk about it at the end.

172
00:11:59,440 --> 00:12:02,440
 So, I'd say both of our pipelines so far, we could say,

173
00:12:02,440 --> 00:12:06,440
 and people do actively use instance segmentation.

174
00:12:06,440 --> 00:12:11,440
 So, we'll take an RGB image, we'll pick out the pixels,

175
00:12:11,440 --> 00:12:16,440
 and we'll then do, for instance, ICP to find the known location

176
00:12:16,440 --> 00:12:20,440
 and then pick up a known, you know, use model-based grasp synthesis.

177
00:12:20,440 --> 00:12:24,440
 But you could also use instance segmentation with our

178
00:12:24,440 --> 00:12:27,440
 clutter clearing grasp selection.

179
00:12:27,440 --> 00:12:29,440
 So, if I just took the point clouds that were left,

180
00:12:29,440 --> 00:12:32,440
 the points in the point cloud that were left after a segmentation

181
00:12:32,440 --> 00:12:35,440
 and then I did my antipodal grasping, that'll work too.

182
00:12:35,440 --> 00:12:38,440
 Both of those are super powerful pipelines.

183
00:12:38,440 --> 00:12:44,440
 Okay, so ImageNet was mostly about object detection and image recognition.

184
00:12:45,440 --> 00:12:52,440
 Cocoa dataset was the sort of watershed moment for more instance segmentation.

185
00:12:52,440 --> 00:12:55,440
 They got many fewer, hundreds of thousands of labels,

186
00:12:55,440 --> 00:12:58,440
 but at the pixel-wise level.

187
00:12:58,440 --> 00:13:04,440
 Now, imagine going through and labeling every pixel of every image, right,

188
00:13:04,440 --> 00:13:06,440
 for a hundred thousand images.

189
00:13:06,440 --> 00:13:12,440
 It's a good annotation tools, right, and a lot of people on Amazon Turk at the time, right,

190
00:13:13,440 --> 00:13:17,440
 and they got it done, but that's a laborious task, okay.

191
00:13:17,440 --> 00:13:22,440
 Some of the first annotation tools came out of CSAIL.

192
00:13:22,440 --> 00:13:24,440
 Antonio Torraldo is upstairs.

193
00:13:24,440 --> 00:13:27,440
 He's got a lab that's done some of the really defining work in this.

194
00:13:27,440 --> 00:13:31,440
 And when this was all starting and he was educating the rest of us

195
00:13:31,440 --> 00:13:34,440
 about crowdsourcing image labels and stuff like this,

196
00:13:34,440 --> 00:13:37,440
 he said if you look at the quality of the image labels,

197
00:13:37,440 --> 00:13:41,440
 you pay people like a penny to do that, right,

198
00:13:42,440 --> 00:13:43,440
 it's incredibly good.

199
00:13:43,440 --> 00:13:45,440
 He's like if you paid me a penny, I'd be just like, you know,

200
00:13:45,440 --> 00:13:48,440
 go to the next one, but somehow people are just really, really meticulous

201
00:13:48,440 --> 00:13:50,440
 about getting every single pixel right.

202
00:13:50,440 --> 00:13:53,440
 And he said if you look at the statistics and there was an anomaly

203
00:13:53,440 --> 00:13:56,440
 of like someone who had labeled way more than anybody else,

204
00:13:56,440 --> 00:13:58,440
 and it turned out it was his mom.

205
00:13:58,440 --> 00:14:01,440
 He said my mom's an incredibly good labeler, yeah.

206
00:14:01,440 --> 00:14:04,440
 Okay, but that was this revolution where people started to get

207
00:14:04,440 --> 00:14:08,440
 crowdsourced large-scale datasets for image segmentation.

208
00:14:10,440 --> 00:14:13,440
 One of the crazy things, one of the magical things,

209
00:14:13,440 --> 00:14:17,440
 if you look at the COCO dataset, for instance,

210
00:14:17,440 --> 00:14:20,440
 it's got a bunch of different categories.

211
00:14:20,440 --> 00:14:23,440
 You can just go on the website and list them, right.

212
00:14:23,440 --> 00:14:26,440
 It's got bicycles, cars, motorcycles, traffic lights,

213
00:14:26,440 --> 00:14:28,440
 that's useful for autonomous driving, right.

214
00:14:28,440 --> 00:14:31,440
 Birds, cats, dogs, horses, sheep,

215
00:14:31,440 --> 00:14:34,440
 those aren't the things that I want to manipulate most of the time.

216
00:14:34,440 --> 00:14:37,440
 There's a few manipulation-specific ones.

217
00:14:38,440 --> 00:14:42,440
 You know, snowboards and plates, there we go, that's a useful one.

218
00:14:42,440 --> 00:14:44,440
 Plates and bottles and cups and forks.

219
00:14:44,440 --> 00:14:48,440
 I'd say more than, if you take something that's pre-trained on COCO,

220
00:14:48,440 --> 00:14:51,440
 it's going to call most of the things in your bin

221
00:14:51,440 --> 00:14:55,440
 a mug or a bottle or a fork or something like that, okay.

222
00:14:55,440 --> 00:15:01,440
 So this is super useful, but it's not quite enough

223
00:15:01,440 --> 00:15:04,440
 for us to do most of the things we want in manipulation.

224
00:15:05,440 --> 00:15:10,440
 One of the biggest ideas, and I don't think anybody really saw it coming,

225
00:15:10,440 --> 00:15:14,440
 it's even hard to justify still, given our best theory of deep learning,

226
00:15:14,440 --> 00:15:17,440
 but one of the amazing things that happened in deep learning

227
00:15:17,440 --> 00:15:20,440
 is the story of transfer learning, right.

228
00:15:20,440 --> 00:15:27,440
 So COCO is a hundred thousand image dataset.

229
00:15:27,440 --> 00:15:31,440
 ImageNet was 1.28 million images.

230
00:15:33,440 --> 00:15:37,440
 The crazy thing is that if you train on ImageNet first,

231
00:15:37,440 --> 00:15:42,440
 even though it's only got image detection and object detection labels,

232
00:15:42,440 --> 00:15:47,440
 and you take those weights and then you retrain on COCO,

233
00:15:47,440 --> 00:15:50,440
 instead of starting from scratch and only using COCO,

234
00:15:50,440 --> 00:15:52,440
 then you can actually do better on COCO

235
00:15:52,440 --> 00:15:54,440
 because you trained on ImageNet before.

236
00:15:54,440 --> 00:15:58,440
 Okay.

237
00:15:58,440 --> 00:16:02,440
 So this is the idea of transfer learning or fine-tuning, okay.

238
00:16:03,440 --> 00:16:14,440
 [Writing on Board]

239
00:16:14,440 --> 00:16:22,440
 Training on one dataset, pre-training, let's say, on one dataset,

240
00:16:22,440 --> 00:16:27,440
 almost always ImageNet, for instance,

241
00:16:27,440 --> 00:16:30,440
 because it's big and diverse in the right way.

242
00:16:31,440 --> 00:16:36,440
 Improves performance on a downstream,

243
00:16:36,440 --> 00:16:42,440
 let me just say on COCO here.

244
00:16:42,440 --> 00:16:48,440
 It didn't have to be that, right.

245
00:16:48,440 --> 00:16:53,440
 Why should you, you know, why would you do better

246
00:16:53,440 --> 00:16:56,440
 having trained previously on ImageNet

247
00:16:56,440 --> 00:16:58,440
 than just training directly on the objective?

248
00:16:59,440 --> 00:17:03,440
 From the optimization point of view, that's a super weird thing to think, right.

249
00:17:03,440 --> 00:17:08,440
 If I were to say, if you solve an inverse kinematics problem on a panda,

250
00:17:08,440 --> 00:17:11,440
 then you move it over and there you're going to solve

251
00:17:11,440 --> 00:17:14,440
 an inverse kinematics problem better on an IWA.

252
00:17:14,440 --> 00:17:17,440
 I'd look at you like you're crazy, because that's just a crazy thing to do, okay.

253
00:17:17,440 --> 00:17:23,440
 But that is a property that we've seen in the deep network architectures

254
00:17:23,440 --> 00:17:26,440
 that people are using day in and day out.

255
00:17:27,440 --> 00:17:31,440
 So a standard thing to do, even if you change tasks a little bit

256
00:17:31,440 --> 00:17:35,440
 from object detection to instance segmentation,

257
00:17:35,440 --> 00:17:39,440
 you can take your original, let's say, ImageNet dataset,

258
00:17:39,440 --> 00:17:42,440
 you've got a deep network with many layers, right,

259
00:17:42,440 --> 00:17:49,440
 and the last layer of ImageNet is a mapping from some weird,

260
00:17:49,440 --> 00:17:54,440
 you know, neural representation to the labels of ImageNet.

261
00:17:55,440 --> 00:17:58,440
 You don't have the same labels in your COCO dataset, okay.

262
00:17:58,440 --> 00:18:03,440
 So we rip off the last layer and replace it with a fresh last layer,

263
00:18:03,440 --> 00:18:07,440
 which has outputs for the labels that you want in the COCO dataset.

264
00:18:07,440 --> 00:18:15,440
 And then, you just retrain, but you don't retrain from scratch.

265
00:18:15,440 --> 00:18:17,440
 When you're training, I'll talk a little bit about training,

266
00:18:17,440 --> 00:18:19,440
 but not too much about training.

267
00:18:19,440 --> 00:18:22,440
 But when you're training, you take the weights that you already acquired from ImageNet

268
00:18:23,440 --> 00:18:27,440
 and you just fine-tune them for these layers,

269
00:18:27,440 --> 00:18:29,440
 and you've trained from scratch the last one

270
00:18:29,440 --> 00:18:31,440
 by just running more gradient descent on COCO.

271
00:18:31,440 --> 00:18:33,440
 This is the magic of transfer learning.

272
00:18:33,440 --> 00:18:35,440
 Yeah?

273
00:18:35,440 --> 00:18:37,440
 [inaudible]

274
00:18:37,440 --> 00:18:40,440
 That's right, up to the last layers.

275
00:18:40,440 --> 00:18:42,440
 Now, what we're going to see, for instance,

276
00:18:42,440 --> 00:18:44,440
 is for the instance segmentation,

277
00:18:44,440 --> 00:18:47,440
 you actually put a pretty sophisticated last layer that's different,

278
00:18:47,440 --> 00:18:52,440
 but still using the front half of the network from ImageNet, for instance,

279
00:18:52,440 --> 00:18:56,440
 is enough to do better on the full task.

280
00:18:56,440 --> 00:19:00,440
 Now, the intuition might be that you've somehow learned,

281
00:19:00,440 --> 00:19:02,440
 you know, ImageNet was big enough,

282
00:19:02,440 --> 00:19:04,440
 you learned something about natural images,

283
00:19:04,440 --> 00:19:07,440
 you learned some intermediate representations

284
00:19:07,440 --> 00:19:10,440
 that captured the diversity of natural images,

285
00:19:10,440 --> 00:19:16,440
 and this put you in the right part of the neural network parameter space

286
00:19:16,440 --> 00:19:21,440
 that somehow staying near there and finding the best instance segmentation

287
00:19:21,440 --> 00:19:25,440
 was better, you leveraged the diversity of the ImageNet dataset

288
00:19:25,440 --> 00:19:27,440
 to do better at COCO.

289
00:19:27,440 --> 00:19:34,440
 That's too hand-wavy for my taste, but that's the view of it.

290
00:19:34,440 --> 00:19:40,440
 Okay, so this fine-tuning is one of the biggest things

291
00:19:40,440 --> 00:19:42,440
 that happened in deep learning.

292
00:19:42,440 --> 00:19:44,440
 It didn't have to happen.

293
00:19:44,440 --> 00:19:47,440
 It's also what provides us the ability to do, you know,

294
00:19:48,440 --> 00:19:51,440
 the same things with smaller datasets in manipulation.

295
00:19:51,440 --> 00:19:56,440
 So now the prospect is I don't have to label 128,

296
00:19:56,440 --> 00:19:59,440
 or 1.28 million images for manipulation.

297
00:19:59,440 --> 00:20:02,440
 It turns out in many cases you can label tens of images,

298
00:20:02,440 --> 00:20:07,440
 hundreds of images, and do surprisingly well, sometimes zero, right?

299
00:20:07,440 --> 00:20:09,440
 But you have to somehow, you know,

300
00:20:09,440 --> 00:20:13,440
 at least that output layer needs to be trained with your new dataset.

301
00:20:13,440 --> 00:20:24,440
 Okay, so how do we make the instance-level training data for manipulation?

302
00:20:24,440 --> 00:20:27,440
 There's a few sort of standard tools that I'd say

303
00:20:27,440 --> 00:20:31,440
 almost every manipulation pipeline is using something kind of like this,

304
00:20:31,440 --> 00:20:34,440
 you know, that if you want to come up with a lot of labels,

305
00:20:34,440 --> 00:20:39,440
 pixel-wise labels of objects that you're going to manipulate,

306
00:20:39,440 --> 00:20:42,440
 this is one called Label Fusion.

307
00:20:42,440 --> 00:20:44,440
 Let me tell you the steps.

308
00:20:44,440 --> 00:20:46,440
 I mentioned it once before when we were talking about ICP,

309
00:20:46,440 --> 00:20:49,440
 but now you have more context here, okay?

310
00:20:49,440 --> 00:20:54,440
 So you've got a drill in the lab there.

311
00:20:54,440 --> 00:20:57,440
 You want to somehow use this to create a training dataset

312
00:20:57,440 --> 00:21:01,440
 with pixel-wise labels of the drill, okay?

313
00:21:01,440 --> 00:21:04,440
 The steps are pretty simple.

314
00:21:04,440 --> 00:21:06,440
 We're going to first just take a lot,

315
00:21:06,440 --> 00:21:10,440
 just move the camera around the drill in lots of different ways.

316
00:21:11,440 --> 00:21:13,440
 Then we're going to do a dense reconstruction,

317
00:21:13,440 --> 00:21:17,440
 which people do, you know, a few years ago,

318
00:21:17,440 --> 00:21:21,440
 it was always with point clouds and these fusion algorithms,

319
00:21:21,440 --> 00:21:25,440
 which are a lot like ICP, but basically you're going to make

320
00:21:25,440 --> 00:21:29,440
 all of these views with RGB data into one big point cloud.

321
00:21:29,440 --> 00:21:32,440
 The same way we fused our multiple views of the cameras,

322
00:21:32,440 --> 00:21:36,440
 just imagine doing that with a moving camera, right?

323
00:21:36,440 --> 00:21:38,440
 Join all the point clouds together.

324
00:21:39,440 --> 00:21:41,440
 So that next step is pretty effective at also estimating

325
00:21:41,440 --> 00:21:43,440
 the pose of the camera.

326
00:21:43,440 --> 00:21:45,440
 We assumed we knew where the pose of the camera was,

327
00:21:45,440 --> 00:21:48,440
 but this will just estimate the pose of the camera.

328
00:21:48,440 --> 00:21:50,440
 Okay?

329
00:21:50,440 --> 00:21:54,440
 Now we said that ICP isn't strong enough to label the drill

330
00:21:54,440 --> 00:21:56,440
 if you just give me a huge point cloud,

331
00:21:56,440 --> 00:21:58,440
 which we wanted it to be.

332
00:21:58,440 --> 00:22:00,440
 It's not, okay?

333
00:22:00,440 --> 00:22:02,440
 But it turns out with a good guess,

334
00:22:02,440 --> 00:22:05,440
 then ICP is fantastic, right?

335
00:22:06,440 --> 00:22:09,440
 So the pipeline is basically make a user interface

336
00:22:09,440 --> 00:22:13,440
 so that after taking a whole video of images,

337
00:22:13,440 --> 00:22:17,440
 a human clicks like three times to, you know,

338
00:22:17,440 --> 00:22:20,440
 they say here's three points on the drill CAD model.

339
00:22:20,440 --> 00:22:23,440
 Here's three points in the generated point cloud.

340
00:22:23,440 --> 00:22:25,440
 That's an initial guess.

341
00:22:25,440 --> 00:22:27,440
 Humans aren't very accurate at that,

342
00:22:27,440 --> 00:22:30,440
 but then it just snaps into place with ICP.

343
00:22:30,440 --> 00:22:32,440
 Now you know a ground truth, you know,

344
00:22:32,440 --> 00:22:34,440
 up to our ICP resolution,

345
00:22:35,440 --> 00:22:36,440
 location, pose of the drill.

346
00:22:36,440 --> 00:22:39,440
 But we want instance labels out.

347
00:22:39,440 --> 00:22:41,440
 Okay?

348
00:22:41,440 --> 00:22:44,440
 So given the CAD model and given the videos,

349
00:22:44,440 --> 00:22:48,440
 you can just render back the drill

350
00:22:48,440 --> 00:22:50,440
 into all of the images you took.

351
00:22:50,440 --> 00:22:52,440
 And now suddenly you have a bunch of,

352
00:22:52,440 --> 00:22:54,440
 now this isn't, you know,

353
00:22:54,440 --> 00:22:56,440
 these are very correlated images,

354
00:22:56,440 --> 00:22:58,440
 so it's not as good as having 100,000

355
00:22:58,440 --> 00:23:00,440
 completely different images.

356
00:23:00,440 --> 00:23:02,440
 But although they're very correlated,

357
00:23:02,440 --> 00:23:04,440
 you have perfect labels,

358
00:23:04,440 --> 00:23:07,440
 perfect labels of the pixel-wise mask

359
00:23:07,440 --> 00:23:09,440
 of that drill.

360
00:23:09,440 --> 00:23:11,440
 Okay?

361
00:23:11,440 --> 00:23:12,440
 And there's various ways to make that pipeline,

362
00:23:12,440 --> 00:23:15,440
 but something like that has been used

363
00:23:15,440 --> 00:23:17,440
 over and over and over again

364
00:23:17,440 --> 00:23:21,440
 to generate training data

365
00:23:21,440 --> 00:23:23,440
 for relevant manipulation items.

366
00:23:23,440 --> 00:23:27,440
 What's amazing is that, you know,

367
00:23:27,440 --> 00:23:29,440
 relatively small amounts of training data

368
00:23:29,440 --> 00:23:32,440
 with a pre-trained instance segmentation network

369
00:23:33,440 --> 00:23:34,440
 works incredibly well in practice.

370
00:23:34,440 --> 00:23:41,440
 Yeah? Does that make sense?

371
00:23:41,440 --> 00:23:45,440
 Okay.

372
00:23:45,440 --> 00:23:46,440
 So that's one way that we get

373
00:23:46,440 --> 00:23:49,440
 ground truth labels for our manipulation.

374
00:23:49,440 --> 00:23:51,440
 The other big way

375
00:23:51,440 --> 00:23:53,440
 is synthetic data.

376
00:23:53,440 --> 00:23:55,440
 Okay?

377
00:23:55,440 --> 00:23:57,440
 Over and over and over again now,

378
00:23:57,440 --> 00:23:59,440
 people have been turning their pipelines

379
00:23:59,440 --> 00:24:01,440
 to be more towards using

380
00:24:02,440 --> 00:24:05,440
 manipulation-based data generation

381
00:24:05,440 --> 00:24:07,440
 to train deep learning systems

382
00:24:07,440 --> 00:24:09,440
 that are going to work in the real world.

383
00:24:09,440 --> 00:24:12,440
 So I motivated our clutter clearing example

384
00:24:12,440 --> 00:24:14,440
 by this case,

385
00:24:14,440 --> 00:24:16,440
 but if you look at the RGBD sensor

386
00:24:16,440 --> 00:24:18,440
 that we've been using the whole time,

387
00:24:18,440 --> 00:24:21,440
 right, we've been using the color image out,

388
00:24:21,440 --> 00:24:23,440
 the depth image out,

389
00:24:23,440 --> 00:24:25,440
 in order to make our point cloud,

390
00:24:25,440 --> 00:24:27,440
 but there's another image that comes out,

391
00:24:27,440 --> 00:24:29,440
 which is the label image,

392
00:24:29,440 --> 00:24:31,440
 which the real camera, of course,

393
00:24:31,440 --> 00:24:34,440
 is designed entirely for generating training data.

394
00:24:34,440 --> 00:24:36,440
 And this is just a random example

395
00:24:36,440 --> 00:24:40,440
 of me dropping the objects in the bin

396
00:24:40,440 --> 00:24:43,440
 and then making perfect pixel-wide masks

397
00:24:43,440 --> 00:24:45,440
 from the label image.

398
00:24:45,440 --> 00:24:50,440
 It's super interesting that,

399
00:24:50,440 --> 00:24:54,440
 I mean, this doesn't look very realistic, right?

400
00:24:54,440 --> 00:24:57,440
 We can do a better job.

401
00:24:57,440 --> 00:24:59,440
 We have better renderers that are slower,

402
00:25:00,440 --> 00:25:03,440
 and I would absolutely recommend you use that

403
00:25:03,440 --> 00:25:06,440
 if you want your system-trained simulation

404
00:25:06,440 --> 00:25:08,440
 to work in reality,

405
00:25:08,440 --> 00:25:10,440
 but always, even the best rendering

406
00:25:10,440 --> 00:25:12,440
 is going to have some, we call it a domain gap, right?

407
00:25:12,440 --> 00:25:16,440
 That most of the time, a human eye

408
00:25:16,440 --> 00:25:18,440
 can tell you which one was simulated,

409
00:25:18,440 --> 00:25:20,440
 which one was real.

410
00:25:20,440 --> 00:25:22,440
 There's a really interesting trade-off

411
00:25:22,440 --> 00:25:24,440
 that happens, though.

412
00:25:24,440 --> 00:25:26,440
 So you can give me

413
00:25:26,440 --> 00:25:29,440
 some amount of hand-labeled data,

414
00:25:29,440 --> 00:25:31,440
 maybe using the label fusion kind of pipeline

415
00:25:31,440 --> 00:25:34,440
 where a human has annotated it.

416
00:25:34,440 --> 00:25:37,440
 It's close, but it's slightly imperfect,

417
00:25:37,440 --> 00:25:39,440
 but they're realistic images.

418
00:25:39,440 --> 00:25:42,440
 Or you can give me arbitrary gobs

419
00:25:42,440 --> 00:25:44,440
 of basically free data

420
00:25:44,440 --> 00:25:47,440
 that are perfectly labeled down to the pixel level

421
00:25:47,440 --> 00:25:49,440
 with a domain gap.

422
00:25:49,440 --> 00:25:52,440
 It turns out that, I think,

423
00:25:52,440 --> 00:25:54,440
 the standard recipe now is to use

424
00:25:54,440 --> 00:25:56,440
 a lot of simulated data

425
00:25:56,440 --> 00:25:58,440
 and a very little bit of real data,

426
00:25:58,440 --> 00:25:59,440
 but a lot of times,

427
00:25:59,440 --> 00:26:01,440
 the simulated data is actually enough

428
00:26:01,440 --> 00:26:03,440
 to outperform the real data.

429
00:26:03,440 --> 00:26:05,440
 Having a domain gap,

430
00:26:05,440 --> 00:26:07,440
 but absolutely perfect labels

431
00:26:07,440 --> 00:26:09,440
 can actually be better

432
00:26:09,440 --> 00:26:11,440
 than having the real images

433
00:26:11,440 --> 00:26:13,440
 that are imperfectly labeled.

434
00:26:13,440 --> 00:26:16,440
 There's a well-known story

435
00:26:16,440 --> 00:26:20,440
 that most of the big real data data sets,

436
00:26:20,440 --> 00:26:22,440
 even MNIST, which is the number data set

437
00:26:22,440 --> 00:26:24,440
 that everybody starts with in deep learning,

438
00:26:24,440 --> 00:26:26,440
 they have errors in the labels, right?

439
00:26:27,440 --> 00:26:28,440
 There's somehow a ceiling

440
00:26:28,440 --> 00:26:30,440
 in what total performance

441
00:26:30,440 --> 00:26:32,440
 the learning system probably can get

442
00:26:32,440 --> 00:26:34,440
 unless it has to learn the error,

443
00:26:34,440 --> 00:26:37,440
 which is probably almost random.

444
00:26:37,440 --> 00:26:40,440
 Human labels are imperfect.

445
00:26:40,440 --> 00:26:43,440
 Synthetic generation can get around that.

446
00:26:43,440 --> 00:26:46,440
 I went through and I made

447
00:26:46,440 --> 00:26:48,440
 just exactly from the clutter clearing.

448
00:26:48,440 --> 00:26:51,440
 I just dropped 10,000 images.

449
00:26:51,440 --> 00:26:54,440
 I just randomized the initial conditions,

450
00:26:54,440 --> 00:26:56,440
 picked from the random bin,

451
00:26:56,440 --> 00:26:58,440
 dropped it, waited until they settled,

452
00:26:58,440 --> 00:27:00,440
 took a picture, rendered both this

453
00:27:00,440 --> 00:27:04,440
 and the object labels, the masks,

454
00:27:04,440 --> 00:27:06,440
 the object instances,

455
00:27:06,440 --> 00:27:08,440
 a handful of metadata,

456
00:27:08,440 --> 00:27:10,440
 and I just made a big data set.

457
00:27:10,440 --> 00:27:12,440
 You'll use it on your p-set,

458
00:27:12,440 --> 00:27:14,440
 and we'll use it in the examples today.

459
00:27:14,440 --> 00:27:16,440
 I did 10,000 images,

460
00:27:16,440 --> 00:27:18,440
 which was probably way more than I needed,

461
00:27:18,440 --> 00:27:20,440
 but I was going for it.

462
00:27:20,440 --> 00:27:24,440
 Rather have too many than too few.

463
00:27:25,440 --> 00:27:26,440
 We're going to use this to train

464
00:27:26,440 --> 00:27:30,440
 our cheese-it box and mustard bottle detector.

465
00:27:30,440 --> 00:27:37,440
 Okay, questions about that so far?

466
00:27:37,440 --> 00:27:39,440
 Yeah.

467
00:27:39,440 --> 00:27:41,440
 [audience member]

468
00:27:41,440 --> 00:27:43,440
 The question being,

469
00:27:43,440 --> 00:27:45,440
 if the rendering was realistic

470
00:27:45,440 --> 00:27:47,440
 on synthetic data,

471
00:27:47,440 --> 00:27:49,440
 so you were able to render

472
00:27:49,440 --> 00:27:51,440
 completely realistic,

473
00:27:51,440 --> 00:27:53,440
 would there be any tradeoff

474
00:27:54,440 --> 00:27:55,440
 between the two?

475
00:27:55,440 --> 00:27:57,440
 [David]

476
00:27:57,440 --> 00:27:59,440
 That's a good question.

477
00:27:59,440 --> 00:28:01,440
 If I could render so that there was

478
00:28:01,440 --> 00:28:03,440
 really no domain gap,

479
00:28:03,440 --> 00:28:05,440
 I think I would probably always pick that.

480
00:28:05,440 --> 00:28:07,440
 The domain gap is more subtle

481
00:28:07,440 --> 00:28:09,440
 than you might think.

482
00:28:09,440 --> 00:28:11,440
 I emphasize the rendering quality.

483
00:28:11,440 --> 00:28:13,440
 Oftentimes, the shadows would look

484
00:28:13,440 --> 00:28:15,440
 just a little artificial,

485
00:28:15,440 --> 00:28:17,440
 or the lighting is a little too spotlight,

486
00:28:17,440 --> 00:28:19,440
 and the real image is more,

487
00:28:19,440 --> 00:28:21,440
 whatever.

488
00:28:21,440 --> 00:28:23,440
 It's almost always the material

489
00:28:23,440 --> 00:28:24,440
 that's rendering hard.

490
00:28:24,440 --> 00:28:26,440
 But that's not the part that,

491
00:28:26,440 --> 00:28:28,440
 I think the bigger part of the domain gap

492
00:28:28,440 --> 00:28:30,440
 that you might not think about,

493
00:28:30,440 --> 00:28:32,440
 is just that,

494
00:28:32,440 --> 00:28:34,440
 is like the random way

495
00:28:34,440 --> 00:28:36,440
 that I made these images.

496
00:28:36,440 --> 00:28:38,440
 I dropped objects from the sky,

497
00:28:38,440 --> 00:28:40,440
 and they rendered in some initial condition.

498
00:28:40,440 --> 00:28:42,440
 But if you look at real sinks,

499
00:28:42,440 --> 00:28:44,440
 probably people kind of put the plates

500
00:28:44,440 --> 00:28:46,440
 down first, and then the mugs.

501
00:28:46,440 --> 00:28:48,440
 There's some statistics of the environments

502
00:28:48,440 --> 00:28:50,440
 that I probably didn't capture perfectly

503
00:28:50,440 --> 00:28:52,440
 in the sink.

504
00:28:52,440 --> 00:28:53,440
 But there's probably,

505
00:28:53,440 --> 00:28:55,440
 I used 10 objects,

506
00:28:55,440 --> 00:28:57,440
 the ones I had 10 CAD files for.

507
00:28:57,440 --> 00:28:59,440
 And the real world is open world,

508
00:28:59,440 --> 00:29:01,440
 anybody could put anything in the sink.

509
00:29:01,440 --> 00:29:03,440
 So I think that's the domain gap

510
00:29:03,440 --> 00:29:05,440
 that's a bigger one.

511
00:29:05,440 --> 00:29:07,440
 It's more about the art assets,

512
00:29:07,440 --> 00:29:09,440
 and the distributions over

513
00:29:09,440 --> 00:29:11,440
 initial conditions,

514
00:29:11,440 --> 00:29:13,440
 than the render quality these days.

515
00:29:13,440 --> 00:29:15,440
 Yeah, sure.

516
00:29:15,440 --> 00:29:17,440
 (audience member speaking)

517
00:29:17,440 --> 00:29:19,440
 That's an awesome question.

518
00:29:19,440 --> 00:29:21,440
 So yeah, what about the noise?

519
00:29:21,440 --> 00:29:22,440
 So, I think that's a really good question.

520
00:29:22,440 --> 00:29:24,440
 I think that's a really good question.

521
00:29:24,440 --> 00:29:26,440
 I think that's a really good question.

522
00:29:26,440 --> 00:29:28,440
 I think that's a really good question.

523
00:29:28,440 --> 00:29:30,440
 I think that's a really good question.

524
00:29:30,440 --> 00:29:32,440
 I think that's a really good question.

525
00:29:32,440 --> 00:29:34,440
 I think that's a really good question.

526
00:29:34,440 --> 00:29:36,440
 I think that's a really good question.

527
00:29:36,440 --> 00:29:38,440
 I think that's a really good question.

528
00:29:38,440 --> 00:29:40,440
 I think that's a really good question.

529
00:29:40,440 --> 00:29:42,440
 I think that's a really good question.

530
00:29:42,440 --> 00:29:44,440
 I think that's a really good question.

531
00:29:44,440 --> 00:29:46,440
 I think that's a really good question.

532
00:29:46,440 --> 00:29:48,440
 I think that's a really good question.

533
00:29:48,440 --> 00:29:50,440
 I think that's a really good question.

534
00:29:50,440 --> 00:29:51,440
 I think that's a really good question.

535
00:29:51,440 --> 00:29:53,440
 I think that's a really good question.

536
00:29:53,440 --> 00:29:55,440
 I think that's a really good question.

537
00:29:55,440 --> 00:29:57,440
 I think that's a really good question.

538
00:29:57,440 --> 00:29:59,440
 I think that's a really good question.

539
00:29:59,440 --> 00:30:01,440
 I think that's a really good question.

540
00:30:01,440 --> 00:30:03,440
 I think that's a really good question.

541
00:30:03,440 --> 00:30:05,440
 I think that's a really good question.

542
00:30:05,440 --> 00:30:07,440
 I think that's a really good question.

543
00:30:07,440 --> 00:30:09,440
 I think that's a really good question.

544
00:30:09,440 --> 00:30:11,440
 I think that's a really good question.

545
00:30:11,440 --> 00:30:13,440
 I think that's a really good question.

546
00:30:13,440 --> 00:30:15,440
 I think that's a really good question.

547
00:30:15,440 --> 00:30:17,440
 I think that's a really good question.

548
00:30:17,440 --> 00:30:19,440
 I think that's a really good question.

549
00:30:19,440 --> 00:30:20,440
 I think that's a really good question.

550
00:30:20,440 --> 00:30:22,440
 I think that's a really good question.

551
00:30:22,440 --> 00:30:24,440
 I think that's a really good question.

552
00:30:24,440 --> 00:30:26,440
 I think that's a really good question.

553
00:30:26,440 --> 00:30:28,440
 I think that's a really good question.

554
00:30:28,440 --> 00:30:30,440
 I think that's a really good question.

555
00:30:30,440 --> 00:30:32,440
 I think that's a really good question.

556
00:30:32,440 --> 00:30:34,440
 I think that's a really good question.

557
00:30:34,440 --> 00:30:36,440
 I think that's a really good question.

558
00:30:36,440 --> 00:30:38,440
 I think that's a really good question.

559
00:30:38,440 --> 00:30:40,440
 I think that's a really good question.

560
00:30:40,440 --> 00:30:42,440
 I think that's a really good question.

561
00:30:42,440 --> 00:30:44,440
 I think that's a really good question.

562
00:30:44,440 --> 00:30:46,440
 I think that's a really good question.

563
00:30:46,440 --> 00:30:48,440
 I think that's a really good question.

564
00:30:48,440 --> 00:30:49,440
 I think that's a really good question.

565
00:30:49,440 --> 00:30:51,440
 I think that's a really good question.

566
00:30:51,440 --> 00:30:53,440
 I think that's a really good question.

567
00:30:53,440 --> 00:30:55,440
 I think that's a really good question.

568
00:30:55,440 --> 00:30:57,440
 I think that's a really good question.

569
00:30:57,440 --> 00:30:59,440
 I think that's a really good question.

570
00:30:59,440 --> 00:31:01,440
 I think that's a really good question.

571
00:31:01,440 --> 00:31:03,440
 I think that's a really good question.

572
00:31:03,440 --> 00:31:05,440
 I think that's a really good question.

573
00:31:05,440 --> 00:31:07,440
 I think that's a really good question.

574
00:31:07,440 --> 00:31:09,440
 I think that's a really good question.

575
00:31:09,440 --> 00:31:11,440
 I think that's a really good question.

576
00:31:11,440 --> 00:31:13,440
 I think that's a really good question.

577
00:31:13,440 --> 00:31:15,440
 I think that's a really good question.

578
00:31:15,440 --> 00:31:17,440
 I think that's a really good question.

579
00:31:17,440 --> 00:31:18,440
 I think that's a really good question.

580
00:31:18,440 --> 00:31:20,440
 I think that's a really good question.

581
00:31:20,440 --> 00:31:22,440
 I think that's a really good question.

582
00:31:22,440 --> 00:31:24,440
 I think that's a really good question.

583
00:31:24,440 --> 00:31:26,440
 I think that's a really good question.

584
00:31:26,440 --> 00:31:28,440
 I think that's a really good question.

585
00:31:28,440 --> 00:31:30,440
 I think that's a really good question.

586
00:31:30,440 --> 00:31:32,440
 I think that's a really good question.

587
00:31:32,440 --> 00:31:34,440
 I think that's a really good question.

588
00:31:34,440 --> 00:31:36,440
 I think that's a really good question.

589
00:31:36,440 --> 00:31:38,440
 I think that's a really good question.

590
00:31:38,440 --> 00:31:40,440
 I think that's a really good question.

591
00:31:40,440 --> 00:31:42,440
 I think that's a really good question.

592
00:31:42,440 --> 00:31:44,440
 I think that's a really good question.

593
00:31:44,440 --> 00:31:46,440
 I think that's a really good question.

594
00:31:46,440 --> 00:31:47,440
 I think that's a really good question.

595
00:31:47,440 --> 00:31:49,440
 I think that's a really good question.

596
00:31:49,440 --> 00:31:51,440
 I think that's a really good question.

597
00:31:51,440 --> 00:31:53,440
 I think that's a really good question.

598
00:31:53,440 --> 00:31:55,440
 I think that's a really good question.

599
00:31:55,440 --> 00:31:57,440
 I think that's a really good question.

600
00:31:57,440 --> 00:31:59,440
 I think that's a really good question.

601
00:31:59,440 --> 00:32:01,440
 I think that's a really good question.

602
00:32:01,440 --> 00:32:03,440
 I think that's a really good question.

603
00:32:03,440 --> 00:32:05,440
 I think that's a really good question.

604
00:32:05,440 --> 00:32:07,440
 I think that's a really good question.

605
00:32:07,440 --> 00:32:09,440
 I think that's a really good question.

606
00:32:09,440 --> 00:32:11,440
 I think that's a really good question.

607
00:32:11,440 --> 00:32:13,440
 I think that's a really good question.

608
00:32:13,440 --> 00:32:15,440
 I think that's a really good question.

609
00:32:15,440 --> 00:32:16,440
 I think that's a really good question.

610
00:32:16,440 --> 00:32:18,440
 I think that's a really good question.

611
00:32:18,440 --> 00:32:20,440
 I think that's a really good question.

612
00:32:20,440 --> 00:32:22,440
 I think that's a really good question.

613
00:32:22,440 --> 00:32:24,440
 I think that's a really good question.

614
00:32:24,440 --> 00:32:26,440
 I think that's a really good question.

615
00:32:26,440 --> 00:32:28,440
 I think that's a really good question.

616
00:32:28,440 --> 00:32:30,440
 I think that's a really good question.

617
00:32:30,440 --> 00:32:32,440
 I think that's a really good question.

618
00:32:32,440 --> 00:32:34,440
 I think that's a really good question.

619
00:32:34,440 --> 00:32:36,440
 I think that's a really good question.

620
00:32:36,440 --> 00:32:38,440
 I think that's a really good question.

621
00:32:38,440 --> 00:32:40,440
 I think that's a really good question.

622
00:32:40,440 --> 00:32:42,440
 I think that's a really good question.

623
00:32:42,440 --> 00:32:44,440
 I think that's a really good question.

624
00:32:44,440 --> 00:32:45,440
 I think that's a really good question.

625
00:32:45,440 --> 00:32:47,440
 I think that's a really good question.

626
00:32:47,440 --> 00:32:49,440
 I think that's a really good question.

627
00:32:49,440 --> 00:32:51,440
 I think that's a really good question.

628
00:32:51,440 --> 00:32:53,440
 I think that's a really good question.

629
00:32:53,440 --> 00:32:55,440
 I think that's a really good question.

630
00:32:55,440 --> 00:32:57,440
 I think that's a really good question.

631
00:32:57,440 --> 00:32:59,440
 I think that's a really good question.

632
00:32:59,440 --> 00:33:01,440
 I think that's a really good question.

633
00:33:01,440 --> 00:33:03,440
 I think that's a really good question.

634
00:33:03,440 --> 00:33:05,440
 I think that's a really good question.

635
00:33:05,440 --> 00:33:07,440
 I think that's a really good question.

636
00:33:07,440 --> 00:33:09,440
 I think that's a really good question.

637
00:33:09,440 --> 00:33:11,440
 I think that's a really good question.

638
00:33:11,440 --> 00:33:13,440
 I think that's a really good question.

639
00:33:13,440 --> 00:33:14,440
 I think that's a really good question.

640
00:33:14,440 --> 00:33:16,440
 I think that's a really good question.

641
00:33:16,440 --> 00:33:18,440
 I think that's a really good question.

642
00:33:18,440 --> 00:33:20,440
 I think that's a really good question.

643
00:33:20,440 --> 00:33:22,440
 I think that's a really good question.

644
00:33:22,440 --> 00:33:24,440
 I think that's a really good question.

645
00:33:24,440 --> 00:33:26,440
 I think that's a really good question.

646
00:33:26,440 --> 00:33:28,440
 I think that's a really good question.

647
00:33:28,440 --> 00:33:30,440
 I think that's a really good question.

648
00:33:30,440 --> 00:33:32,440
 I think that's a really good question.

649
00:33:32,440 --> 00:33:34,440
 I think that's a really good question.

650
00:33:34,440 --> 00:33:36,440
 I think that's a really good question.

651
00:33:36,440 --> 00:33:38,440
 I think that's a really good question.

652
00:33:38,440 --> 00:33:40,440
 I think that's a really good question.

653
00:33:40,440 --> 00:33:42,440
 I think that's a really good question.

654
00:33:42,440 --> 00:33:43,440
 I think that's a really good question.

655
00:33:43,440 --> 00:33:45,440
 I think that's a really good question.

656
00:33:45,440 --> 00:33:47,440
 I think that's a really good question.

657
00:33:47,440 --> 00:33:49,440
 I think that's a really good question.

658
00:33:49,440 --> 00:33:51,440
 I think that's a really good question.

659
00:33:51,440 --> 00:33:53,440
 I think that's a really good question.

660
00:33:53,440 --> 00:33:55,440
 I think that's a really good question.

661
00:33:55,440 --> 00:33:57,440
 I think that's a really good question.

662
00:33:57,440 --> 00:33:59,440
 I think that's a really good question.

663
00:33:59,440 --> 00:34:01,440
 I think that's a really good question.

664
00:34:01,440 --> 00:34:03,440
 I think that's a really good question.

665
00:34:03,440 --> 00:34:05,440
 I think that's a really good question.

666
00:34:05,440 --> 00:34:07,440
 I think that's a really good question.

667
00:34:07,440 --> 00:34:09,440
 I think that's a really good question.

668
00:34:09,440 --> 00:34:11,440
 I think that's a really good question.

669
00:34:11,440 --> 00:34:12,440
 I think that's a really good question.

670
00:34:12,440 --> 00:34:14,440
 I think that's a really good question.

671
00:34:14,440 --> 00:34:16,440
 I think that's a really good question.

672
00:34:16,440 --> 00:34:18,440
 I think that's a really good question.

673
00:34:18,440 --> 00:34:20,440
 I think that's a really good question.

674
00:34:20,440 --> 00:34:22,440
 I think that's a really good question.

675
00:34:22,440 --> 00:34:24,440
 I think that's a really good question.

676
00:34:24,440 --> 00:34:26,440
 I think that's a really good question.

677
00:34:26,440 --> 00:34:28,440
 I think that's a really good question.

678
00:34:28,440 --> 00:34:30,440
 I think that's a really good question.

679
00:34:30,440 --> 00:34:32,440
 I think that's a really good question.

680
00:34:32,440 --> 00:34:34,440
 I think that's a really good question.

681
00:34:34,440 --> 00:34:36,440
 I think that's a really good question.

682
00:34:36,440 --> 00:34:38,440
 I think that's a really good question.

683
00:34:38,440 --> 00:34:40,440
 I think that's a really good question.

684
00:34:40,440 --> 00:34:41,440
 I think that's a really good question.

685
00:34:41,440 --> 00:34:43,440
 I think that's a really good question.

686
00:34:43,440 --> 00:34:45,440
 I think that's a really good question.

687
00:34:45,440 --> 00:34:47,440
 I think that's a really good question.

688
00:34:47,440 --> 00:34:49,440
 I think that's a really good question.

689
00:34:49,440 --> 00:34:51,440
 I think that's a really good question.

690
00:34:51,440 --> 00:34:53,440
 I think that's a really good question.

691
00:34:53,440 --> 00:34:55,440
 I think that's a really good question.

692
00:34:55,440 --> 00:34:57,440
 I think that's a really good question.

693
00:34:57,440 --> 00:34:59,440
 I think that's a really good question.

694
00:34:59,440 --> 00:35:01,440
 I think that's a really good question.

695
00:35:01,440 --> 00:35:03,440
 I think that's a really good question.

696
00:35:03,440 --> 00:35:05,440
 I think that's a really good question.

697
00:35:05,440 --> 00:35:07,440
 I think that's a really good question.

698
00:35:07,440 --> 00:35:09,440
 I think that's a really good question.

699
00:35:09,440 --> 00:35:10,440
 I think that's a really good question.

700
00:35:10,440 --> 00:35:12,440
 I think that's a really good question.

701
00:35:12,440 --> 00:35:14,440
 I think that's a really good question.

702
00:35:14,440 --> 00:35:16,440
 I think that's a really good question.

703
00:35:16,440 --> 00:35:18,440
 I think that's a really good question.

704
00:35:18,440 --> 00:35:20,440
 I think that's a really good question.

705
00:35:20,440 --> 00:35:22,440
 I think that's a really good question.

706
00:35:22,440 --> 00:35:24,440
 I think that's a really good question.

707
00:35:24,440 --> 00:35:26,440
 I think that's a really good question.

708
00:35:26,440 --> 00:35:28,440
 I think that's a really good question.

709
00:35:28,440 --> 00:35:30,440
 I think that's a really good question.

710
00:35:30,440 --> 00:35:32,440
 I think that's a really good question.

711
00:35:32,440 --> 00:35:34,440
 I think that's a really good question.

712
00:35:34,440 --> 00:35:36,440
 I think that's a really good question.

713
00:35:36,440 --> 00:35:38,440
 I think that's a really good question.

714
00:35:38,440 --> 00:35:39,440
 I think that's a really good question.

715
00:35:39,440 --> 00:35:41,440
 I think that's a really good question.

716
00:35:41,440 --> 00:35:43,440
 I think that's a really good question.

717
00:35:43,440 --> 00:35:45,440
 I think that's a really good question.

718
00:35:45,440 --> 00:35:47,440
 I think that's a really good question.

719
00:35:47,440 --> 00:35:49,440
 I think that's a really good question.

720
00:35:49,440 --> 00:35:51,440
 I think that's a really good question.

721
00:35:51,440 --> 00:35:53,440
 I think that's a really good question.

722
00:35:53,440 --> 00:35:55,440
 I think that's a really good question.

723
00:35:55,440 --> 00:35:57,440
 I think that's a really good question.

724
00:35:57,440 --> 00:35:59,440
 I think that's a really good question.

725
00:35:59,440 --> 00:36:01,440
 I think that's a really good question.

726
00:36:01,440 --> 00:36:03,440
 I think that's a really good question.

727
00:36:03,440 --> 00:36:05,440
 I think that's a really good question.

728
00:36:05,440 --> 00:36:07,440
 I think that's a really good question.

729
00:36:07,440 --> 00:36:08,440
 I think that's a really good question.

730
00:36:08,440 --> 00:36:10,440
 I think that's a really good question.

731
00:36:10,440 --> 00:36:12,440
 I think that's a really good question.

732
00:36:12,440 --> 00:36:14,440
 I think that's a really good question.

733
00:36:14,440 --> 00:36:16,440
 I think that's a really good question.

734
00:36:16,440 --> 00:36:18,440
 I think that's a really good question.

735
00:36:18,440 --> 00:36:20,440
 I think that's a really good question.

736
00:36:20,440 --> 00:36:22,440
 I think that's a really good question.

737
00:36:22,440 --> 00:36:24,440
 I think that's a really good question.

738
00:36:24,440 --> 00:36:26,440
 I think that's a really good question.

739
00:36:26,440 --> 00:36:28,440
 I think that's a really good question.

740
00:36:28,440 --> 00:36:30,440
 I think that's a really good question.

741
00:36:30,440 --> 00:36:32,440
 I think that's a really good question.

742
00:36:32,440 --> 00:36:34,440
 I think that's a really good question.

743
00:36:34,440 --> 00:36:36,440
 I think that's a really good question.

744
00:36:36,440 --> 00:36:37,440
 I think that's a really good question.

745
00:36:37,440 --> 00:36:39,440
 I think that's a really good question.

746
00:36:39,440 --> 00:36:41,440
 I think that's a really good question.

747
00:36:41,440 --> 00:36:43,440
 I think that's a really good question.

748
00:36:43,440 --> 00:36:45,440
 I think that's a really good question.

749
00:36:45,440 --> 00:36:47,440
 I think that's a really good question.

750
00:36:47,440 --> 00:36:49,440
 I think that's a really good question.

751
00:36:49,440 --> 00:36:51,440
 I think that's a really good question.

752
00:36:51,440 --> 00:36:53,440
 I think that's a really good question.

753
00:36:53,440 --> 00:36:55,440
 I think that's a really good question.

754
00:36:55,440 --> 00:36:57,440
 I think that's a really good question.

755
00:36:57,440 --> 00:36:59,440
 I think that's a really good question.

756
00:36:59,440 --> 00:37:01,440
 I think that's a really good question.

757
00:37:01,440 --> 00:37:03,440
 I think that's a really good question.

758
00:37:03,440 --> 00:37:05,440
 I think that's a really good question.

759
00:37:05,440 --> 00:37:06,440
 I think that's a really good question.

760
00:37:06,440 --> 00:37:08,440
 I think that's a really good question.

761
00:37:08,440 --> 00:37:10,440
 I think that's a really good question.

762
00:37:10,440 --> 00:37:12,440
 I think that's a really good question.

763
00:37:12,440 --> 00:37:14,440
 I think that's a really good question.

764
00:37:14,440 --> 00:37:16,440
 I think that's a really good question.

765
00:37:16,440 --> 00:37:18,440
 I think that's a really good question.

766
00:37:18,440 --> 00:37:20,440
 I think that's a really good question.

767
00:37:20,440 --> 00:37:22,440
 I think that's a really good question.

768
00:37:22,440 --> 00:37:24,440
 I think that's a really good question.

769
00:37:24,440 --> 00:37:26,440
 I think that's a really good question.

770
00:37:26,440 --> 00:37:28,440
 I think that's a really good question.

771
00:37:28,440 --> 00:37:30,440
 I think that's a really good question.

772
00:37:30,440 --> 00:37:32,440
 I think that's a really good question.

773
00:37:32,440 --> 00:37:34,440
 I think that's a really good question.

774
00:37:34,440 --> 00:37:35,440
 I think that's a really good question.

775
00:37:35,440 --> 00:37:37,440
 I think that's a really good question.

776
00:37:37,440 --> 00:37:39,440
 I think that's a really good question.

777
00:37:39,440 --> 00:37:41,440
 I think that's a really good question.

778
00:37:41,440 --> 00:37:43,440
 I think that's a really good question.

779
00:37:43,440 --> 00:37:45,440
 I think that's a really good question.

780
00:37:45,440 --> 00:37:47,440
 I think that's a really good question.

781
00:37:47,440 --> 00:37:49,440
 I think that's a really good question.

782
00:37:49,440 --> 00:37:51,440
 I think that's a really good question.

783
00:37:51,440 --> 00:37:53,440
 I think that's a really good question.

784
00:37:53,440 --> 00:37:55,440
 I think that's a really good question.

785
00:37:55,440 --> 00:37:57,440
 I think that's a really good question.

786
00:37:57,440 --> 00:37:59,440
 I think that's a really good question.

787
00:37:59,440 --> 00:38:01,440
 I think that's a really good question.

788
00:38:01,440 --> 00:38:03,440
 I think that's a really good question.

789
00:38:03,440 --> 00:38:04,440
 I think that's a really good question.

790
00:38:04,440 --> 00:38:06,440
 I think that's a really good question.

791
00:38:06,440 --> 00:38:08,440
 I think that's a really good question.

792
00:38:08,440 --> 00:38:10,440
 I think that's a really good question.

793
00:38:10,440 --> 00:38:12,440
 I think that's a really good question.

794
00:38:12,440 --> 00:38:14,440
 I think that's a really good question.

795
00:38:14,440 --> 00:38:16,440
 I think that's a really good question.

796
00:38:16,440 --> 00:38:18,440
 I think that's a really good question.

797
00:38:18,440 --> 00:38:20,440
 I think that's a really good question.

798
00:38:20,440 --> 00:38:22,440
 I think that's a really good question.

799
00:38:22,440 --> 00:38:24,440
 I think that's a really good question.

800
00:38:24,440 --> 00:38:26,440
 I think that's a really good question.

801
00:38:26,440 --> 00:38:28,440
 I think that's a really good question.

802
00:38:28,440 --> 00:38:30,440
 I think that's a really good question.

803
00:38:30,440 --> 00:38:32,440
 I think that's a really good question.

804
00:38:32,440 --> 00:38:33,440
 I think that's a really good question.

805
00:38:33,440 --> 00:38:35,440
 I think that's a really good question.

806
00:38:35,440 --> 00:38:37,440
 I think that's a really good question.

807
00:38:37,440 --> 00:38:39,440
 I think that's a really good question.

808
00:38:39,440 --> 00:38:41,440
 I think that's a really good question.

809
00:38:41,440 --> 00:38:43,440
 I think that's a really good question.

810
00:38:43,440 --> 00:38:45,440
 I think that's a really good question.

811
00:38:45,440 --> 00:38:47,440
 I think that's a really good question.

812
00:38:47,440 --> 00:38:49,440
 I think that's a really good question.

813
00:38:49,440 --> 00:38:51,440
 I think that's a really good question.

814
00:38:51,440 --> 00:38:53,440
 I think that's a really good question.

815
00:38:53,440 --> 00:38:55,440
 I think that's a really good question.

816
00:38:55,440 --> 00:38:57,440
 I think that's a really good question.

817
00:38:57,440 --> 00:38:59,440
 I think that's a really good question.

818
00:38:59,440 --> 00:39:01,440
 I think that's a really good question.

819
00:39:01,440 --> 00:39:11,440
 So I similarly don't want to tell the entire deep learning optimization story,

820
00:39:11,440 --> 00:39:18,440
 but I just feel I want to say a few words to connect the type of optimization landscapes we've talked about,

821
00:39:18,440 --> 00:39:24,440
 since this is an optimization problem that's being solved, with what's happening here.

822
00:39:24,440 --> 00:39:30,440
 OK, so when we talked about nonlinear optimization before, right,

823
00:39:30,440 --> 00:39:50,440
 we said we're going to minimize f of x, for instance,

824
00:39:50,440 --> 00:39:54,440
 and maybe f of x was this complicated landscape.

825
00:39:55,440 --> 00:40:00,440
 One of the ways that you could do that is with just a gradient descent kind of algorithm, right?

826
00:40:00,440 --> 00:40:07,440
 So you have some initial guess, and you go downhill, and you land at a minima.

827
00:40:07,440 --> 00:40:14,440
 It's not guaranteed to be the global optima, but it would get you somewhere.

828
00:40:14,440 --> 00:40:17,440
 For inverse kinematics, this can be a real problem.

829
00:40:17,440 --> 00:40:20,440
 We can get stuck in bad local optima.

830
00:40:20,440 --> 00:40:22,440
 There might be a good solution for inverse kinematics,

831
00:40:22,440 --> 00:40:24,440
 and we don't find it with gradient descent.

832
00:40:24,440 --> 00:40:31,440
 Deep learning is very much using gradient descent, right?

833
00:40:31,440 --> 00:40:34,440
 It's using a stochastic version of gradient descent.

834
00:40:34,440 --> 00:40:40,440
 The standard way that it's stochastic is just by taking, if you have a huge pipe,

835
00:40:40,440 --> 00:40:43,440
 I have my 10,000 bucket images.

836
00:40:43,440 --> 00:40:48,440
 I'm going to take a small subset of them, 32 of them or something, at a time,

837
00:40:48,440 --> 00:40:51,440
 pass them through the network, and I'll pick a random 32 each time.

838
00:40:52,440 --> 00:40:56,440
 That gives me a random evaluation of my gradient,

839
00:40:56,440 --> 00:40:59,440
 and I'll do stochastic gradient descent to get down.

840
00:40:59,440 --> 00:41:03,440
 That's when you hear people talk about SGD, that's stochastic gradient descent.

841
00:41:03,440 --> 00:41:11,440
 But for the purposes of our discussion here, it's almost the same.

842
00:41:11,440 --> 00:41:16,440
 What can happen, you know, the stochastic version,

843
00:41:17,440 --> 00:41:22,440
 in a general optimization problem, you would think that it has a lot of the same properties.

844
00:41:22,440 --> 00:41:24,440
 It might walk downhill a little slower, right?

845
00:41:24,440 --> 00:41:29,440
 You might kind of take a meandering path down to the optima.

846
00:41:29,440 --> 00:41:36,440
 It might also, the stochastic version of it might bounce out of a local minima, by luck, you know.

847
00:41:36,440 --> 00:41:41,440
 But it's roughly this same sort of picture.

848
00:41:43,440 --> 00:41:48,440
 So, just like fine-tuning and transfer learning was just an amazing thing that happened in deep learning,

849
00:41:48,440 --> 00:41:55,440
 something else amazing happened, which meant that my ability to train this with high confidence and good solutions,

850
00:41:55,440 --> 00:42:00,440
 despite it solving this nonlinear optimization, it's somehow working.

851
00:42:00,440 --> 00:42:06,440
 And that's been one of the mysteries that people are doing a lot of work in deep learning theory to understand why.

852
00:42:06,440 --> 00:42:09,440
 Do people know the basic story of that?

853
00:42:09,440 --> 00:42:12,440
 Have people heard the basic story of that?

854
00:42:12,440 --> 00:42:17,440
 There's a couple big ideas.

855
00:42:17,440 --> 00:42:22,440
 One of them is over-parameterization.

856
00:42:22,440 --> 00:42:37,440
 The idea is that the pictures I draw here are wrong.

857
00:42:38,440 --> 00:42:42,440
 They're not the pictures for deep learning because we have so many parameters,

858
00:42:42,440 --> 00:42:46,440
 even compared to our data, that the landscape is so high-dimensional

859
00:42:46,440 --> 00:42:51,440
 that even though I have many nooks and crannies,

860
00:42:51,440 --> 00:42:58,440
 they're with high probability probably connected in some weird way in the super high-dimensional space.

861
00:42:58,440 --> 00:43:03,440
 In particular, I can talk as little as you guys want about this,

862
00:43:03,440 --> 00:43:06,440
 and there's people that know much more about it for sure,

863
00:43:07,440 --> 00:43:09,440
 but the basic story is,

864
00:43:09,440 --> 00:43:16,440
 if you have, let's say, the simplest version of it is this.

865
00:43:16,440 --> 00:43:19,440
 If my second to last layer in my network,

866
00:43:19,440 --> 00:43:25,440
 imagine I have the last layer here, and it's really big.

867
00:43:25,440 --> 00:43:30,440
 Let's say it's a million possible neurons in my second to last layer,

868
00:43:30,440 --> 00:43:34,440
 and then I have a function I'm trying to learn from x to y.

869
00:43:35,440 --> 00:43:39,440
 Even if this first part of the network is just completely random,

870
00:43:39,440 --> 00:43:44,440
 if I have random vectors here in some high-dimensional space,

871
00:43:44,440 --> 00:43:47,440
 then I can actually, with just my last layer,

872
00:43:47,440 --> 00:43:50,440
 fit most functions almost perfectly.

873
00:43:50,440 --> 00:43:55,440
 And this last layer is actually typically a least squares problem,

874
00:43:55,440 --> 00:44:01,440
 and I can expect that to work, and I can expect my training error to go to zero.

875
00:44:02,440 --> 00:44:06,440
 For big, complicated networks, just because I had a ridiculously large,

876
00:44:06,440 --> 00:44:09,440
 even random network to start with.

877
00:44:09,440 --> 00:44:13,440
 That idea is called the neural tangent kernel, if you want.

878
00:44:13,440 --> 00:44:19,440
 Or ultra-wide networks.

879
00:44:19,440 --> 00:44:26,440
 OK, and then the second thing that seems to happen,

880
00:44:26,440 --> 00:44:27,440
 seems to happen,

881
00:44:27,440 --> 00:44:38,440
 is something that people refer to as implicit regularization

882
00:44:38,440 --> 00:44:41,440
 of stochastic gradient descent.

883
00:44:41,440 --> 00:44:44,440
 Which is that,

884
00:44:44,440 --> 00:44:49,440
 when I'm, let's say after I've gotten my training error to zero,

885
00:44:49,440 --> 00:44:51,440
 I've got sort of random vectors here.

886
00:44:51,440 --> 00:44:54,440
 Gradient descent seems to do something good

887
00:44:55,440 --> 00:44:57,440
 in the null space of the optimization

888
00:44:57,440 --> 00:45:00,440
 that makes the weights here

889
00:45:00,440 --> 00:45:04,440
 solve the problem not just in an arbitrary way,

890
00:45:04,440 --> 00:45:07,440
 but it somehow chooses not random vectors in here

891
00:45:07,440 --> 00:45:11,440
 in a way that generalizes incredibly well to new problems

892
00:45:11,440 --> 00:45:13,440
 in the fine-tuning sort of story.

893
00:45:13,440 --> 00:45:17,440
 And that's been a big object of study in theoretical machine learning,

894
00:45:17,440 --> 00:45:20,440
 is trying to understand why and how

895
00:45:20,440 --> 00:45:24,440
 the things we're accidentally doing for gradient descent on these data sets

896
00:45:24,440 --> 00:45:27,440
 leads to strong generalization.

897
00:45:27,440 --> 00:45:34,440
 But there's two points I want you to have from the user perspective.

898
00:45:34,440 --> 00:45:39,440
 It is not the case,

899
00:45:39,440 --> 00:45:41,440
 some of you might disagree with me on this,

900
00:45:41,440 --> 00:45:45,440
 it is not the case that you can put an arbitrary cost function

901
00:45:45,440 --> 00:45:48,440
 on the end of these networks and experience success.

902
00:45:51,440 --> 00:45:57,440
 This pixel-wise cost that they use in Mask R-CNN,

903
00:45:57,440 --> 00:46:00,440
 the particular architecture they used,

904
00:46:00,440 --> 00:46:05,440
 leveraged these ideas in a deep way

905
00:46:05,440 --> 00:46:08,440
 and was very successful.

906
00:46:08,440 --> 00:46:10,440
 You could mess it up very easily.

907
00:46:10,440 --> 00:46:12,440
 It can't learn everything arbitrarily.

908
00:46:12,440 --> 00:46:16,440
 If it did, then I would be using it for inverse kinematics

909
00:46:16,440 --> 00:46:20,440
 and I would probably be mining Bitcoin or something like this, right?

910
00:46:20,440 --> 00:46:23,440
 There's things that we've figured out that it does extremely well

911
00:46:23,440 --> 00:46:26,440
 and there's things that don't fit in that framework yet.

912
00:46:26,440 --> 00:46:31,440
 A good example actually is if you were to do pose estimation.

913
00:46:31,440 --> 00:46:33,440
 I mentioned this before, right?

914
00:46:33,440 --> 00:46:35,440
 If you trained a network for pose estimation

915
00:46:35,440 --> 00:46:38,440
 and you chose rotation,

916
00:46:38,440 --> 00:46:40,440
 if you parameterized rotations badly,

917
00:46:40,440 --> 00:46:43,440
 the network would have a very hard time learning.

918
00:46:43,440 --> 00:46:47,440
 But other pose parameterizations work well

919
00:46:48,440 --> 00:46:49,440
 and that's because the landscape,

920
00:46:49,440 --> 00:46:51,440
 even in the high-dimensional space,

921
00:46:51,440 --> 00:46:53,440
 is more suitable for learning.

922
00:46:53,440 --> 00:46:57,440
 So unfortunately, the story's a little bit complicated,

923
00:46:57,440 --> 00:46:59,440
 but it is all connected.

924
00:46:59,440 --> 00:47:01,440
 There's all one truth here,

925
00:47:01,440 --> 00:47:03,440
 which is that I'm trying to do nonlinear optimization

926
00:47:03,440 --> 00:47:06,440
 and these neural networks are setting up a rich landscape

927
00:47:06,440 --> 00:47:11,440
 that works shockingly well when my kid wants to identify Legos.

928
00:47:11,440 --> 00:47:14,440
 That's like from here to there.

929
00:47:14,440 --> 00:47:16,440
 Yeah, okay.

930
00:47:17,440 --> 00:47:18,440
 Questions?

931
00:47:18,440 --> 00:47:20,440
 At that level of detail. Yes?

932
00:47:20,440 --> 00:47:25,440
 [inaudible]

933
00:47:25,440 --> 00:47:28,440
 We're increasingly understanding, yes,

934
00:47:28,440 --> 00:47:30,440
 that these are good things.

935
00:47:30,440 --> 00:47:32,440
 And people would argue about what are the most important features,

936
00:47:32,440 --> 00:47:38,440
 but I think both of these have got a lot of consensus behind them.

937
00:47:38,440 --> 00:47:40,440
 Yeah, the overparameterization story,

938
00:47:40,440 --> 00:47:43,440
 there are two different pieces of the puzzle.

939
00:47:44,440 --> 00:47:47,440
 The first thing is that for most deep learning problems,

940
00:47:47,440 --> 00:47:51,440
 we put ourselves in a regime where we get training error equals zero.

941
00:47:51,440 --> 00:47:55,440
 You basically, for your training set,

942
00:47:55,440 --> 00:48:01,440
 you expect to basically perfectly recover your desired training set.

943
00:48:01,440 --> 00:48:05,440
 And the reason that that is possible is this overparameterization story.

944
00:48:05,440 --> 00:48:08,440
 That's the training error equals zero part of the story.

945
00:48:08,440 --> 00:48:12,440
 Why does it generalize to new things out of your training set?

946
00:48:13,440 --> 00:48:15,440
 That's the implicit regularization story.

947
00:48:15,440 --> 00:48:17,440
 This is the generalization part of the story.

948
00:48:17,440 --> 00:48:22,440
 Both are amazing and deserve more study.

949
00:48:22,440 --> 00:48:30,440
 [inaudible]

950
00:48:30,440 --> 00:48:32,440
 Yes?

951
00:48:32,440 --> 00:48:36,440
 [inaudible]

952
00:48:36,440 --> 00:48:38,440
 Great question. Yeah, yeah.

953
00:48:38,440 --> 00:48:40,440
 So, okay, you've probably heard of overfitting.

954
00:48:41,440 --> 00:48:44,440
 So the concern, the classic picture of overfitting,

955
00:48:44,440 --> 00:48:53,440
 would be if I'm trying to regress some points,

956
00:48:53,440 --> 00:48:58,440
 maybe the right function is something like this,

957
00:48:58,440 --> 00:49:01,440
 and the points had a little bit of noise.

958
00:49:01,440 --> 00:49:08,440
 But really I wanted some simple function to come out,

959
00:49:09,440 --> 00:49:13,440
 and accept that the data was generated with a little bit of noise.

960
00:49:13,440 --> 00:49:17,440
 My overfit solution, if I'm really trying to get the training error

961
00:49:17,440 --> 00:49:21,440
 to be almost zero on this, might have, in order to fit the function,

962
00:49:21,440 --> 00:49:24,440
 might have done something like this, which is not the solution I was looking for,

963
00:49:24,440 --> 00:49:27,440
 but it set the training error to zero.

964
00:49:27,440 --> 00:49:34,440
 This is the story of implicit regularization,

965
00:49:34,440 --> 00:49:38,440
 is that it tends to somehow find the solutions that are more like this.

966
00:49:38,440 --> 00:49:42,440
 That seem to generalize better, and not this.

967
00:49:42,440 --> 00:49:44,440
 Okay?

968
00:49:44,440 --> 00:49:49,440
 There's very good theory in detail, I mean, the noise story of how does it fit,

969
00:49:49,440 --> 00:49:52,440
 how does it do both of those at the same time?

970
00:49:52,440 --> 00:49:54,440
 It sounds like that's inconsistent, but it's actually not inconsistent.

971
00:49:54,440 --> 00:49:57,440
 People understand that the function it is learning,

972
00:49:57,440 --> 00:50:01,440
 even in noise, you know, at least my mental image,

973
00:50:01,440 --> 00:50:04,440
 I think this is a continually moving story,

974
00:50:04,440 --> 00:50:06,440
 is that it actually learns something like this,

975
00:50:07,440 --> 00:50:09,440
 but if you zoom in, it's actually learning like little delta functions

976
00:50:09,440 --> 00:50:12,440
 that explain the noise, and it does actually get the training error to zero,

977
00:50:12,440 --> 00:50:16,440
 but it learns these beautiful generalizing functions.

978
00:50:16,440 --> 00:50:19,440
 Okay?

979
00:50:19,440 --> 00:50:21,440
 Theory of deep learning is an awesome topic.

980
00:50:21,440 --> 00:50:24,440
 This is a poor representation of it,

981
00:50:24,440 --> 00:50:28,440
 but maybe just enough for you to put it in context.

982
00:50:28,440 --> 00:50:34,440
 I'm happy to take more, you know, those are useful questions.

983
00:50:35,440 --> 00:50:37,440
 Okay.

984
00:50:37,440 --> 00:50:40,440
 So, can we play with it for a second?

985
00:50:40,440 --> 00:50:43,440
 So this is, I made, you know, I made a,

986
00:50:43,440 --> 00:50:46,440
 I actually made three notebooks, okay?

987
00:50:46,440 --> 00:50:48,440
 All of them are there.

988
00:50:48,440 --> 00:50:53,440
 Now, it's actually, Deep Note is completely awesome in every way,

989
00:50:53,440 --> 00:50:55,440
 almost every way.

990
00:50:55,440 --> 00:50:59,440
 They give free compute, you know, it's incredible,

991
00:50:59,440 --> 00:51:02,440
 it's got a good interface, it's most of the time works.

992
00:51:03,440 --> 00:51:05,440
 Their GPU support is not free.

993
00:51:05,440 --> 00:51:08,440
 That's the one, you know, it's a buy-in.

994
00:51:08,440 --> 00:51:10,440
 If you needed it for your project, honestly,

995
00:51:10,440 --> 00:51:12,440
 there's a chance I could ask and get you, you know,

996
00:51:12,440 --> 00:51:14,440
 like a special thing for it, because they really are,

997
00:51:14,440 --> 00:51:16,440
 they like the class.

998
00:51:16,440 --> 00:51:19,440
 It happens that one of them that's high up in the company did robotics,

999
00:51:19,440 --> 00:51:22,440
 so it's like, yes, we got an in.

1000
00:51:22,440 --> 00:51:26,440
 Okay, so, so, so that's great,

1001
00:51:26,440 --> 00:51:29,440
 but these are the three notebooks that we're going to,

1002
00:51:29,440 --> 00:51:32,440
 we actually point you to Google's Co-Laboratory instead of Deep Note.

1003
00:51:32,440 --> 00:51:35,440
 And the reason for that is that Co-Lab is just another online server,

1004
00:51:35,440 --> 00:51:38,440
 it's from Google, it happens to have a different pay structure

1005
00:51:38,440 --> 00:51:40,440
 and gives you GPUs.

1006
00:51:40,440 --> 00:51:42,440
 For training a deep network, you want a GPU.

1007
00:51:42,440 --> 00:51:44,440
 Okay?

1008
00:51:44,440 --> 00:51:46,440
 So I'm going to run it locally here, but it runs fine on,

1009
00:51:46,440 --> 00:51:48,440
 on Co-Lab.

1010
00:51:48,440 --> 00:51:52,440
 Okay, so, there's three notebooks.

1011
00:51:52,440 --> 00:51:54,440
 One of them was the data generation notebook,

1012
00:51:54,440 --> 00:51:56,440
 which I just, you probably don't want to run that ever.

1013
00:51:56,440 --> 00:51:58,440
 You could look at it and say like,

1014
00:51:58,440 --> 00:52:01,440
 oh, when you want to use it for your own pipeline, that's great.

1015
00:52:01,440 --> 00:52:06,440
 It's the thing that runs the clutter clearing for like a while,

1016
00:52:06,440 --> 00:52:10,440
 generates a huge file on my disk with all the labels and everything like that.

1017
00:52:10,440 --> 00:52:13,440
 Then there's the training, which runs for a long time,

1018
00:52:13,440 --> 00:52:16,440
 that will train the neural network,

1019
00:52:16,440 --> 00:52:21,440
 given pre-trained weights from Cocoa v1,

1020
00:52:21,440 --> 00:52:25,440
 which was pre-trained from ImageNet.

1021
00:52:25,440 --> 00:52:27,440
 Okay?

1022
00:52:27,440 --> 00:52:30,440
 And that works better than if I were to just train from scratch.

1023
00:52:30,440 --> 00:52:33,440
 The last one is the inference network.

1024
00:52:33,440 --> 00:52:35,440
 That's what I'm going to run now,

1025
00:52:35,440 --> 00:52:37,440
 which is just, I'm going to put a new image in.

1026
00:52:37,440 --> 00:52:39,440
 I'm going to just drop my bins again,

1027
00:52:39,440 --> 00:52:41,440
 new image in, and render the output.

1028
00:52:41,440 --> 00:52:43,440
 Okay, and we'll see how it works.

1029
00:52:43,440 --> 00:52:52,440
 It's interesting.

1030
00:52:52,440 --> 00:52:54,440
 So this is just in, it's using PyTorch.

1031
00:52:54,440 --> 00:52:56,440
 If I didn't say it, we were using PyTorch

1032
00:52:56,440 --> 00:52:58,440
 for these parts of the class.

1033
00:52:59,440 --> 00:53:04,440
 So, he said, "Make sure you tell them that if you're really running,

1034
00:53:04,440 --> 00:53:06,440
 you know, training PyTorch is great,

1035
00:53:06,440 --> 00:53:08,440
 but when you're running it on the robot,

1036
00:53:08,440 --> 00:53:10,440
 you should use TensorRT or something else

1037
00:53:10,440 --> 00:53:13,440
 that would compile it into a much faster, you know,

1038
00:53:13,440 --> 00:53:15,440
 PyTorch is not the fast inference engine.

1039
00:53:15,440 --> 00:53:17,440
 It's the great training, flexible thing,

1040
00:53:17,440 --> 00:53:21,440
 but you should compile it down into some more optimized code for runtime."

1041
00:53:21,440 --> 00:53:23,440
 Okay.

1042
00:53:23,440 --> 00:53:25,440
 The output of MaskRCNN, if you just say,

1043
00:53:25,440 --> 00:53:28,440
 "Give me an input, what's the output?"

1044
00:53:28,440 --> 00:53:31,440
 It has, oh, that's the model, sorry.

1045
00:53:31,440 --> 00:53:36,440
 If you look at the output,

1046
00:53:36,440 --> 00:53:38,440
 it gives you this big dictionary, right?

1047
00:53:38,440 --> 00:53:40,440
 It gives you a dictionary that has, like,

1048
00:53:40,440 --> 00:53:42,440
 it could do multiple images at a time.

1049
00:53:42,440 --> 00:53:45,440
 This is, for each image, it tells you a list of boxes

1050
00:53:45,440 --> 00:53:47,440
 that were possible detections,

1051
00:53:47,440 --> 00:53:50,440
 a list of labels for those boxes,

1052
00:53:50,440 --> 00:53:52,440
 which are the numbers I assigned in my training data.

1053
00:53:52,440 --> 00:53:55,440
 It gives you scores on how confident it is.

1054
00:53:56,440 --> 00:53:58,440
 It looks like in this one, it was actually very confident,

1055
00:53:58,440 --> 00:54:00,440
 very confident, very confident,

1056
00:54:00,440 --> 00:54:02,440
 and then not very confident at all.

1057
00:54:02,440 --> 00:54:05,440
 So we'll probably see something ridiculous on the last detection,

1058
00:54:05,440 --> 00:54:10,440
 because it's a 0.05 confidence compared to 0.99 for all the others.

1059
00:54:10,440 --> 00:54:12,440
 Okay.

1060
00:54:12,440 --> 00:54:15,440
 And then it gives me the images, which are the masks.

1061
00:54:15,440 --> 00:54:17,440
 That's a crazy thing to come up--

1062
00:54:17,440 --> 00:54:20,440
 let's just appreciate for a second that that's absolutely nuts,

1063
00:54:20,440 --> 00:54:22,440
 that a network would produce all that stuff.

1064
00:54:22,440 --> 00:54:25,440
 I mean, I remember-- I'm old now.

1065
00:54:25,440 --> 00:54:29,440
 So, like, we had projects with Jan Lekun, for instance, a while ago, right?

1066
00:54:29,440 --> 00:54:32,440
 And Jan Lekun used to come to our meetings,

1067
00:54:32,440 --> 00:54:34,440
 and he would bring a camera around

1068
00:54:34,440 --> 00:54:36,440
 and, like, train a neural network on the--

1069
00:54:36,440 --> 00:54:38,440
 a little convolutional neural network on the fly,

1070
00:54:38,440 --> 00:54:40,440
 and his demos were just always awesome.

1071
00:54:40,440 --> 00:54:42,440
 But I completely admit that I--

1072
00:54:42,440 --> 00:54:44,440
 every night, I would go home and be like,

1073
00:54:44,440 --> 00:54:46,440
 "Okay, but there's just-- that just doesn't scale.

1074
00:54:46,440 --> 00:54:48,440
 Like, what are you going to have, like,

1075
00:54:48,440 --> 00:54:51,440
 a million outputs for all the different possible labels on your network?

1076
00:54:51,440 --> 00:54:53,440
 Like, that's-- no one's ever going to do that."

1077
00:54:54,440 --> 00:54:55,440
 People do do that, right?

1078
00:54:55,440 --> 00:54:57,440
 You have, like, cats and dogs and elephants and everything.

1079
00:54:57,440 --> 00:55:01,440
 He always had three labels that he was training on the fly,

1080
00:55:01,440 --> 00:55:03,440
 and I thought that was great, but these things are enormous,

1081
00:55:03,440 --> 00:55:07,440
 massive, millions of parameters, millions of outputs,

1082
00:55:07,440 --> 00:55:11,440
 and it's all on the GPU and super fast.

1083
00:55:11,440 --> 00:55:14,440
 I'm running on the CPU now, by the way, so--

1084
00:55:14,440 --> 00:55:17,440
 This is my image in,

1085
00:55:17,440 --> 00:55:21,440
 and this is my masks out, okay?

1086
00:55:22,440 --> 00:55:24,440
 So let's take a look at your image.

1087
00:55:24,440 --> 00:55:27,440
 Mask number one, amazing, right?

1088
00:55:27,440 --> 00:55:29,440
 They found a mustard bottle.

1089
00:55:29,440 --> 00:55:31,440
 Mask number two,

1090
00:55:31,440 --> 00:55:34,440
 it's probably my Jell-O, right?

1091
00:55:34,440 --> 00:55:37,440
 The other Jell-O.

1092
00:55:37,440 --> 00:55:39,440
 Okay, and the last one is ridiculous, right?

1093
00:55:39,440 --> 00:55:42,440
 Because it was-- it told me it was going to be ridiculous, right?

1094
00:55:42,440 --> 00:55:44,440
 That one looks weird.

1095
00:55:44,440 --> 00:55:47,440
 Oh, that's like the occlude-- oh, wow, right?

1096
00:55:47,440 --> 00:55:49,440
 That's the occluded mustard bottle,

1097
00:55:50,440 --> 00:55:52,440
 and it just gave the-- it gave a pretty darn good--

1098
00:55:52,440 --> 00:55:54,440
 look at that, you can actually see the box cut out of it.

1099
00:55:54,440 --> 00:55:56,440
 That's actually incredibly good, right?

1100
00:55:56,440 --> 00:56:00,440
 And I didn't change anything.

1101
00:56:00,440 --> 00:56:04,440
 This is just the default parameters of everything, okay?

1102
00:56:04,440 --> 00:56:08,440
 Now let's change-- check the object detections.

1103
00:56:08,440 --> 00:56:11,440
 Okay, that's a bit embarrassing.

1104
00:56:11,440 --> 00:56:13,440
 Completely missed the Cheez-It box on the side.

1105
00:56:13,440 --> 00:56:15,440
 That is pretty funny, but--

1106
00:56:15,440 --> 00:56:19,440
 All right, the rest of them, incredibly good, right?

1107
00:56:19,440 --> 00:56:21,440
 Incredibly good.

1108
00:56:21,440 --> 00:56:23,440
 I wonder-- that might be-- I should have changed

1109
00:56:23,440 --> 00:56:25,440
 my region proposal network parameters, right?

1110
00:56:25,440 --> 00:56:27,440
 Maybe it didn't have one big enough to--

1111
00:56:27,440 --> 00:56:30,440
 to get the big flat Cheez-It box.

1112
00:56:30,440 --> 00:56:32,440
 It's probably dialed in for things that are about

1113
00:56:32,440 --> 00:56:35,440
 the size of a dog in a picture.

1114
00:56:35,440 --> 00:56:37,440
 Okay, right? Amazing.

1115
00:56:37,440 --> 00:56:40,440
 So each one of those gives a bounding box,

1116
00:56:40,440 --> 00:56:42,440
 which is the pixels, that's what--

1117
00:56:42,440 --> 00:56:44,440
 the corners of the pixels, and a label,

1118
00:56:44,440 --> 00:56:47,440
 which I can associate back with my text label.

1119
00:56:48,440 --> 00:56:49,440
 Okay, so that's--

1120
00:56:49,440 --> 00:56:52,440
 You can run it a few more times here.

1121
00:56:52,440 --> 00:56:56,440
 This is-- finds spam cans, finds domino cans,

1122
00:56:56,440 --> 00:56:59,440
 potted meat, sugar box.

1123
00:56:59,440 --> 00:57:01,440
 It's incredible, right?

1124
00:57:01,440 --> 00:57:03,440
 Absolutely incredible.

1125
00:57:03,440 --> 00:57:10,440
 Amazing, right?

1126
00:57:10,440 --> 00:57:13,440
 Okay, so that-- like, obviously we should use this

1127
00:57:13,440 --> 00:57:16,440
 in robotics. It's just so good.

1128
00:57:17,440 --> 00:57:19,440
 Any questions on that? Anybody-- you know,

1129
00:57:19,440 --> 00:57:22,440
 we could poke it, if you have, like, something else.

1130
00:57:22,440 --> 00:57:24,440
 I wanted to see what you try--

1131
00:57:24,440 --> 00:57:26,440
 I mean, I can't retrain it on my laptop now,

1132
00:57:26,440 --> 00:57:30,440
 but I could do any inference queries you're curious about.

1133
00:57:30,440 --> 00:57:32,440
 Or you can-- tonight.

1134
00:57:32,440 --> 00:57:34,440
 [Audience member] There was one that allowed us--

1135
00:57:34,440 --> 00:57:37,440
 that allowed us to visualize the region proposal.

1136
00:57:37,440 --> 00:57:39,440
 Could you go through that quickly?

1137
00:57:39,440 --> 00:57:41,440
 Yeah, sure.

1138
00:57:41,440 --> 00:57:43,440
 [Audience member] And see what happens with the Cheez-It box?

1139
00:57:43,440 --> 00:57:46,440
 So, the-- the-- the--

1140
00:57:46,440 --> 00:57:48,440
 So, there are--

1141
00:57:48,440 --> 00:57:50,440
 Okay, I could probably get--

1142
00:57:50,440 --> 00:57:56,440
 Let me do it so that it has the right image.

1143
00:57:56,440 --> 00:58:10,440
 The caveat here is there's, like,

1144
00:58:10,440 --> 00:58:15,440
 order a thousand or more images

1145
00:58:15,440 --> 00:58:17,440
 proposed, and I didn't visualize all of them.

1146
00:58:17,440 --> 00:58:21,440
 So, the test you-- I think your test is extremely good,

1147
00:58:21,440 --> 00:58:23,440
 but-- oh, okay, well, so we did learn here that--

1148
00:58:23,440 --> 00:58:25,440
 is that the boxes are at least big enough.

1149
00:58:25,440 --> 00:58:28,440
 So, my concern about them not being big enough is wrong.

1150
00:58:28,440 --> 00:58:32,440
 But I can't say that it didn't have a box around the Cheez-It.

1151
00:58:32,440 --> 00:58:34,440
 I would guess it probably did.

1152
00:58:34,440 --> 00:58:38,440
 There's, like, a thousand-- order a thousand region proposals.

1153
00:58:38,440 --> 00:58:41,440
 [Audience member] So, do you have another idea

1154
00:58:41,440 --> 00:58:44,440
 for what could have caused it to not pick up on the Cheez-It box?

1155
00:58:44,440 --> 00:58:46,440
 It could-- I mean, it could be that I never had

1156
00:58:46,440 --> 00:58:48,440
 a flat Cheez-It box in my data set.

1157
00:58:48,440 --> 00:58:53,440
 Could be not a perfect network, you know?

1158
00:58:53,440 --> 00:58:58,440
 That is the one-- as powerful and amazing as it is,

1159
00:58:58,440 --> 00:59:01,440
 when it doesn't work, the only recourse we have

1160
00:59:01,440 --> 00:59:03,440
 is to add more data, really.

1161
00:59:03,440 --> 00:59:05,440
 I mean, you could change parameters and retrain

1162
00:59:05,440 --> 00:59:08,440
 and do some hyperparameter sweeps, but--

1163
00:59:08,440 --> 00:59:12,440
 You know, a lot of the stuff we're talking about in this class,

1164
00:59:13,440 --> 00:59:15,440
 if it doesn't work, I could-- you know, we could tell you why.

1165
00:59:15,440 --> 00:59:17,440
 We could tell you how to debug it.

1166
00:59:17,440 --> 00:59:19,440
 Right now, this one, I can't.

1167
00:59:19,440 --> 00:59:21,440
 Yes?

1168
00:59:21,440 --> 00:59:23,440
 [Audience member] Wouldn't it predict, like, something

1169
00:59:23,440 --> 00:59:25,440
 which is familiar within the image map of Cocoa

1170
00:59:25,440 --> 00:59:29,440
 if it had, like, cornflakes, cornflakes boxes on there?

1171
00:59:29,440 --> 00:59:33,440
 So, the question is, why wouldn't it produce cornflakes

1172
00:59:33,440 --> 00:59:35,440
 or something from the Cocoa data set?

1173
00:59:35,440 --> 00:59:37,440
 That's a good question.

1174
00:59:37,440 --> 00:59:39,440
 So, it will never do that, because I've ripped off

1175
00:59:39,440 --> 00:59:42,440
 the Cocoa head, and the last layer is specific

1176
00:59:42,440 --> 00:59:44,440
 to my data set, so it will only ever say

1177
00:59:44,440 --> 00:59:46,440
 the things in my data set.

1178
00:59:46,440 --> 00:59:48,440
 It might be biased towards things that were

1179
00:59:48,440 --> 00:59:52,440
 in the Cocoa data set because of its pre-trained layers,

1180
00:59:52,440 --> 00:59:57,440
 but, you know, it has confidence thresholds

1181
00:59:57,440 --> 00:59:59,440
 that it will only report that the object was there

1182
00:59:59,440 --> 01:00:01,440
 if it was above some confidence threshold.

1183
01:00:01,440 --> 01:00:03,440
 So, it might be that there's a great box

1184
01:00:03,440 --> 01:00:05,440
 right around the Cheez-It, and it's just

1185
01:00:05,440 --> 01:00:07,440
 slightly below some confidence threshold.

1186
01:00:07,440 --> 01:00:11,440
 Good. Okay.

1187
01:00:11,440 --> 01:00:17,440
 I want to land a few more sort of high-level ideas.

1188
01:00:17,440 --> 01:00:19,440
 We can take our stretch today, yeah?

1189
01:00:19,440 --> 01:00:21,440
 Let's take our quick stretch.

1190
01:00:21,440 --> 01:00:23,440
 That's a good time.

1191
01:00:24,440 --> 01:00:25,440
 (Pause.)

1192
01:00:26,440 --> 01:00:27,440
 (Pause.)

1193
01:00:28,440 --> 01:00:29,440
 (Pause.)

1194
01:00:55,440 --> 01:00:59,440
 Okay. So, if you've learned one thing so far,

1195
01:00:59,440 --> 01:01:03,440
 or, you know, I think the MaskRCNN is going to be

1196
01:01:03,440 --> 01:01:06,440
 a tool that you will use if you understand

1197
01:01:06,440 --> 01:01:09,440
 its inputs and outputs. You're already going to have

1198
01:01:09,440 --> 01:01:11,440
 an incredibly powerful tool at your disposal,

1199
01:01:11,440 --> 01:01:14,440
 and maybe you picked up a few of the buzzwords

1200
01:01:14,440 --> 01:01:16,440
 from deep learning theory and the like

1201
01:01:16,440 --> 01:01:20,440
 that I would encourage you to study further.

1202
01:01:20,440 --> 01:01:23,440
 But I still think we haven't, I haven't told you

1203
01:01:24,440 --> 01:01:26,440
 the complete story yet about how to get

1204
01:01:26,440 --> 01:01:28,440
 big data for robotics, right?

1205
01:01:28,440 --> 01:01:30,440
 I told you two examples.

1206
01:01:30,440 --> 01:01:33,440
 The label fusion kind of idea, where we

1207
01:01:33,440 --> 01:01:38,440
 annotated our lab captured, and then the synthetic data.

1208
01:01:38,440 --> 01:01:42,440
 Both of those are somewhat limited, because,

1209
01:01:42,440 --> 01:01:44,440
 for instance, I only have a handful of different

1210
01:01:44,440 --> 01:01:46,440
 object models that I put in my simulator.

1211
01:01:46,440 --> 01:01:49,440
 I can generate as many of them as I want,

1212
01:01:49,440 --> 01:01:52,440
 but I don't have the diversity of the real world, right?

1213
01:01:53,440 --> 01:01:55,440
 And the same thing, what I can get in lab

1214
01:01:55,440 --> 01:01:57,440
 is not going to represent the diversity.

1215
01:01:57,440 --> 01:01:59,440
 If I want to think about open world manipulation,

1216
01:01:59,440 --> 01:02:01,440
 I want a robot that I got to program,

1217
01:02:01,440 --> 01:02:03,440
 it leaves, and it's going to manipulate

1218
01:02:03,440 --> 01:02:05,440
 anything in your house. I haven't given you

1219
01:02:05,440 --> 01:02:07,440
 an answer for that yet, and I don't have

1220
01:02:07,440 --> 01:02:09,440
 a complete answer yet, but this is what

1221
01:02:09,440 --> 01:02:11,440
 people are working on hard now.

1222
01:02:11,440 --> 01:02:13,440
 How do you have this kind of a tool chain

1223
01:02:13,440 --> 01:02:16,440
 that could manipulate incredibly large

1224
01:02:16,440 --> 01:02:18,440
 classes of objects?

1225
01:02:19,440 --> 01:02:20,440
 There was one more point I was going to make,

1226
01:02:20,440 --> 01:02:22,440
 but I think you know roughly that you

1227
01:02:22,440 --> 01:02:24,440
 could go from the mass Garcian end

1228
01:02:24,440 --> 01:02:27,440
 to the model-based grasp selection,

1229
01:02:27,440 --> 01:02:29,440
 or the antipodal grasp selection.

1230
01:02:29,440 --> 01:02:31,440
 Okay.

1231
01:02:31,440 --> 01:02:36,440
 So, I'll come back to those at the end

1232
01:02:36,440 --> 01:02:42,440
 to close things, but the big new trend

1233
01:02:42,440 --> 01:02:45,440
 is that we're going to have a lot more

1234
01:02:46,440 --> 01:02:48,440
 of this kind of self-supervised learning.

1235
01:02:48,440 --> 01:02:51,440
 The big new trend is self-supervised

1236
01:02:51,440 --> 01:02:53,440
 learning. I think many of you will

1237
01:02:53,440 --> 01:02:57,440
 have heard that also. In particular,

1238
01:02:57,440 --> 01:03:00,440
 we have this amazing property that if

1239
01:03:00,440 --> 01:03:03,440
 I've trained on ImageNet, on object

1240
01:03:03,440 --> 01:03:05,440
 detection for instance, then I could use

1241
01:03:05,440 --> 01:03:07,440
 those weights to help me do better

1242
01:03:07,440 --> 01:03:11,440
 on pixel-level segmentation.

1243
01:03:11,440 --> 01:03:14,440
 I trained on one task on a relevant

1244
01:03:15,440 --> 01:03:17,440
 layer, but I had a different task.

1245
01:03:17,440 --> 01:03:19,440
 So, if you open up your mind then and say,

1246
01:03:19,440 --> 01:03:21,440
 "Well, why did I pick object detection,

1247
01:03:21,440 --> 01:03:23,440
 which required human labels for my

1248
01:03:23,440 --> 01:03:25,440
 first task? Why don't I pick something

1249
01:03:25,440 --> 01:03:27,440
 that doesn't require human labels,

1250
01:03:27,440 --> 01:03:30,440
 that I can auto-label for my first task?

1251
01:03:30,440 --> 01:03:32,440
 I just need to pick some surrogate task

1252
01:03:32,440 --> 01:03:34,440
 that the network, in order to achieve,

1253
01:03:34,440 --> 01:03:36,440
 learns something relevant in those

1254
01:03:36,440 --> 01:03:38,440
 first layers to copy over."

1255
01:03:38,440 --> 01:03:42,440
 So, the new thing is find clever new

1256
01:03:42,440 --> 01:03:44,440
 tasks that don't require human

1257
01:03:44,440 --> 01:03:47,440
 supervision, unleash them on the entire

1258
01:03:47,440 --> 01:03:51,440
 internet, and use those backbones as

1259
01:03:51,440 --> 01:03:55,440
 pre-training for your run time.

1260
01:03:55,440 --> 01:03:58,440
 One of the most famous examples is

1261
01:03:58,440 --> 01:04:03,440
 SimClear, where the idea is very

1262
01:04:03,440 --> 01:04:05,440
 simple. It's basically, I'm going to

1263
01:04:05,440 --> 01:04:09,440
 take my original image of my dog, and

1264
01:04:09,440 --> 01:04:11,440
 I'm going to just start perturbing the

1265
01:04:11,440 --> 01:04:13,440
 image in lots of different ways.

1266
01:04:13,440 --> 01:04:15,440
 I'll crop and resize, whatever.

1267
01:04:15,440 --> 01:04:18,440
 Google basically threw the, this was a

1268
01:04:18,440 --> 01:04:20,440
 shotgun approach to research, which is

1269
01:04:20,440 --> 01:04:22,440
 powerful and good, but they're like,

1270
01:04:22,440 --> 01:04:24,440
 "Let's try every possible perturbation,

1271
01:04:24,440 --> 01:04:26,440
 and then we'll take the 10 that worked

1272
01:04:26,440 --> 01:04:28,440
 best, roughly, and we'll call that our

1273
01:04:28,440 --> 01:04:31,440
 algorithm." But they just absolutely

1274
01:04:31,440 --> 01:04:34,440
 tried all kinds of crazy stuff.

1275
01:04:34,440 --> 01:04:37,440
 And then most of these, one of the

1276
01:04:37,440 --> 01:04:39,440
 dominant ways to do self-supervised

1277
01:04:39,440 --> 01:04:41,440
 learning is to set up something where

1278
01:04:42,440 --> 01:04:44,440
 you take the training data, and you

1279
01:04:44,440 --> 01:04:46,440
 just compare and contrast things that

1280
01:04:46,440 --> 01:04:49,440
 you know to be true or false. So, this

1281
01:04:49,440 --> 01:04:51,440
 is a contrastive learning paradigm. The

1282
01:04:51,440 --> 01:04:53,440
 animation's a little annoying, but

1283
01:04:53,440 --> 01:04:56,440
 hopefully it gets the point across.

1284
01:04:56,440 --> 01:04:58,440
 So instead of having labels for the

1285
01:04:58,440 --> 01:05:01,440
 dog, and labels for the chair, it turns

1286
01:05:01,440 --> 01:05:05,440
 out to be enough to say, "The dog is

1287
01:05:05,440 --> 01:05:10,440
 not the chair." If you can say that

1288
01:05:11,440 --> 01:05:12,440
 this and this are the same image,

1289
01:05:12,440 --> 01:05:14,440
 because they're just perturbations of

1290
01:05:14,440 --> 01:05:16,440
 the same image, which you know to be

1291
01:05:16,440 --> 01:05:18,440
 true, you've constructed that by

1292
01:05:18,440 --> 01:05:20,440
 construction to be true, you say those

1293
01:05:20,440 --> 01:05:22,440
 are the same image, and those are not

1294
01:05:22,440 --> 01:05:24,440
 the same as the perturbations of the

1295
01:05:24,440 --> 01:05:26,440
 chair, then you don't need any human to

1296
01:05:26,440 --> 01:05:30,440
 annotate that. And what's amazing, okay,

1297
01:05:30,440 --> 01:05:34,440
 the general trend is that they,

1298
01:05:34,440 --> 01:05:37,440
 oftentimes, these don't do quite as well

1299
01:05:37,440 --> 01:05:40,440
 in peak performance, but for free, you

1300
01:05:40,440 --> 01:05:41,440
 know, you get so much, you can feed

1301
01:05:41,440 --> 01:05:43,440
 them with so much data that they do

1302
01:05:43,440 --> 01:05:45,440
 incredibly well. I mean, at some point

1303
01:05:45,440 --> 01:05:48,440
 you can actually outperform some of the

1304
01:05:48,440 --> 01:05:55,440
 original human labels. One more second,

1305
01:05:55,440 --> 01:05:57,440
 one second. Another example that's a

1306
01:05:57,440 --> 01:05:59,440
 little closer to robotics, which tends

1307
01:05:59,440 --> 01:06:01,440
 to be, I think, learning representations

1308
01:06:01,440 --> 01:06:04,440
 that are more about 3D understanding of

1309
01:06:04,440 --> 01:06:07,440
 the world, is monocular depth

1310
01:06:07,440 --> 01:06:09,440
 estimation. This is actually, this is

1311
01:06:09,440 --> 01:06:11,440
 the one that works the best right now,

1312
01:06:11,440 --> 01:06:13,440
 or one of the best right now, but the

1313
01:06:13,440 --> 01:06:15,440
 original idea is even simpler. So,

1314
01:06:15,440 --> 01:06:18,440
 imagine I have two cameras, and I could

1315
01:06:18,440 --> 01:06:20,440
 say, from two cameras I can figure out

1316
01:06:20,440 --> 01:06:23,440
 the depth using a stereo algorithm, and

1317
01:06:23,440 --> 01:06:25,440
 I want to train to be able to guess

1318
01:06:25,440 --> 01:06:28,440
 stereo from one camera. Okay, well, I'll

1319
01:06:28,440 --> 01:06:30,440
 just carry my two cameras around and use

1320
01:06:30,440 --> 01:06:33,440
 the two cameras to project the computed

1321
01:06:33,440 --> 01:06:35,440
 stereo to give me the ground truth

1322
01:06:35,440 --> 01:06:37,440
 answer, but just train the function from

1323
01:06:38,440 --> 01:06:40,440
 one camera, and you train monocular

1324
01:06:40,440 --> 01:06:43,440
 depth from an image to predict the

1325
01:06:43,440 --> 01:06:46,440
 depth. Okay, the newer architectures are

1326
01:06:46,440 --> 01:06:48,440
 a little bit more complicated. They're

1327
01:06:48,440 --> 01:06:50,440
 doing reconstruction. They take your

1328
01:06:50,440 --> 01:06:52,440
 current frame, you take your second

1329
01:06:52,440 --> 01:06:55,440
 frame, either from in time or alongside,

1330
01:06:55,440 --> 01:06:57,440
 you try to predict the relative, I mean,

1331
01:06:57,440 --> 01:06:59,440
 it's more complicated architectures now,

1332
01:06:59,440 --> 01:07:01,440
 but these work incredibly well, to the

1333
01:07:01,440 --> 01:07:04,440
 point where people can take just an RGB

1334
01:07:04,440 --> 01:07:06,440
 camera and move it around as if it's a

1335
01:07:07,440 --> 01:07:09,440
 depth camera, except, by the way, it still

1336
01:07:09,440 --> 01:07:11,440
 does well on blank walls, and by the way,

1337
01:07:11,440 --> 01:07:13,440
 it still does pretty well on invisible,

1338
01:07:13,440 --> 01:07:15,440
 you know, transparent objects and things

1339
01:07:15,440 --> 01:07:18,440
 like that. It's incredibly good.

1340
01:07:18,440 --> 01:07:21,440
 It's incredibly good. Sorry, did you have a question?

1341
01:07:21,440 --> 01:07:24,440
 [inaudible]

1342
01:07:24,440 --> 01:07:27,440
 Explain this? The cost function, instead

1343
01:07:27,440 --> 01:07:30,440
 of saying it is a dog or it is a chair,

1344
01:07:30,440 --> 01:07:33,440
 is to say, I want to, I'm going to

1345
01:07:33,440 --> 01:07:36,440
 predict an outcome that says that the

1346
01:07:36,440 --> 01:07:38,440
 dog and the chair are, the dog is not a

1347
01:07:38,440 --> 01:07:41,440
 chair, and the pieces of dog that have

1348
01:07:41,440 --> 01:07:43,440
 been translated are the same. So these

1349
01:07:43,440 --> 01:07:45,440
 are contrastive learnings. You take

1350
01:07:45,440 --> 01:07:47,440
 typically two images, you push them

1351
01:07:47,440 --> 01:07:50,440
 through. The ones that are the same, you

1352
01:07:50,440 --> 01:07:52,440
 try to make close together in some

1353
01:07:52,440 --> 01:07:54,440
 representation space, and the ones that

1354
01:07:54,440 --> 01:07:56,440
 are, you know to be different, you push

1355
01:07:56,440 --> 01:07:59,440
 them apart.

1356
01:07:59,440 --> 01:08:02,440
 [inaudible]

1357
01:08:03,440 --> 01:08:05,440
 I wouldn't ever say you couldn't do

1358
01:08:05,440 --> 01:08:08,440
 that with a deep learning perception

1359
01:08:08,440 --> 01:08:11,440
 system. What I would say is you need

1360
01:08:11,440 --> 01:08:14,440
 more data, maybe, or a bigger network,

1361
01:08:14,440 --> 01:08:18,440
 right, or more hours of SGD. Yeah, it's

1362
01:08:18,440 --> 01:08:21,440
 just a matter of levels of accuracy.

1363
01:08:21,440 --> 01:08:24,440
 Yes?

1364
01:08:24,440 --> 01:08:27,440
 [inaudible]

1365
01:08:28,440 --> 01:08:30,440
 Okay, that's a really good point. Good

1366
01:08:30,440 --> 01:08:33,440
 point. So, yeah, sorry, the point to say

1367
01:08:33,440 --> 01:08:36,440
 it into the microphone here is that I

1368
01:08:36,440 --> 01:08:39,440
 could have accidentally picked two

1369
01:08:39,440 --> 01:08:42,440
 pictures of the same dog in my data set,

1370
01:08:42,440 --> 01:08:44,440
 and then the contrastive learning is

1371
01:08:44,440 --> 01:08:46,440
 actually sort of wrong, right, because I

1372
01:08:46,440 --> 01:08:49,440
 could have had, I could have said this

1373
01:08:49,440 --> 01:08:52,440
 dog is not the dog. The objective is

1374
01:08:52,440 --> 01:08:54,440
 basically that that happens rarely

1375
01:08:55,440 --> 01:08:57,440
 enough in a big data set that it's okay.

1376
01:08:57,440 --> 01:09:00,440
 Yeah, good point, good point. Yeah,

1377
01:09:00,440 --> 01:09:03,440
 there's no, there's no label dog, no

1378
01:09:03,440 --> 01:09:05,440
 label chair anywhere here. It's just raw

1379
01:09:05,440 --> 01:09:12,440
 images. Good point. Okay, so this is

1380
01:09:12,440 --> 01:09:16,440
 actually, so Leroy's been playing with

1381
01:09:16,440 --> 01:09:19,440
 this kind of, the self-supervised

1382
01:09:19,440 --> 01:09:22,440
 paradigms, and asking about some of the

1383
01:09:22,440 --> 01:09:24,440
 harder questions about what it means to

1384
01:09:24,440 --> 01:09:26,440
 have representations for manipulation.

1385
01:09:26,440 --> 01:09:29,440
 And it's actually super interesting if

1386
01:09:29,440 --> 01:09:32,440
 you think about, I mean, everybody's on

1387
01:09:32,440 --> 01:09:35,440
 this quest for finding, you know, the big

1388
01:09:35,440 --> 01:09:38,440
 data moment in manipulation, right? And

1389
01:09:38,440 --> 01:09:40,440
 so we've been working with Amazon, who

1390
01:09:40,440 --> 01:09:42,440
 has big data, and they are doing

1391
01:09:42,440 --> 01:09:45,440
 manipulation, right? And Leroy started

1392
01:09:45,440 --> 01:09:47,440
 asking the question, you know, do they

1393
01:09:47,440 --> 01:09:49,440
 have enough data? How should we process

1394
01:09:49,440 --> 01:09:51,440
 that data in order to learn

1395
01:09:51,440 --> 01:09:53,440
 representations that would be

1396
01:09:53,440 --> 01:09:56,440
 incredibly powerful, okay? And the

1397
01:09:56,440 --> 01:09:58,440
 interesting thing that happens in a lot

1398
01:09:58,440 --> 01:10:00,440
 of these real-world applications is

1399
01:10:00,440 --> 01:10:03,440
 something called distribution shift. So

1400
01:10:03,440 --> 01:10:06,440
 if you think about, I mean, I took the

1401
01:10:06,440 --> 01:10:08,440
 same image three times, that was sort of

1402
01:10:08,440 --> 01:10:11,440
 antithetical to my point, but you can

1403
01:10:11,440 --> 01:10:14,440
 imagine if you have similar robots

1404
01:10:14,440 --> 01:10:17,440
 deployed in different warehouses, right,

1405
01:10:17,440 --> 01:10:19,440
 then they have very similar data coming

1406
01:10:19,440 --> 01:10:22,440
 in. But there's a big question of, should

1407
01:10:22,440 --> 01:10:24,440
 you train only, but they're a little

1408
01:10:24,440 --> 01:10:26,440
 bit different, like the, I'll show you a

1409
01:10:26,440 --> 01:10:28,440
 couple different specific ways that

1410
01:10:28,440 --> 01:10:31,440
 they're different, okay? So should the

1411
01:10:31,440 --> 01:10:34,440
 robot in New Jersey train on the

1412
01:10:34,440 --> 01:10:37,440
 perception data from Boston? Yes or no,

1413
01:10:37,440 --> 01:10:41,440
 right? It seems like it's more data, but

1414
01:10:41,440 --> 01:10:43,440
 the boxes in Boston might be

1415
01:10:43,440 --> 01:10:45,440
 statistically a little bit different

1416
01:10:45,440 --> 01:10:47,440
 than the boxes in New Jersey. Or the

1417
01:10:47,440 --> 01:10:49,440
 boxes at peak season might be more

1418
01:10:49,440 --> 01:10:51,440
 dense, for instance, on the conveyor

1419
01:10:51,440 --> 01:10:53,440
 belt than the boxes off peak, right?

1420
01:10:53,440 --> 01:10:56,440
 They have crazy shifts in distribution.

1421
01:10:56,440 --> 01:10:59,440
 So there's this question of more data

1422
01:10:59,440 --> 01:11:01,440
 is not actually, we know that more data

1423
01:11:01,440 --> 01:11:03,440
 is not always better, that if you have

1424
01:11:03,440 --> 01:11:05,440
 different data that is different,

1425
01:11:05,440 --> 01:11:07,440
 from a different distribution, it can

1426
01:11:07,440 --> 01:11:10,440
 sometimes hurt you. Sometimes it doesn't,

1427
01:11:10,440 --> 01:11:13,440
 but we know that it can. So there's a

1428
01:11:13,440 --> 01:11:15,440
 big question, how much should you share

1429
01:11:15,440 --> 01:11:17,440
 data, how much should you snarf up, how

1430
01:11:17,440 --> 01:11:20,440
 much should you specialize? And even

1431
01:11:20,440 --> 01:11:21,440
 more interesting, I won't talk much at

1432
01:11:21,440 --> 01:11:23,440
 all about this, is that you're doing

1433
01:11:23,440 --> 01:11:26,440
 this training in a decentralized way. So

1434
01:11:26,440 --> 01:11:28,440
 you have many different neural networks

1435
01:11:28,440 --> 01:11:30,440
 potentially living in different places

1436
01:11:30,440 --> 01:11:32,440
 and different copies of the neural

1437
01:11:32,440 --> 01:11:34,440
 network, how do you do the gradient

1438
01:11:34,440 --> 01:11:37,440
 descent update on that? Okay, but the

1439
01:11:37,440 --> 01:11:40,440
 distribution shift is very real in their

1440
01:11:40,440 --> 01:11:42,440
 data sets, right? They have different

1441
01:11:42,440 --> 01:11:44,440
 lighting conditions, different densities,

1442
01:11:44,440 --> 01:11:46,440
 they have different robots and

1443
01:11:46,440 --> 01:11:49,440
 upstream handling systems. Things like

1444
01:11:49,440 --> 01:11:51,440
 the sensors respond differently at

1445
01:11:51,440 --> 01:11:53,440
 different altitudes and stuff like this.

1446
01:11:53,440 --> 01:11:55,440
 They have different suction grippers in

1447
01:11:55,440 --> 01:11:59,440
 different places. So, you know, if you

1448
01:11:59,440 --> 01:12:01,440
 look at a few different locations, you

1449
01:12:01,440 --> 01:12:04,440
 might see very different types of

1450
01:12:04,440 --> 01:12:06,440
 packages or density of packages. This

1451
01:12:06,440 --> 01:12:10,440
 place does mostly, you know, the envelopes

1452
01:12:10,440 --> 01:12:13,440
 that you can almost recycle. And then,

1453
01:12:13,440 --> 01:12:15,440
 you know, this one has more of these,

1454
01:12:15,440 --> 01:12:17,440
 right, for instance. And sometimes you

1455
01:12:18,440 --> 01:12:21,440
 have very dense, sometimes you have very

1456
01:12:21,440 --> 01:12:23,440
 sparse. So it's just an interesting

1457
01:12:23,440 --> 01:12:25,440
 question of like self-supervised

1458
01:12:25,440 --> 01:12:27,440
 learning seems to work, but how exactly

1459
01:12:27,440 --> 01:12:29,440
 do you deploy it at the scale? And is

1460
01:12:29,440 --> 01:12:31,440
 it, you know, is it going to get us big

1461
01:12:31,440 --> 01:12:35,440
 data and manipulation? And this is the

1462
01:12:35,440 --> 01:12:40,440
 takeaway, is that if you just train

1463
01:12:40,440 --> 01:12:44,440
 directly on doing image segmentation

1464
01:12:44,440 --> 01:12:47,440
 with supervised learning, for instance,

1465
01:12:47,440 --> 01:12:50,440
 on your local data, you can actually

1466
01:12:50,440 --> 01:12:52,440
 overfit. I just, I said overfitting isn't

1467
01:12:52,440 --> 01:12:54,440
 as big of an issue anymore, but you can

1468
01:12:54,440 --> 01:12:57,440
 overfit still to your data and have

1469
01:12:57,440 --> 01:13:00,440
 worse performance if you start applying

1470
01:13:00,440 --> 01:13:02,440
 that across a distribution shift. You

1471
01:13:02,440 --> 01:13:04,440
 have limited robustness to distribution

1472
01:13:04,440 --> 01:13:06,440
 shift. If the, if peak season comes, your

1473
01:13:06,440 --> 01:13:08,440
 distribution moves, you can have overfit

1474
01:13:08,440 --> 01:13:13,440
 to your data. And there's a really big

1475
01:13:13,440 --> 01:13:15,440
 thing that seems to be happening, and

1476
01:13:16,440 --> 01:13:17,440
 I think you know this, you've seen like

1477
01:13:17,440 --> 01:13:19,440
 the GPT-3 and DALI and stable diffusion

1478
01:13:19,440 --> 01:13:21,440
 and all this craziness, right? But

1479
01:13:21,440 --> 01:13:23,440
 there's something about these self-

1480
01:13:23,440 --> 01:13:25,440
 supervised objectives that seem to be

1481
01:13:25,440 --> 01:13:27,440
 learning something more general about

1482
01:13:27,440 --> 01:13:30,440
 the internet or about the data in the

1483
01:13:30,440 --> 01:13:32,440
 warehouses. And they tend to be more

1484
01:13:32,440 --> 01:13:35,440
 robust to distribution shift. And this

1485
01:13:35,440 --> 01:13:39,440
 is like the big, big question in

1486
01:13:39,440 --> 01:13:41,440
 supervised learning, self-supervised

1487
01:13:41,440 --> 01:13:45,440
 learning for manipulation is what's the

1488
01:13:45,440 --> 01:13:46,440
 right way to learn these

1489
01:13:46,440 --> 01:13:49,440
 representations using as much data as

1490
01:13:49,440 --> 01:13:51,440
 possible that work for lots of

1491
01:13:51,440 --> 01:13:54,440
 downstream tasks? And the

1492
01:13:54,440 --> 01:13:56,440
 self-supervised objectives seem to be

1493
01:13:56,440 --> 01:13:58,440
 learning representations that transfer

1494
01:13:58,440 --> 01:14:02,440
 more generally than the supervised ones.

1495
01:14:02,440 --> 01:14:04,440
 It's almost like saying that this dog

1496
01:14:04,440 --> 01:14:06,440
 is not a chair forces me to learn

1497
01:14:06,440 --> 01:14:08,440
 something general about images and

1498
01:14:08,440 --> 01:14:10,440
 saying it's a dog, I could have

1499
01:14:10,440 --> 01:14:14,440
 special-case the dog.

1500
01:14:15,440 --> 01:14:17,440
 Okay, and the last thing I'll mention

1501
01:14:17,440 --> 01:14:22,440
 here is all the GPT-3, everything, it's

1502
01:14:22,440 --> 01:14:25,440
 coming into manipulation in a big way,

1503
01:14:25,440 --> 01:14:29,440
 which is even if we can't get enough

1504
01:14:29,440 --> 01:14:32,440
 labeled data or self-supervised data

1505
01:14:32,440 --> 01:14:34,440
 locally, people are asking the big

1506
01:14:34,440 --> 01:14:36,440
 questions of how do I take everybody

1507
01:14:36,440 --> 01:14:39,440
 else's foundation models, right? So the

1508
01:14:39,440 --> 01:14:41,440
 models that have been trained on huge

1509
01:14:42,440 --> 01:14:44,440
 language corpuses, huge vision

1510
01:14:44,440 --> 01:14:46,440
 corpuses, huge text-to-vision

1511
01:14:46,440 --> 01:14:48,440
 corpuses, and somehow use that

1512
01:14:48,440 --> 01:14:51,440
 information to augment my small data in

1513
01:14:51,440 --> 01:14:57,440
 robotics. And CLIP is the one that I

1514
01:14:57,440 --> 01:14:59,440
 think most roboticists have picked up

1515
01:14:59,440 --> 01:15:03,440
 of the big models. CLIP is a vision-to-text

1516
01:15:03,440 --> 01:15:06,440
 model and it's like every

1517
01:15:06,440 --> 01:15:08,440
 roboticist is like, oh, I could have

1518
01:15:08,440 --> 01:15:10,440
 taken my image and put it into this

1519
01:15:11,440 --> 01:15:12,440
 encoding and I could have a sentence, I

1520
01:15:12,440 --> 01:15:14,440
 could put that into the encoding. And so

1521
01:15:14,440 --> 01:15:16,440
 people are finding lots of different

1522
01:15:16,440 --> 01:15:19,440
 ways to use that. And the takeaway is

1523
01:15:19,440 --> 01:15:23,440
 that compared to the MaskRCNN pipeline

1524
01:15:23,440 --> 01:15:25,440
 which I talked about, which does

1525
01:15:25,440 --> 01:15:27,440
 reasonable things on the labels you've

1526
01:15:27,440 --> 01:15:29,440
 trained, these foundation models are

1527
01:15:29,440 --> 01:15:32,440
 getting us to the point where out of the

1528
01:15:32,440 --> 01:15:35,440
 box you now have potential labels from

1529
01:15:35,440 --> 01:15:37,440
 the entire Internet, like kind of the

1530
01:15:37,440 --> 01:15:39,440
 captions that everybody has put on the

1531
01:15:40,440 --> 01:15:41,440
 screen, you could just walk around the

1532
01:15:41,440 --> 01:15:44,440
 lab and point it at stuff and there's a

1533
01:15:44,440 --> 01:15:46,440
 chance it will label some, it will give

1534
01:15:46,440 --> 01:15:49,440
 you a sensible label out of the box in

1535
01:15:49,440 --> 01:15:51,440
 the open world, right? It's not

1536
01:15:51,440 --> 01:15:53,440
 perfect, it's not perfect, but it's

1537
01:15:53,440 --> 01:15:56,440
 mind-blowing. It's mind-blowing. So how

1538
01:15:56,440 --> 01:15:58,440
 do we use, even for the instance

1539
01:15:58,440 --> 01:16:00,440
 segmentation problem, how do you

1540
01:16:00,440 --> 01:16:03,440
 leverage super large data trained with

1541
01:16:03,440 --> 01:16:05,440
 self-supervised learning, even on a

1542
01:16:05,440 --> 01:16:09,440
 surrogate task, to make us pick up any

1543
01:16:09,440 --> 01:16:13,440
 object and manipulate any object? Okay.

1544
01:16:13,440 --> 01:16:15,440
 Good. So the instance segmentation is

1545
01:16:15,440 --> 01:16:17,440
 very much a geometry computer vision

1546
01:16:17,440 --> 01:16:20,440
 task. It is not enough for

1547
01:16:20,440 --> 01:16:22,440
 manipulation. If I want to know how much

1548
01:16:22,440 --> 01:16:25,440
 the objects weigh, right, Flickr

1549
01:16:25,440 --> 01:16:27,440
 doesn't have a data set that where

1550
01:16:27,440 --> 01:16:29,440
 people, everybody, I mean, actually

1551
01:16:29,440 --> 01:16:31,440
 probably does. You can probably say how

1552
01:16:31,440 --> 01:16:33,440
 much does that weigh and it would say,

1553
01:16:33,440 --> 01:16:35,440
 you know, 2.7 kilograms and it would

1554
01:16:35,440 --> 01:16:37,440
 probably be like almost right. But I'm

1555
01:16:38,440 --> 01:16:42,440
 not a data pipeline, right? So if you

1556
01:16:42,440 --> 01:16:44,440
 want to know like how, what's the

1557
01:16:44,440 --> 01:16:46,440
 friction, where's a good place to pick

1558
01:16:46,440 --> 01:16:48,440
 this up, right? That I don't think we

1559
01:16:48,440 --> 01:16:50,440
 have the answer directly from this.

1560
01:16:50,440 --> 01:16:52,440
 That's why I tried, I wanted to say in

1561
01:16:52,440 --> 01:16:54,440
 this lecture, I wanted to give you like

1562
01:16:54,440 --> 01:16:56,440
 a super fast overview of what computer

1563
01:16:56,440 --> 01:16:58,440
 vision relative to, you know, the

1564
01:16:58,440 --> 01:17:00,440
 standard computer vision pipelines for

1565
01:17:00,440 --> 01:17:02,440
 manipulation can look like. On

1566
01:17:02,440 --> 01:17:04,440
 Thursday we're going to say that the

1567
01:17:04,440 --> 01:17:06,440
 computer vision pipelines don't answer

1568
01:17:07,440 --> 01:17:08,440
 the question. We're going to use more

1569
01:17:08,440 --> 01:17:10,440
 than computer vision to pick things up.

1570
01:17:10,440 --> 01:17:12,440
 There's other properties of the

1571
01:17:12,440 --> 01:17:14,440
 object that we care about rather than

1572
01:17:14,440 --> 01:17:16,440
 just what it's every pixel label is.

1573
01:17:16,440 --> 01:17:18,440
 Okay, so we'll talk about that on

1574
01:17:18,440 --> 01:17:20,440
 Thursday. Any other big questions about

1575
01:17:20,440 --> 01:17:22,440
 that?

1576
01:17:22,440 --> 01:17:35,440
 [inaudible]

1577
01:17:36,440 --> 01:17:37,440
 There's going to be an entire

1578
01:17:37,440 --> 01:17:39,440
 lecture on it a little bit later.

1579
01:17:39,440 --> 01:17:42,440
 Yeah, so, but on the control side

1580
01:17:42,440 --> 01:17:44,440
 learning is having a big impact too

1581
01:17:44,440 --> 01:17:47,440
 for sure. And the biggest impact is

1582
01:17:47,440 --> 01:17:50,440
 connecting with vision. So a lot of

1583
01:17:50,440 --> 01:17:52,440
 our classic pipelines didn't have a way,

1584
01:17:52,440 --> 01:17:54,440
 they're incredibly good, but they

1585
01:17:54,440 --> 01:17:56,440
 didn't have a way to talk to cameras

1586
01:17:56,440 --> 01:17:59,440
 because that's handing a 640 by 480

1587
01:17:59,440 --> 01:18:01,440
 RGB image into a PID controller

1588
01:18:01,440 --> 01:18:05,440
 doesn't make sense. So we found,

1589
01:18:05,440 --> 01:18:06,440
 we're getting new tools for

1590
01:18:06,440 --> 01:18:09,440
 connecting those wires. Yes, good.

1591
01:18:09,440 --> 01:18:18,440
 [inaudible]

1592
01:18:18,440 --> 01:18:20,440
 Yeah, I didn't put it in. He told me I

1593
01:18:20,440 --> 01:18:22,440
 should put in this. Yeah, I read the

1594
01:18:22,440 --> 01:18:24,440
 paper. I read the paper, but I...

1595
01:18:25,440 --> 01:18:33,440
 [inaudible]

1596
01:18:33,440 --> 01:18:35,440
 There are more than models that do

1597
01:18:35,440 --> 01:18:37,440
 this much, much better. And it's like you

1598
01:18:37,440 --> 01:18:40,440
 input the text and like it could, can

1599
01:18:40,440 --> 01:18:42,440
 find the object really accurately with

1600
01:18:42,440 --> 01:18:44,440
 fine-grained detections. For example,

1601
01:18:44,440 --> 01:18:46,440
 like some people say, I put a sticker,

1602
01:18:46,440 --> 01:18:48,440
 a yellow sticker on my laptop. And now

1603
01:18:48,440 --> 01:18:51,440
 like I take a photo of a room with all

1604
01:18:51,440 --> 01:18:53,440
 the laptops and acquire with a laptop

1605
01:18:54,440 --> 01:18:56,440
 with yellow stickers. In the latest state

1606
01:18:56,440 --> 01:18:58,440
 of our model can do it pretty well. It

1607
01:18:58,440 --> 01:19:00,440
 will only give you like a design, the

1608
01:19:00,440 --> 01:19:02,440
 highest probability to the laptop with

1609
01:19:02,440 --> 01:19:05,440
 yellow sticker. And I think it's going

1610
01:19:05,440 --> 01:19:07,440
 to be very, very impactful in

1611
01:19:07,440 --> 01:19:10,440
 robotics because previously we could

1612
01:19:10,440 --> 01:19:12,440
 only do like a fixed, a fixed list of

1613
01:19:12,440 --> 01:19:14,440
 like 20 categories or like a hundred

1614
01:19:14,440 --> 01:19:16,440
 categories. Now it's just anything

1615
01:19:16,440 --> 01:19:18,440
 described as soon as you can describe

1616
01:19:18,440 --> 01:19:22,440
 it with language. Awesome. Yes. Thank

1617
01:19:23,440 --> 01:19:24,440
 you. So yeah, I think that is a part

1618
01:19:24,440 --> 01:19:27,440
 of, in my mind, a zoo of ways that

1619
01:19:27,440 --> 01:19:29,440
 people have found to put these large

1620
01:19:29,440 --> 01:19:31,440
 language models and connect them to

1621
01:19:31,440 --> 01:19:35,440
 manipulation. Right. Good. Any other?

1622
01:19:35,440 --> 01:19:37,440
 I mean, any other big commentaries?

1623
01:19:37,440 --> 01:19:43,440
 That's good. Okay. I'll hang around

1624
01:19:43,440 --> 01:19:45,440
 outside. I think we have a lecture

1625
01:19:45,440 --> 01:19:47,440
 coming in. So I'll hang around outside

1626
01:19:47,440 --> 01:19:49,440
 if anybody wants to talk about

1627
01:19:49,440 --> 01:19:51,440
 projects, but I will see you Thursday.

1628
01:19:51,440 --> 01:20:01,440
 [BLANK_AUDIO]

