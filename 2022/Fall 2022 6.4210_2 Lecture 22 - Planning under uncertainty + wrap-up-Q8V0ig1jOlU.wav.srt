1
00:00:00,000 --> 00:00:03,000
 [INAUDIBLE]

2
00:00:03,000 --> 00:00:06,480
 So the real problem is not that [INAUDIBLE]

3
00:00:06,480 --> 00:00:10,460
 I'm just asking what causes the sale, what do you do?

4
00:00:10,460 --> 00:00:18,480
 And I think that the second half of the strategy

5
00:00:18,480 --> 00:00:22,460
 is to reduce the number of cash that catch this growth.

6
00:00:22,460 --> 00:00:25,460
 Because we do not need to do it with cash all the time.

7
00:00:25,460 --> 00:00:28,960
 So the second half, the cash growth, I mean,

8
00:00:28,960 --> 00:00:31,960
 if this was a reason, I'm not in an ideal trajectory.

9
00:00:31,960 --> 00:00:32,960
 [INAUDIBLE]

10
00:00:32,960 --> 00:00:49,800
 OK.

11
00:00:49,800 --> 00:00:51,080
 Welcome back, everybody.

12
00:00:51,080 --> 00:00:55,980
 So today I'm going to try to split the lecture roughly.

13
00:00:55,980 --> 00:00:58,240
 I mean, probably do a little bit more than half

14
00:00:58,240 --> 00:00:59,440
 on planning under uncertainty.

15
00:00:59,440 --> 00:01:00,600
 There's a lot to say there.

16
00:01:00,600 --> 00:01:02,920
 But I think in the spirit of this

17
00:01:02,920 --> 00:01:05,000
 is a boutique lecture, kind of at the end,

18
00:01:05,000 --> 00:01:06,800
 we're going to give you some of the reasons

19
00:01:06,800 --> 00:01:08,500
 why you might want to learn more and just

20
00:01:08,500 --> 00:01:12,520
 some of the key ideas of planning under uncertainty.

21
00:01:12,520 --> 00:01:14,680
 And then I'd really like to spend a little bit of time

22
00:01:14,680 --> 00:01:17,400
 just kind of summarizing what we did.

23
00:01:17,400 --> 00:01:19,400
 We picked up a lot of tools.

24
00:01:19,400 --> 00:01:22,920
 I think the connective tissue that puts them all together

25
00:01:22,920 --> 00:01:24,360
 is still forming.

26
00:01:24,360 --> 00:01:27,040
 And I think it's just useful to kind of make

27
00:01:27,040 --> 00:01:28,400
 some of those connections again.

28
00:01:28,400 --> 00:01:30,120
 Remember what we learned, why we learned them,

29
00:01:30,120 --> 00:01:32,120
 how they've come up in your projects,

30
00:01:32,120 --> 00:01:34,800
 and wrap it up like that.

31
00:01:34,800 --> 00:01:38,280
 So let's give a few examples about why

32
00:01:38,280 --> 00:01:40,680
 you might want to do planning under uncertainty.

33
00:01:40,680 --> 00:01:42,680
 So maybe--

34
00:01:42,680 --> 00:01:43,640
 [INAUDIBLE]

35
00:01:43,640 --> 00:01:52,640
 Maybe one example would be at the task level, right?

36
00:01:52,640 --> 00:01:57,880
 So if I asked one of Boolean's chatbots or whatever,

37
00:01:57,880 --> 00:02:03,080
 if I said, hey, robot, get me-- I don't know,

38
00:02:03,080 --> 00:02:05,120
 I have to think of something that has uncertainty,

39
00:02:05,120 --> 00:02:06,600
 but get me the mustard.

40
00:02:06,600 --> 00:02:24,120
 And maybe because of our massive understanding of the way

41
00:02:24,120 --> 00:02:26,240
 kitchens are laid out and where people put things,

42
00:02:26,240 --> 00:02:30,000
 maybe there's, I don't know, a 40% chance

43
00:02:30,000 --> 00:02:32,080
 that it's in the fridge.

44
00:02:32,080 --> 00:02:34,280
 I don't know if you refrigerate your mustard or not.

45
00:02:34,280 --> 00:02:40,240
 I know it's a personal choice, right?

46
00:02:40,240 --> 00:02:42,280
 Maybe there's a 30% chance it's in the cupboard.

47
00:02:42,280 --> 00:02:48,560
 Maybe there's a 10% chance there's one in the pantry.

48
00:02:48,560 --> 00:02:55,040
 And maybe, I don't know, a 20% chance we're just out.

49
00:02:55,040 --> 00:02:57,720
 You've got to go to the store, right?

50
00:02:57,720 --> 00:03:04,160
 So a planner thinking about the task level that's only

51
00:03:04,160 --> 00:03:08,520
 thinking about, let's say, the most likely scenario

52
00:03:08,520 --> 00:03:10,680
 is going to be impoverished in its ability

53
00:03:10,680 --> 00:03:13,600
 to accomplish the task, for sure, right?

54
00:03:13,600 --> 00:03:17,000
 But not only in the total success, right?

55
00:03:17,000 --> 00:03:19,200
 If it's not in the fridge, it fails.

56
00:03:19,200 --> 00:03:21,120
 But also, maybe if the robot is already

57
00:03:21,120 --> 00:03:22,480
 in the pantry for something else,

58
00:03:22,480 --> 00:03:24,000
 even if it's a low probability, maybe

59
00:03:24,000 --> 00:03:26,040
 it's worth a quick check before you take the time

60
00:03:26,040 --> 00:03:28,600
 to drive over to the fridge, right?

61
00:03:28,600 --> 00:03:33,680
 So reasoning about probabilities at the level of decision making

62
00:03:33,680 --> 00:03:36,120
 is hugely important.

63
00:03:36,120 --> 00:03:42,080
 And I think at the task level, it's sort of clear, I think.

64
00:03:42,080 --> 00:03:45,720
 I guess with all the large language models

65
00:03:45,720 --> 00:03:49,080
 that Bojan was talking about, maybe there's a 10% chance

66
00:03:49,080 --> 00:03:52,760
 that it'll tell you to pour water over your head

67
00:03:52,760 --> 00:03:54,260
 or something like this, too, right?

68
00:03:54,260 --> 00:03:57,880
 But that's the wild west.

69
00:03:57,880 --> 00:04:00,200
 OK, but it turns out it's super useful down

70
00:04:00,200 --> 00:04:04,320
 at the dexterous manipulation control level, too.

71
00:04:04,320 --> 00:04:06,120
 So let me think of a good example.

72
00:04:06,120 --> 00:04:08,320
 So I actually brought a plate so I can make--

73
00:04:08,320 --> 00:04:11,720
 one of my favorite examples is one

74
00:04:11,720 --> 00:04:13,960
 that came up when we were loading dishes a lot, right?

75
00:04:13,960 --> 00:04:15,240
 So I brought a dish.

76
00:04:15,240 --> 00:04:18,920
 I would have brought the dish rack, but that didn't work out.

77
00:04:18,920 --> 00:04:21,480
 OK, so it turns out the way that the robot--

78
00:04:21,480 --> 00:04:26,240
 imagine this is the rack in the bottom of the dishwasher, OK?

79
00:04:26,240 --> 00:04:28,560
 And the way that our robot loaded dishes

80
00:04:28,560 --> 00:04:31,800
 was very characteristically, it would grab the plate,

81
00:04:31,800 --> 00:04:33,920
 have a pretty good thing, and then it

82
00:04:33,920 --> 00:04:37,920
 would line up the plate above and go down, right?

83
00:04:37,920 --> 00:04:39,520
 And that always bugged me.

84
00:04:39,520 --> 00:04:40,640
 It always bugged me, right?

85
00:04:40,640 --> 00:04:42,840
 Because humans don't do that whatsoever.

86
00:04:42,840 --> 00:04:44,640
 They do it much faster, first of all,

87
00:04:44,640 --> 00:04:46,520
 but they also take a fundamentally different

88
00:04:46,520 --> 00:04:47,240
 strategy, right?

89
00:04:47,240 --> 00:04:48,120
 They come in.

90
00:04:48,120 --> 00:04:50,480
 They almost always make contact.

91
00:04:50,480 --> 00:04:53,640
 They're compliant, and they go in like this.

92
00:04:53,640 --> 00:04:54,760
 Does that make sense, right?

93
00:04:54,760 --> 00:04:57,640
 Instead of having a sort of straight line trajectory

94
00:04:57,640 --> 00:05:02,520
 where the plate-- you kind of estimate, possibly

95
00:05:02,520 --> 00:05:09,280
 with some accuracy, the location of the tines in the tray, right?

96
00:05:09,280 --> 00:05:14,360
 And line up the plate perfectly and try to put it straight down.

97
00:05:14,360 --> 00:05:18,880
 And I think a human would come in at an angle,

98
00:05:18,880 --> 00:05:23,080
 intentionally make contact here, then passive mechanics

99
00:05:23,080 --> 00:05:24,660
 would have it rotate up a little bit,

100
00:05:24,660 --> 00:05:27,820
 and it would kind of slide down.

101
00:05:27,820 --> 00:05:31,760
 The only explanation for that is something

102
00:05:31,760 --> 00:05:35,860
 about being more robust, something about uncertainty,

103
00:05:35,860 --> 00:05:38,960
 not needing to accurately understand

104
00:05:38,960 --> 00:05:42,040
 the orientation of the plate or the position of the tines,

105
00:05:42,040 --> 00:05:42,820
 right?

106
00:05:42,820 --> 00:05:45,360
 And it's actually just a fundamentally different

107
00:05:45,360 --> 00:05:47,600
 strategy at the level of the controller

108
00:05:47,600 --> 00:05:52,880
 because we are implicitly thinking about uncertainty.

109
00:05:52,880 --> 00:05:58,200
 We saw another one, too, in our journey here, right?

110
00:05:58,200 --> 00:06:00,440
 Remember the example of pushing the book?

111
00:06:00,440 --> 00:06:06,520
 This was the example of force control

112
00:06:06,520 --> 00:06:11,080
 where you wanted to control the friction cones between the finger

113
00:06:11,080 --> 00:06:11,760
 and the book.

114
00:06:11,760 --> 00:06:13,360
 You also wanted the book and the table.

115
00:06:14,160 --> 00:06:18,560
 [AUDIO OUT]

116
00:06:18,560 --> 00:06:22,200
 But right there, what's that motion right there, right?

117
00:06:22,200 --> 00:06:24,320
 And then he's going to go around and pick it up.

118
00:06:24,320 --> 00:06:28,640
 The only justification for that second motion

119
00:06:28,640 --> 00:06:31,280
 was that he could reduce the uncertainty

120
00:06:31,280 --> 00:06:33,320
 of the orientation of the book, right?

121
00:06:33,320 --> 00:06:36,360
 So as he slid it around, he even rotated the book,

122
00:06:36,360 --> 00:06:38,640
 it was all about the friction cones, right?

123
00:06:38,640 --> 00:06:40,800
 And the exact orientation of the book

124
00:06:40,800 --> 00:06:44,880
 was some relatively subtle function of the friction cones.

125
00:06:44,880 --> 00:06:48,880
 But by coming in with this known position of the fingers,

126
00:06:48,880 --> 00:06:51,360
 even if the book was at a relatively different orientation,

127
00:06:51,360 --> 00:06:54,360
 if he starts pushing, it's going to line the book up nicely.

128
00:06:54,360 --> 00:06:56,400
 It's a very robust strategy that would get it

129
00:06:56,400 --> 00:06:58,560
 to the end of the table at a known location.

130
00:06:58,560 --> 00:07:00,720
 That's the only justification for that middle move.

131
00:07:00,720 --> 00:07:03,320
 Otherwise, it was completely worthless, right?

132
00:07:03,320 --> 00:07:05,560
 So even at the level of planning control,

133
00:07:05,560 --> 00:07:07,280
 reasoning about uncertainty really matters.

134
00:07:10,240 --> 00:07:12,640
 And I think the hallmarks of a system that

135
00:07:12,640 --> 00:07:16,160
 is reasoning about uncertainty and planning and control

136
00:07:16,160 --> 00:07:17,200
 are pretty clear, right?

137
00:07:17,200 --> 00:07:37,640
 So one of them is certainly just a level of robustness

138
00:07:37,640 --> 00:07:40,080
 that you can obtain if you're thinking about all the things

139
00:07:40,080 --> 00:07:43,280
 that could possibly happen, not just planning optimistically

140
00:07:43,280 --> 00:07:44,920
 through the world, right?

141
00:07:44,920 --> 00:07:47,000
 And there's various ways to talk about robustness.

142
00:07:47,000 --> 00:07:50,400
 And some of them I would certainly

143
00:07:50,400 --> 00:07:52,860
 put under this umbrella, but there's other approaches that

144
00:07:52,860 --> 00:07:54,360
 can achieve robustness, I guess.

145
00:07:54,360 --> 00:07:58,560
 But I think the absolute hallmark identifying feature

146
00:07:58,560 --> 00:08:00,520
 of a system that's reasoning about uncertainty

147
00:08:00,520 --> 00:08:04,440
 when it's making its decisions is information

148
00:08:04,440 --> 00:08:05,320
 gathering actions.

149
00:08:06,200 --> 00:08:12,120
 We will see examples throughout the lecture

150
00:08:12,120 --> 00:08:15,240
 here of cases where the robot does something fundamentally

151
00:08:15,240 --> 00:08:15,760
 different.

152
00:08:15,760 --> 00:08:17,880
 You need to program it fundamentally different,

153
00:08:17,880 --> 00:08:19,400
 not for the sake of accomplishing

154
00:08:19,400 --> 00:08:21,720
 the task of getting the book to a certain orientation,

155
00:08:21,720 --> 00:08:24,080
 but actually just for the sake of gaining information,

156
00:08:24,080 --> 00:08:28,040
 reducing uncertainty, and using that reduced uncertainty

157
00:08:28,040 --> 00:08:29,960
 to accomplish the task with higher confidence.

158
00:08:29,960 --> 00:08:31,880
 So I think that's the hallmark of a system

159
00:08:31,880 --> 00:08:33,280
 that's reasoning about robustness.

160
00:08:33,280 --> 00:08:35,520
 Using uncertainty to accomplish the task with higher

161
00:08:35,520 --> 00:08:36,520
 confidence.

162
00:08:36,520 --> 00:08:40,320
 So really, if you don't have uncertainty

163
00:08:40,320 --> 00:08:42,480
 flowing through your system, you will never

164
00:08:42,480 --> 00:08:45,880
 see robots making actions just to gain information.

165
00:08:45,880 --> 00:08:49,080
 That's a property that only happens

166
00:08:49,080 --> 00:08:52,160
 if you're trying to think about optimizing uncertainty

167
00:08:52,160 --> 00:08:53,400
 or something like that.

168
00:08:53,400 --> 00:08:59,520
 OK, so you need a whole stack to start

169
00:08:59,520 --> 00:09:00,760
 reasoning about uncertainty.

170
00:09:00,760 --> 00:09:03,400
 You need to think about uncertainty

171
00:09:03,400 --> 00:09:05,680
 at the level of perception.

172
00:09:05,680 --> 00:09:07,560
 And then we're going to talk mostly about how

173
00:09:07,560 --> 00:09:09,520
 to use it in planning and control.

174
00:09:09,520 --> 00:09:13,840
 Luckily, our perception systems are actually pretty good.

175
00:09:13,840 --> 00:09:17,360
 There's lots of ways that we already have probabilities

176
00:09:17,360 --> 00:09:19,360
 flowing through a lot of our state-of-the-art perception

177
00:09:19,360 --> 00:09:19,880
 systems.

178
00:09:19,880 --> 00:09:21,880
 So the image recognition was putting out

179
00:09:21,880 --> 00:09:24,560
 probability that it was a sheep, probability that it was a dog,

180
00:09:24,560 --> 00:09:27,480
 probability that it was a cat, so on and so forth.

181
00:09:27,480 --> 00:09:28,520
 That's just one example.

182
00:09:28,520 --> 00:09:31,520
 But we talked about pose estimation,

183
00:09:31,520 --> 00:09:33,720
 where it was outputting entire distributions

184
00:09:33,720 --> 00:09:39,520
 over possible orientations of the mugs.

185
00:09:39,520 --> 00:09:41,440
 We talked about key point estimators

186
00:09:41,440 --> 00:09:45,320
 that weren't putting out x, y, z coordinates of the key points,

187
00:09:45,320 --> 00:09:49,000
 but were actually putting out a heat map

188
00:09:49,000 --> 00:09:55,240
 over possible locations of the key point.

189
00:09:55,240 --> 00:09:56,580
 And over and over and over again,

190
00:09:56,580 --> 00:09:58,960
 you'll see people have-- I mean, certainly neural networks

191
00:09:58,960 --> 00:10:01,200
 are capable of putting out lots of interesting things.

192
00:10:01,200 --> 00:10:02,880
 Oftentimes, those interesting things

193
00:10:02,880 --> 00:10:06,280
 include distributions over possible outcomes.

194
00:10:06,280 --> 00:10:09,560
 And I don't think there's a big barrier anymore

195
00:10:09,560 --> 00:10:12,640
 to asking your perception system to tell you a little bit

196
00:10:12,640 --> 00:10:14,400
 about how confident it is.

197
00:10:14,400 --> 00:10:16,840
 There are different types of uncertainty.

198
00:10:16,840 --> 00:10:19,120
 You might have heard about aleatoric

199
00:10:19,120 --> 00:10:20,680
 versus epistemic uncertainty.

200
00:10:20,680 --> 00:10:22,920
 And there are different ways to ask

201
00:10:22,920 --> 00:10:24,920
 neural networks to address them.

202
00:10:24,920 --> 00:10:25,840
 I won't dig into those.

203
00:10:25,840 --> 00:10:28,480
 But they are great, actually, at the perception level.

204
00:10:28,480 --> 00:10:31,720
 I think there's many good ways to think about uncertainty.

205
00:10:31,720 --> 00:10:34,400
 The challenge, though, becomes-- and the thing we'll focus on

206
00:10:34,400 --> 00:10:37,920
 here-- is how do you consume those estimates of uncertainty

207
00:10:37,920 --> 00:10:40,720
 down through the planning and control stack?

208
00:10:40,720 --> 00:10:42,720
 Because we haven't said anything about that yet.

209
00:10:42,720 --> 00:10:47,700
 So how do you make long-term decisions

210
00:10:47,700 --> 00:10:49,000
 that reason about uncertainty?

211
00:10:49,000 --> 00:10:51,080
 Well, if you're going to make long-term decisions,

212
00:10:51,080 --> 00:10:53,640
 if you're going to make a plan, you need a model.

213
00:10:53,640 --> 00:10:55,340
 So we need to think about a model of how

214
00:10:55,340 --> 00:10:57,280
 our uncertainty is going to change over time

215
00:10:57,280 --> 00:10:58,480
 if we make certain decisions.

216
00:10:58,480 --> 00:11:01,080
 So that I know if I make this decision,

217
00:11:01,080 --> 00:11:03,040
 my uncertainty might evolve in some way.

218
00:11:03,040 --> 00:11:04,440
 If I make a different decision, I

219
00:11:04,440 --> 00:11:06,200
 would like it to evolve in some other way.

220
00:11:06,200 --> 00:11:08,000
 And that way, I'm going to choose my actions

221
00:11:08,000 --> 00:11:11,000
 based on the way the uncertainty is going to evolve.

222
00:11:11,000 --> 00:11:25,480
 So we need a model for the way the uncertainty evolves,

223
00:11:25,480 --> 00:11:26,800
 the dynamics of the uncertainty.

224
00:11:26,800 --> 00:11:33,400
 And there's lots of ways to think about this.

225
00:11:33,400 --> 00:11:36,700
 This is stochastic processes and stochastic dynamics.

226
00:11:36,700 --> 00:11:38,280
 There's lots of things to think about.

227
00:11:38,280 --> 00:11:40,360
 But we've actually already written down

228
00:11:40,360 --> 00:11:41,200
 everything we need.

229
00:11:41,200 --> 00:11:47,520
 When I write this general form of f of x is potentially x,

230
00:11:47,520 --> 00:12:12,880
 n, where I had my randomness coming in here,

231
00:12:12,880 --> 00:12:16,480
 if I've authored my systems in this way,

232
00:12:16,480 --> 00:12:19,720
 where the randomness, the random variables come in here,

233
00:12:19,720 --> 00:12:24,640
 then I actually already have a model of uncertainty

234
00:12:24,640 --> 00:12:27,400
 and how the uncertainty can propagate through the system.

235
00:12:27,400 --> 00:12:30,800
 But so far, we've been just talking about this dynamics

236
00:12:30,800 --> 00:12:31,720
 here.

237
00:12:31,720 --> 00:12:34,480
 And we've been only saying, for simulation, for instance,

238
00:12:34,480 --> 00:12:35,860
 we've just been saying, I'm going

239
00:12:35,860 --> 00:12:37,440
 to take a random draw of this.

240
00:12:37,440 --> 00:12:40,080
 I'm going to evaluate what the next xn is.

241
00:12:40,080 --> 00:12:41,840
 For planning, we've so far pretended

242
00:12:41,840 --> 00:12:44,080
 that that's just 0, for instance.

243
00:12:44,080 --> 00:12:46,520
 And we've done deterministic planning.

244
00:12:46,520 --> 00:12:48,320
 And today, I want to say, well, let's say

245
00:12:48,320 --> 00:12:50,900
 we want to do planning where we admit that this one's not 0.

246
00:12:50,900 --> 00:12:56,040
 And I think it's easiest to think about that first,

247
00:12:56,040 --> 00:12:59,520
 if we look at this just for notational reasons,

248
00:12:59,520 --> 00:13:02,200
 if only, to look at this in the case

249
00:13:02,200 --> 00:13:12,600
 where I have a finite state, x, u, and finite noise, w.

250
00:13:12,600 --> 00:13:15,960
 So that would be the standard tabular Markov decision

251
00:13:15,960 --> 00:13:16,680
 process.

252
00:13:16,680 --> 00:13:20,000
 Or we're going to do a partially observable Markov decision

253
00:13:20,000 --> 00:13:22,000
 process.

254
00:13:22,000 --> 00:13:33,120
 So to start, let's say that state, action,

255
00:13:33,120 --> 00:13:43,960
 and observations are all discrete.

256
00:13:43,960 --> 00:13:49,520
 So there's a finite number of discrete and finite--

257
00:13:49,520 --> 00:13:50,840
 there's a finite number of states

258
00:13:50,840 --> 00:13:52,560
 I could be in, a finite number of actions

259
00:13:52,560 --> 00:13:54,600
 I could be in, a finite number of observations.

260
00:13:54,600 --> 00:13:56,920
 That's not what our manipulation systems look like.

261
00:13:56,920 --> 00:13:59,080
 But that's just how I'll write the-- I'll just

262
00:13:59,080 --> 00:14:02,640
 avoid writing more probability notation than I need to,

263
00:14:02,640 --> 00:14:04,360
 in order to tell that basic story.

264
00:14:04,360 --> 00:14:09,200
 So this, I could have done-- there's

265
00:14:09,200 --> 00:14:11,960
 nothing in those equations that say that x is a continuous

266
00:14:11,960 --> 00:14:13,580
 variable or u is a continuous variable.

267
00:14:13,580 --> 00:14:17,080
 I could write exactly those equations where x is just--

268
00:14:17,080 --> 00:14:20,520
 this is a transition map.

269
00:14:20,520 --> 00:14:21,980
 But when we talk about belief space planning,

270
00:14:21,980 --> 00:14:24,160
 we normally write this in a slightly different way.

271
00:14:24,160 --> 00:14:28,640
 We'll write now the probability over initial conditions,

272
00:14:28,640 --> 00:14:30,640
 let's say, would be a function of x,

273
00:14:30,640 --> 00:14:35,320
 which would be the probability distribution

274
00:14:35,320 --> 00:14:36,320
 over initial conditions.

275
00:14:36,320 --> 00:14:47,120
 And then I'll write the dynamics here,

276
00:14:47,120 --> 00:14:49,000
 which is coming in as a random variable here.

277
00:14:49,000 --> 00:14:53,760
 But I can write that just as a probability of transitioning

278
00:14:53,760 --> 00:14:57,920
 to some new state, given I'm in some current state.

279
00:14:57,920 --> 00:15:01,760
 It depends on p, but it doesn't depend on w here.

280
00:15:01,760 --> 00:15:03,560
 This would be my transition probabilities.

281
00:15:03,560 --> 00:15:10,800
 And similarly, I can write my observations

282
00:15:10,800 --> 00:15:13,680
 as being probabilities that are conditioned

283
00:15:13,680 --> 00:15:17,680
 on my state and my action.

284
00:15:18,680 --> 00:15:27,480
 So is that-- I don't know how comfortable everybody

285
00:15:27,480 --> 00:15:29,600
 is with probability notations, but I just

286
00:15:29,600 --> 00:15:35,280
 want to make it clear that this is sort of a deterministic way

287
00:15:35,280 --> 00:15:38,120
 to write a stochastic equation, where

288
00:15:38,120 --> 00:15:39,840
 I say there's a random variable coming in,

289
00:15:39,840 --> 00:15:42,920
 but this is a deterministic function.

290
00:15:42,920 --> 00:15:45,680
 I could equivalently write that as saying,

291
00:15:45,680 --> 00:15:49,240
 I want to know, given x, u, and p, for instance,

292
00:15:49,240 --> 00:15:51,360
 what is the distribution over y?

293
00:15:51,360 --> 00:15:54,520
 And that sort of removes this from the argument,

294
00:15:54,520 --> 00:15:58,000
 and I have a distribution over possible.

295
00:15:58,000 --> 00:15:59,840
 Is that clear how I could write the same thing

296
00:15:59,840 --> 00:16:02,000
 in two different ways, with two different notations?

297
00:16:02,000 --> 00:16:09,880
 For instance, if this was a Gaussian,

298
00:16:09,880 --> 00:16:11,680
 and I'm pushing this through, and maybe this

299
00:16:11,680 --> 00:16:14,560
 is a linear equation with a Gaussian here,

300
00:16:14,560 --> 00:16:20,440
 then I could write the output as a Gaussian distribution.

301
00:16:20,440 --> 00:16:22,640
 That would be not a function of w.

302
00:16:22,640 --> 00:16:24,480
 Or I could say, make a specific draw,

303
00:16:24,480 --> 00:16:26,360
 and tell me the specific value that comes out

304
00:16:26,360 --> 00:16:27,120
 on the other side.

305
00:16:27,120 --> 00:16:33,080
 And the reason it's so nice to do the fully discrete case

306
00:16:33,080 --> 00:16:35,440
 is that I can represent each of these

307
00:16:35,440 --> 00:16:38,120
 just with a finite set of numbers then.

308
00:16:38,120 --> 00:16:41,880
 So how would I represent a probability over possible x's?

309
00:16:41,880 --> 00:16:55,440
 Well, maybe if x is drawn from just some number of possible x's,

310
00:16:55,440 --> 00:16:59,480
 or maybe I realize I've got a notational overlap there,

311
00:16:59,480 --> 00:17:04,600
 but I've got some notational-- some finite set of x's,

312
00:17:04,600 --> 00:17:10,600
 then my probability over x-- I'll write of x0.

313
00:17:10,600 --> 00:17:12,320
 That'll clear up my notational overlap.

314
00:17:12,320 --> 00:17:17,520
 My probability of x0, I could just

315
00:17:17,520 --> 00:17:23,480
 write it as a vector, which is the probability that x0 is x

316
00:17:23,480 --> 00:17:24,920
 like this.

317
00:17:24,920 --> 00:17:27,920
 Probability of x0, x1.

318
00:17:27,920 --> 00:17:38,520
 It's just a vector.

319
00:17:38,520 --> 00:17:42,280
 And similarly, the transition probabilities,

320
00:17:42,280 --> 00:17:44,040
 they're going to be-- it's almost a matrix,

321
00:17:44,040 --> 00:17:45,200
 but since I've got two variables,

322
00:17:45,200 --> 00:17:47,440
 it's actually a little easier to think about it as a tensor,

323
00:17:47,440 --> 00:17:48,360
 actually, now.

324
00:17:48,360 --> 00:17:50,120
 I wouldn't have done that a few years ago,

325
00:17:50,120 --> 00:17:52,280
 but now I can just say tensor, and it's good.

326
00:17:52,280 --> 00:17:53,520
 Think about that as a tensor.

327
00:17:53,520 --> 00:17:59,920
 For every u, I have a matrix that maps from my current x

328
00:17:59,920 --> 00:18:00,920
 to my next x prime.

329
00:18:00,920 --> 00:18:03,200
 And all the machinery goes through very nicely when you

330
00:18:03,200 --> 00:18:04,760
 just have tables of numbers.

331
00:18:04,760 --> 00:18:07,040
 And the question of how do you represent a probability

332
00:18:07,040 --> 00:18:09,760
 distribution is just not there when you

333
00:18:09,760 --> 00:18:11,080
 have a finite list of numbers.

334
00:18:11,080 --> 00:18:21,360
 OK, so what's going to happen now is we're going to have--

335
00:18:21,360 --> 00:18:25,440
 we're going to watch how this system can evolve

336
00:18:25,440 --> 00:18:27,000
 under the probability distributions.

337
00:18:27,000 --> 00:18:34,000
 What is the sort of state evolution of my probabilities?

338
00:18:34,000 --> 00:18:35,680
 Sorry, I moved twice.

339
00:18:35,960 --> 00:18:38,000
 OK.

340
00:18:38,000 --> 00:18:40,600
 The state of the system, I would say,

341
00:18:40,600 --> 00:18:49,080
 is clearly-- the state of the plant is clearly x.

342
00:18:49,080 --> 00:18:52,400
 That's what we've always been calling the state.

343
00:18:52,400 --> 00:19:00,840
 But from the perspective of the observer,

344
00:19:00,840 --> 00:19:04,520
 someone who's trying to track what's happening going on

345
00:19:04,520 --> 00:19:15,800
 with x, we need a little bit more than just x

346
00:19:15,800 --> 00:19:17,200
 to summarize what's going on.

347
00:19:17,200 --> 00:19:22,400
 So it actually gets very deeply into the things

348
00:19:22,400 --> 00:19:23,400
 we've been talking about.

349
00:19:23,400 --> 00:19:25,360
 We've talked about how to learn different state

350
00:19:25,360 --> 00:19:28,800
 representations, what makes a good state.

351
00:19:28,800 --> 00:19:32,040
 Do people know what's the definition of a state?

352
00:19:32,040 --> 00:19:34,800
 What is the fundamental property that sort of defines a state?

353
00:19:34,800 --> 00:19:38,820
 You.

354
00:19:38,820 --> 00:19:43,320
 [INAUDIBLE]

355
00:19:43,320 --> 00:19:43,820
 OK.

356
00:19:43,820 --> 00:19:47,840
 So he says minimal and sufficient information

357
00:19:47,840 --> 00:19:50,120
 to predict the next state.

358
00:19:50,120 --> 00:19:52,240
 Yes, I mean almost.

359
00:19:52,240 --> 00:19:55,160
 So we could argue about whether states

360
00:19:55,160 --> 00:19:56,320
 need to be minimal or not.

361
00:19:56,320 --> 00:19:58,640
 I think you could talk about a minimal representation

362
00:19:58,640 --> 00:19:59,640
 for state.

363
00:19:59,640 --> 00:20:03,840
 I'll forego minimalism for now.

364
00:20:03,840 --> 00:20:04,360
 OK.

365
00:20:04,360 --> 00:20:07,080
 The question is, so a state is something

366
00:20:07,080 --> 00:20:09,520
 that lets you fully predict the next state.

367
00:20:09,520 --> 00:20:12,520
 That's a super good definition.

368
00:20:12,520 --> 00:20:15,480
 But it's not actually the definition we want here.

369
00:20:15,480 --> 00:20:17,840
 Because the system is stochastic,

370
00:20:17,840 --> 00:20:20,480
 we can't perfectly predict the next state.

371
00:20:20,480 --> 00:20:22,640
 So we need a slightly richer notion of what's state.

372
00:20:22,640 --> 00:20:28,360
 A slightly richer definition, but completely

373
00:20:28,360 --> 00:20:37,760
 consistent, is that a state is a set of information,

374
00:20:37,760 --> 00:20:39,560
 a set of numbers, for instance, that

375
00:20:39,560 --> 00:20:42,400
 lets you forget all of the other things you've seen.

376
00:20:42,400 --> 00:20:45,600
 It's a sufficient statistic for all

377
00:20:45,600 --> 00:20:47,080
 of the history of your observations.

378
00:20:48,080 --> 00:20:48,580
 OK.

379
00:20:48,580 --> 00:21:04,920
 So if I wanted to write the evolution of this system,

380
00:21:04,920 --> 00:21:07,480
 if I wanted to predict, for instance,

381
00:21:07,480 --> 00:21:13,480
 what is the probability of y at the n-th step being,

382
00:21:13,480 --> 00:21:17,200
 I don't know, the i-th y, conditioned on all

383
00:21:17,200 --> 00:21:23,400
 of the things I've seen so far, u0, u1, u2, up to, let's say,

384
00:21:23,400 --> 00:21:36,520
 un minus 1, but also y0, y1, up to yn minus 1,

385
00:21:36,520 --> 00:21:38,800
 potentially the next y I expect to see,

386
00:21:38,800 --> 00:21:40,760
 or the distribution over y's I expect to see,

387
00:21:40,760 --> 00:21:44,680
 is a function of all of the things I've seen in the past.

388
00:21:44,680 --> 00:21:47,240
 OK.

389
00:21:47,240 --> 00:21:49,480
 What we want is to summarize all of the things

390
00:21:49,480 --> 00:21:52,120
 I've seen in the past, so that the prediction based

391
00:21:52,120 --> 00:21:54,960
 on a state that represents this is the same as if I

392
00:21:54,960 --> 00:21:57,400
 had all of the histories.

393
00:21:57,400 --> 00:22:02,800
 So I want to say the probability over yn equals yi,

394
00:22:02,800 --> 00:22:09,080
 conditioned on some b-- I'm going to call it b here-- bn,

395
00:22:09,080 --> 00:22:15,360
 and maybe un, since that's coming in right now,

396
00:22:15,360 --> 00:22:25,240
 is equivalent to having all of that prior information.

397
00:22:25,240 --> 00:22:31,540
 OK.

398
00:22:31,540 --> 00:22:36,360
 So this state here, what does it mean to be a good state?

399
00:22:36,360 --> 00:22:41,280
 It means it's a sufficient statistic for the history,

400
00:22:41,280 --> 00:22:44,600
 or a sufficient summary, let's say,

401
00:22:44,600 --> 00:22:46,280
 of my entire history of observations.

402
00:22:46,280 --> 00:22:59,280
 OK, so for the purposes of the observer,

403
00:22:59,280 --> 00:23:02,640
 the state that you want to track, this state,

404
00:23:02,640 --> 00:23:05,000
 this thing called b, is called a belief state.

405
00:23:06,000 --> 00:23:13,400
 And we want to make sure we get our head around that,

406
00:23:13,400 --> 00:23:16,960
 and then use it for planning.

407
00:23:16,960 --> 00:23:21,320
 OK, so a belief state is some efficient, hopefully,

408
00:23:21,320 --> 00:23:25,360
 not always, but some, let's say, numerical summary

409
00:23:25,360 --> 00:23:29,160
 of all the things that I've seen in the past that

410
00:23:29,160 --> 00:23:30,740
 is sufficient for me to predict what's

411
00:23:30,740 --> 00:23:32,000
 going to happen in the future.

412
00:23:32,000 --> 00:23:32,500
 OK?

413
00:23:32,500 --> 00:23:38,560
 And for these sort of Markov processes

414
00:23:38,560 --> 00:23:41,880
 and dynamical systems of this form,

415
00:23:41,880 --> 00:23:44,600
 there's a natural choice for the belief state.

416
00:23:44,600 --> 00:23:49,920
 It's not a unique choice.

417
00:23:49,920 --> 00:23:53,200
 It's certainly not a minimal choice in most cases.

418
00:23:53,200 --> 00:23:54,480
 OK?

419
00:23:54,480 --> 00:24:01,760
 A minimal choice would be to say that the belief--

420
00:24:01,760 --> 00:24:04,280
 let me say the-- I'll use it as a vector again,

421
00:24:04,280 --> 00:24:06,280
 since everything's nice in the continuous thing.

422
00:24:06,280 --> 00:24:08,240
 So I've got a belief vector.

423
00:24:08,240 --> 00:24:12,600
 And the belief for the element i of that vector

424
00:24:12,600 --> 00:24:17,680
 is going to be the probability that x at n

425
00:24:17,680 --> 00:24:27,680
 equals xi conditioned on u0, u1, y0, y1, and so on and so forth.

426
00:24:30,560 --> 00:24:32,960
 So the belief, a sufficient statistic

427
00:24:32,960 --> 00:24:36,080
 that allows me to forget everything I've seen,

428
00:24:36,080 --> 00:24:38,840
 is a probability distribution over all the possible states

429
00:24:38,840 --> 00:24:39,480
 I might be in.

430
00:24:39,480 --> 00:24:44,400
 OK?

431
00:24:44,400 --> 00:24:46,680
 That's a super powerful thing.

432
00:24:46,680 --> 00:24:49,440
 It says that all I need to keep-- no matter how long my history

433
00:24:49,440 --> 00:24:51,960
 was, how many observations I have,

434
00:24:51,960 --> 00:24:56,080
 if I just summarize my current estimated probability of what's

435
00:24:56,080 --> 00:24:59,920
 in the-- of being in state 0, in state 1, state 2,

436
00:24:59,920 --> 00:25:02,240
 it's just a vector, one vector, right?

437
00:25:02,240 --> 00:25:04,800
 If I can just keep track of that, then I have everything.

438
00:25:04,800 --> 00:25:08,320
 I don't need to remember anything else about the past.

439
00:25:08,320 --> 00:25:10,560
 And more because of that, because it's

440
00:25:10,560 --> 00:25:14,200
 sufficient to summarize the past and to predict the future,

441
00:25:14,200 --> 00:25:18,080
 it's also sufficient for optimal decision making.

442
00:25:18,080 --> 00:25:22,600
 So it turns out we know that the optimal controllers,

443
00:25:22,600 --> 00:25:31,200
 optimal policies, must be of the form un is some pi star--

444
00:25:31,200 --> 00:25:34,960
 it could be a function of n, potentially-- of bn.

445
00:25:34,960 --> 00:25:43,160
 Even when the system is partially observable, right?

446
00:25:43,160 --> 00:25:50,760
 So in the case where y just shows me x without noise,

447
00:25:50,760 --> 00:25:57,000
 for instance, then x actually-- b can just be x.

448
00:25:57,000 --> 00:25:58,660
 And all the things we already know still

449
00:25:58,660 --> 00:26:00,960
 work if they still fit in this framework.

450
00:26:00,960 --> 00:26:02,640
 Because my probability distribution

451
00:26:02,640 --> 00:26:05,920
 collapses to just a single point.

452
00:26:05,920 --> 00:26:08,440
 But in general, I have to keep an entire state distribution

453
00:26:08,440 --> 00:26:08,960
 over x.

454
00:26:08,960 --> 00:26:09,460
 Yeah?

455
00:26:09,460 --> 00:26:11,600
 AUDIENCE: Sir, would you mind explaining a bit more

456
00:26:11,600 --> 00:26:16,200
 why the probability distribution of x at the point

457
00:26:16,200 --> 00:26:17,840
 captures the whole [INAUDIBLE]

458
00:26:17,840 --> 00:26:19,040
 PROFESSOR: Yeah.

459
00:26:19,040 --> 00:26:20,880
 Yeah.

460
00:26:20,880 --> 00:26:24,680
 I wish I had a one-liner for that.

461
00:26:24,680 --> 00:26:27,920
 But it is true.

462
00:26:27,920 --> 00:26:32,360
 So you can derive it recursively from these equations.

463
00:26:32,360 --> 00:26:34,240
 If I wrote out all the equations,

464
00:26:34,240 --> 00:26:36,040
 you can see it recursively in the algebra

465
00:26:36,040 --> 00:26:37,920
 and everything like that.

466
00:26:37,920 --> 00:26:41,080
 It's also the tenant of filtering.

467
00:26:41,080 --> 00:26:46,000
 So the Bayes optimal filter, for instance,

468
00:26:46,000 --> 00:26:48,840
 takes exactly this form.

469
00:26:48,840 --> 00:26:52,880
 But without all that machinery, I

470
00:26:52,880 --> 00:26:56,840
 have to ask you for a little bit of a leap of faith maybe.

471
00:26:56,840 --> 00:26:57,840
 Yeah.

472
00:26:57,840 --> 00:26:59,000
 But thank you for asking.

473
00:26:59,000 --> 00:27:02,800
 OK.

474
00:27:02,800 --> 00:27:06,880
 So now our new commodity is to traffic in these beliefs.

475
00:27:06,880 --> 00:27:11,160
 Now the problem, caveat, spoiler alert,

476
00:27:11,160 --> 00:27:14,240
 sometimes it's hard to keep track of all the possible--

477
00:27:14,240 --> 00:27:16,480
 to write a distribution over big, complicated things.

478
00:27:16,480 --> 00:27:19,960
 This might be the shape of the mustard bottle.

479
00:27:19,960 --> 00:27:21,760
 It could be the time of the day.

480
00:27:21,760 --> 00:27:23,420
 It could be that there's a lot of things

481
00:27:23,420 --> 00:27:25,120
 to potentially keep track of in the world.

482
00:27:25,120 --> 00:27:28,360
 And it becomes untenable to try to keep

483
00:27:28,360 --> 00:27:30,400
 track of all the distribution over everything

484
00:27:30,400 --> 00:27:32,240
 that could possibly happen.

485
00:27:32,240 --> 00:27:36,160
 But in the smaller problems, and with selected, targeted

486
00:27:36,160 --> 00:27:39,720
 reasoning about uncertainty, you can do very well with this.

487
00:27:43,640 --> 00:27:44,240
 OK.

488
00:27:44,240 --> 00:27:46,840
 So this is amazing.

489
00:27:46,840 --> 00:27:48,840
 I mean, I'll give you a few examples here.

490
00:27:48,840 --> 00:27:58,080
 So there's a classic example that people

491
00:27:58,080 --> 00:28:01,240
 talk about in the partially observable Markov decision

492
00:28:01,240 --> 00:28:06,240
 process of discrete worlds that have discrete observations.

493
00:28:06,240 --> 00:28:09,040
 One of them is a cheese maze.

494
00:28:09,040 --> 00:28:12,200
 It's a silly thing, but there's, I don't know, cheese here.

495
00:28:12,200 --> 00:28:14,240
 OK, and the mouse has to go and find the cheese.

496
00:28:14,240 --> 00:28:16,600
 OK.

497
00:28:16,600 --> 00:28:18,220
 And there's a discrete number of places

498
00:28:18,220 --> 00:28:21,000
 that the mouse might be, for instance.

499
00:28:21,000 --> 00:28:24,040
 And there's observation.

500
00:28:24,040 --> 00:28:28,140
 So when you're in a certain place on the board,

501
00:28:28,140 --> 00:28:30,000
 if the mouse-- I'm not going to draw a mouse.

502
00:28:30,000 --> 00:28:31,400
 Well, maybe I could draw a mouse.

503
00:28:31,400 --> 00:28:35,040
 But yeah, something with the tail and the ears.

504
00:28:35,040 --> 00:28:35,920
 OK.

505
00:28:35,920 --> 00:28:37,760
 There's a mouse running around the maze.

506
00:28:37,760 --> 00:28:39,720
 Luckily, the mouse can see the numbers

507
00:28:39,720 --> 00:28:42,180
 that we put down, which are like signposts, which

508
00:28:42,180 --> 00:28:44,200
 tell it where it is.

509
00:28:44,200 --> 00:28:45,880
 And the interesting cheese mazes are

510
00:28:45,880 --> 00:28:49,640
 the ones that have observations that don't tell you

511
00:28:49,640 --> 00:28:50,560
 exactly where you are.

512
00:28:50,560 --> 00:28:51,920
 They give you an indication.

513
00:28:51,920 --> 00:28:53,840
 They give you information about where you are,

514
00:28:53,840 --> 00:28:56,120
 but they don't instantly determine where you are,

515
00:28:56,120 --> 00:28:59,800
 because maybe there's the number 2 appears in multiple places.

516
00:28:59,800 --> 00:29:01,200
 OK.

517
00:29:01,200 --> 00:29:03,400
 Something like this is a classic one.

518
00:29:03,400 --> 00:29:06,840
 Maybe there's 5 in all these places.

519
00:29:06,840 --> 00:29:09,440
 6, 7, 6.

520
00:29:09,440 --> 00:29:11,560
 OK.

521
00:29:11,560 --> 00:29:15,440
 So what is the evolution of this belief going to be?

522
00:29:15,440 --> 00:29:20,320
 So if the mouse wakes up and is following Bayes optimal--

523
00:29:20,320 --> 00:29:23,000
 it's a Bayes optimal mouse, then it's

524
00:29:23,000 --> 00:29:27,200
 keeping track of a finite list of probabilities.

525
00:29:27,200 --> 00:29:31,640
 B at 0, maybe it thinks-- I think there's 11 things here.

526
00:29:31,640 --> 00:29:36,240
 So maybe there's equal probability everywhere

527
00:29:36,240 --> 00:29:40,160
 that it could be anywhere in the board.

528
00:29:40,160 --> 00:29:43,000
 And then it sees-- maybe I should have started down

529
00:29:43,000 --> 00:29:44,200
 at a more interesting place.

530
00:29:44,200 --> 00:29:47,200
 But maybe it sees 2 at the first time step.

531
00:29:47,200 --> 00:29:52,080
 And after one observation, it's collapsed its belief

532
00:29:52,080 --> 00:29:55,920
 to being 0 in most places.

533
00:29:55,920 --> 00:29:57,880
 But there's still half of probability

534
00:29:57,880 --> 00:30:02,120
 that I'm in this place and wherever the next one is.

535
00:30:02,120 --> 00:30:05,000
 And the rest are 0's, right?

536
00:30:05,000 --> 00:30:07,080
 And as the mouse moves through the board,

537
00:30:07,080 --> 00:30:09,240
 it's updating its probability distribution

538
00:30:09,240 --> 00:30:12,600
 over possible states.

539
00:30:12,600 --> 00:30:15,720
 And the recipe for updating that falls directly out

540
00:30:15,720 --> 00:30:21,680
 of Bayes' rule applied to those forward dynamics.

541
00:30:21,680 --> 00:30:26,320
 For a more complicated and more robotics version of that,

542
00:30:26,320 --> 00:30:29,800
 people might know a lot about state estimation and Monte

543
00:30:29,800 --> 00:30:31,480
 Carlo filtering and the like.

544
00:30:31,480 --> 00:30:35,680
 This is one of the first ones, the kind of popularized

545
00:30:35,680 --> 00:30:37,480
 probabilistic robotics.

546
00:30:37,480 --> 00:30:43,120
 So this is a robot, a trash can robot moving around

547
00:30:43,120 --> 00:30:46,080
 with a sonar, because that's what we had back in the day.

548
00:30:46,080 --> 00:30:49,080
 And you could think about this as being a much more

549
00:30:49,080 --> 00:30:51,560
 complicated cheese maze.

550
00:30:51,560 --> 00:30:54,520
 And the observations are now the depth returns of the sonar.

551
00:30:54,520 --> 00:30:56,320
 And if I start it over again, it starts off

552
00:30:56,320 --> 00:30:57,920
 with probability all over.

553
00:30:57,920 --> 00:31:00,640
 This is sampled versions of that probability.

554
00:31:00,640 --> 00:31:03,800
 And as it gets sonar returns, it gets information

555
00:31:03,800 --> 00:31:05,600
 about where it is, but it doesn't completely

556
00:31:05,600 --> 00:31:07,240
 determine where it is.

557
00:31:07,240 --> 00:31:09,120
 And it has a probability mass, which

558
00:31:09,120 --> 00:31:11,920
 is like that vector, all over the space.

559
00:31:11,920 --> 00:31:14,960
 And as it evolves, it can do pretty complicated things.

560
00:31:14,960 --> 00:31:18,200
 That's just a more sophisticated version

561
00:31:18,200 --> 00:31:19,520
 of this super simple example.

562
00:31:19,520 --> 00:31:26,680
 Now, what's essential?

563
00:31:26,680 --> 00:31:31,000
 Now, this is the point of this part of the lecture.

564
00:31:31,000 --> 00:31:32,880
 What's essential is that the rules that

565
00:31:32,880 --> 00:31:40,440
 govern the update of the belief distribution

566
00:31:40,440 --> 00:31:42,920
 just have dynamics that we can write down.

567
00:31:42,920 --> 00:31:44,040
 It's just another system.

568
00:31:44,040 --> 00:31:54,920
 You can write down the evolution through Bayes optimal filter.

569
00:31:54,920 --> 00:31:57,320
 B is a function of n plus 1.

570
00:31:57,320 --> 00:32:05,160
 It's just some function of B of n, U of n, and Y of n.

571
00:32:05,160 --> 00:32:09,080
 It's a system that looks like this.

572
00:32:09,080 --> 00:32:12,160
 U going in, observations coming in.

573
00:32:12,160 --> 00:32:15,440
 It's got an internal state B inside it.

574
00:32:15,440 --> 00:32:17,960
 Maybe you can put the B on the output board if you want.

575
00:32:17,960 --> 00:32:20,920
 That isn't actually essential here.

576
00:32:20,920 --> 00:32:24,640
 And if you have goals that are specified, for instance,

577
00:32:24,640 --> 00:32:26,680
 that I want to get to a certain belief,

578
00:32:26,680 --> 00:32:28,280
 I want to, with high probability,

579
00:32:28,280 --> 00:32:30,160
 be where the cheese is, or I want

580
00:32:30,160 --> 00:32:33,800
 to be in some room in the map with high probability,

581
00:32:33,800 --> 00:32:37,000
 then the task is just like what we've done before,

582
00:32:37,000 --> 00:32:42,600
 where it's a task of choosing the U subject to the dynamics

583
00:32:42,600 --> 00:32:48,520
 F that moves around the target B.

584
00:32:48,520 --> 00:32:52,200
 As a result, all of the tools that we've already

585
00:32:52,200 --> 00:32:55,200
 talked about-- well, with some caveats--

586
00:32:55,200 --> 00:32:58,360
 but the basic tools we've talked about can work.

587
00:32:58,360 --> 00:33:00,760
 So once you have this problem, for instance,

588
00:33:00,760 --> 00:33:02,240
 you can do trajectory optimization.

589
00:33:02,240 --> 00:33:07,200
 And I would say the dominant approaches, maybe,

590
00:33:07,200 --> 00:33:09,520
 for large scale things would be a trajectory optimization

591
00:33:09,520 --> 00:33:14,600
 kind of approach, or a sampling-based motion planning

592
00:33:14,600 --> 00:33:15,240
 type approach.

593
00:33:15,240 --> 00:33:28,160
, OK?

594
00:33:28,160 --> 00:33:32,200
 And so we've talked mostly in this class

595
00:33:32,200 --> 00:33:34,440
 about kinematic trajectory optimization.

596
00:33:34,440 --> 00:33:36,480
 This is really a dynamic trajectory optimization.

597
00:33:36,480 --> 00:33:43,280
 That's the biggest caveat I have for you,

598
00:33:43,280 --> 00:33:46,840
 is that you do have to think about the fact

599
00:33:46,840 --> 00:33:49,560
 that you can't take arbitrary paths through B.

600
00:33:49,560 --> 00:33:53,040
 The dynamics of this function F do limit you.

601
00:33:53,040 --> 00:33:56,160
 So it's an under-actuated system.

602
00:33:56,160 --> 00:33:57,280
 Pretty good, right?

603
00:33:57,280 --> 00:34:00,260
 But it is actually interesting and hard,

604
00:34:00,260 --> 00:34:01,840
 because it's an under-actuated system.

605
00:34:01,840 --> 00:34:07,120
 You don't have enough actuators to control your entire belief.

606
00:34:07,120 --> 00:34:09,340
 And so actually, the trajectory optimization versions

607
00:34:09,340 --> 00:34:11,240
 we do in under-actuated are more suitable

608
00:34:11,240 --> 00:34:12,480
 than the kinematic trajectory.

609
00:34:12,480 --> 00:34:15,360
 But it's a small extension from the types

610
00:34:15,360 --> 00:34:17,520
 of things we've done.

611
00:34:17,520 --> 00:34:20,600
 So you can do trajectory optimization over U,

612
00:34:20,600 --> 00:34:25,600
 subject to constraints that B has some initial condition,

613
00:34:25,600 --> 00:34:26,320
 final condition.

614
00:34:26,320 --> 00:34:30,240
 You could put a cost on B, so on and so forth.

615
00:34:30,240 --> 00:34:31,880
 Now, almost, right?

616
00:34:31,880 --> 00:34:34,960
 So there's one important difference here.

617
00:34:34,960 --> 00:34:38,920
 Why, as I've written it here, is this system's perspective,

618
00:34:38,920 --> 00:34:42,080
 why is still a random variable coming in?

619
00:34:42,080 --> 00:34:49,920
 It's a function of X and U, but also it

620
00:34:49,920 --> 00:34:53,040
 could be a noisy measurement.

621
00:34:53,040 --> 00:34:56,440
 So you actually have to do a form of stochastic trajectory

622
00:34:56,440 --> 00:34:58,720
 optimization, or you can make a choice

623
00:34:58,720 --> 00:35:05,160
 to be optimistic about your observations, y.

624
00:35:05,160 --> 00:35:07,320
 But people have studied nicely how you can do this.

625
00:35:07,320 --> 00:35:09,320
 You could do stochastic trajectory optimization.

626
00:35:09,320 --> 00:35:11,080
 If you've heard of iterative LQG,

627
00:35:11,080 --> 00:35:13,360
 that actually would be-- if I were to recommend one thing

628
00:35:13,360 --> 00:35:16,680
 to solve these problems, I would recommend iterative LQG.

629
00:35:16,680 --> 00:35:20,080
 We had some work that tried to be optimistic about y

630
00:35:20,080 --> 00:35:22,440
 and used deterministic trajectory optimization

631
00:35:22,440 --> 00:35:23,840
 to do it.

632
00:35:23,840 --> 00:35:26,560
 But actually, the flavor of this is very much just trajectory

633
00:35:26,560 --> 00:35:27,400
 optimization.

634
00:35:27,400 --> 00:35:28,280
 Gets you pretty far.

635
00:35:28,280 --> 00:35:38,320
 So let me tell you that version of it.

636
00:35:38,320 --> 00:35:42,760
 So this is a toy version of the problem.

637
00:35:42,760 --> 00:35:46,660
 Imagine you're a point robot.

638
00:35:46,660 --> 00:35:48,840
 Starting here, that's the initial conditions.

639
00:35:48,840 --> 00:35:52,600
 We call this the light-dark domain.

640
00:35:52,600 --> 00:35:53,960
 It's the simplest kind of instance

641
00:35:53,960 --> 00:35:57,720
 of a problem, which has state-dependent observation

642
00:35:57,720 --> 00:35:59,640
 noise.

643
00:35:59,640 --> 00:36:01,960
 So the basic thing is that it's dark over here.

644
00:36:01,960 --> 00:36:06,040
 Your position sensors are noisy when it's dark.

645
00:36:06,040 --> 00:36:08,680
 And over here, it's light.

646
00:36:08,680 --> 00:36:11,160
 Sensors are pretty accurate when it's light.

647
00:36:11,160 --> 00:36:14,040
 Your goal is to get to 0, 0, 0.

648
00:36:14,040 --> 00:36:17,560
 If you didn't reason at all about uncertainty,

649
00:36:17,560 --> 00:36:21,400
 and you felt like you were in this initial condition, that

650
00:36:21,400 --> 00:36:23,760
 was the mean of your initial conditions,

651
00:36:23,760 --> 00:36:26,600
 then you'd take a straight line here.

652
00:36:26,600 --> 00:36:30,880
 But if you have process noise, too, for instance,

653
00:36:30,880 --> 00:36:34,240
 you might end up actually very far from there.

654
00:36:34,240 --> 00:36:37,320
 If you write an optimization to say, I'd like to get here,

655
00:36:37,320 --> 00:36:39,480
 but I'd like to get here with some confidence--

656
00:36:39,480 --> 00:36:42,440
 I'd like my belief to be narrowly distributed

657
00:36:42,440 --> 00:36:44,960
 around that goal-- then it actually

658
00:36:44,960 --> 00:36:48,800
 makes sense to go into the light in order

659
00:36:48,800 --> 00:36:50,720
 to come back to the dark.

660
00:36:50,720 --> 00:36:53,680
 So this is explicitly an information-gathering action

661
00:36:53,680 --> 00:36:56,600
 that you don't get from deterministic reasoning,

662
00:36:56,600 --> 00:36:59,440
 but you do get from reasoning over belief state.

663
00:36:59,440 --> 00:37:01,240
 The reason there's two curves is that we're

664
00:37:01,240 --> 00:37:02,600
 talking about two different ones.

665
00:37:02,600 --> 00:37:06,640
 This is one just based on linearizing the whole equations

666
00:37:06,640 --> 00:37:09,840
 and doing basically one step of the iterative LQG

667
00:37:09,840 --> 00:37:10,920
 kind of algorithm.

668
00:37:10,920 --> 00:37:15,400
 And this one's based on a direct trajectory optimization,

669
00:37:15,400 --> 00:37:18,440
 dynamic trajectory optimization.

670
00:37:18,440 --> 00:37:21,000
 But the principle is the same, is

671
00:37:21,000 --> 00:37:24,120
 that only because you're learning about uncertainty

672
00:37:24,120 --> 00:37:26,560
 did you choose to go into the light to come back.

673
00:37:26,560 --> 00:37:32,280
 The specific objective here-- oh, yeah,

674
00:37:32,280 --> 00:37:34,040
 please.

675
00:37:34,040 --> 00:37:37,080
 So in this method, it's like you need

676
00:37:37,080 --> 00:37:41,600
 to know how the uncertainty will evolve

677
00:37:41,600 --> 00:37:45,160
 in all possible different future--

678
00:37:45,160 --> 00:37:45,720
 That's true.

679
00:37:45,720 --> 00:37:47,160
 --in order to figure it out.

680
00:37:47,160 --> 00:37:50,160
 But then in real life, I don't know--

681
00:37:50,160 --> 00:37:53,880
 at this time, I don't know how my uncertainty would evolve

682
00:37:53,880 --> 00:37:55,000
 10 steps down the line.

683
00:37:55,000 --> 00:37:59,000
 So how can I find an optimal trajectory right now

684
00:37:59,000 --> 00:38:01,400
 all the way to the end if I don't know how it will evolve?

685
00:38:01,400 --> 00:38:03,800
 You don't know-- so this is a very deep question.

686
00:38:03,800 --> 00:38:04,320
 Thank you.

687
00:38:04,320 --> 00:38:10,000
 So the question is, now I can't know how my distribution is

688
00:38:10,000 --> 00:38:11,240
 going to evolve.

689
00:38:11,240 --> 00:38:17,880
 So you can know how the distribution over distribution

690
00:38:17,880 --> 00:38:19,080
 is going to evolve.

691
00:38:19,080 --> 00:38:22,200
 So you don't know what sensor measurement

692
00:38:22,200 --> 00:38:26,320
 you're going to get at time 3 in the future.

693
00:38:26,320 --> 00:38:28,960
 That you have to either think about all possible,

694
00:38:28,960 --> 00:38:34,400
 the random variable of possible measurements I get at time 3.

695
00:38:34,400 --> 00:38:37,400
 Or you can say, I'm going to propagate

696
00:38:37,400 --> 00:38:39,680
 where I think I'll be at time 3 and then

697
00:38:39,680 --> 00:38:42,280
 assume that I'm going to get a particular measurement at time

698
00:38:42,280 --> 00:38:45,160
 3 in order to keep going.

699
00:38:45,160 --> 00:38:51,320
 But actually, you do-- this is a complete-- if we

700
00:38:51,320 --> 00:38:53,960
 agree that we have a dynamic model of how things go

701
00:38:53,960 --> 00:38:56,840
 and what my measurement noise is, for instance,

702
00:38:56,840 --> 00:38:58,760
 I do have-- understand things like if I

703
00:38:58,760 --> 00:39:00,140
 were to look around here, I would

704
00:39:00,140 --> 00:39:02,160
 have a different view of what's behind here.

705
00:39:02,160 --> 00:39:04,320
 And I would expect to get-- I don't know what's behind here,

706
00:39:04,320 --> 00:39:05,960
 but I know that I get more information

707
00:39:05,960 --> 00:39:08,520
 to reduce my uncertainty about what's behind this paper

708
00:39:08,520 --> 00:39:09,800
 if I were to move here.

709
00:39:09,800 --> 00:39:13,160
 And that turns out to be very powerful,

710
00:39:13,160 --> 00:39:15,440
 enough that it causes you to take information gathering

711
00:39:15,440 --> 00:39:16,480
 actions.

712
00:39:16,480 --> 00:39:18,320
 And then, because you might be surprised,

713
00:39:18,320 --> 00:39:20,400
 and what you find there might very much determine

714
00:39:20,400 --> 00:39:23,320
 what you do, we often use this in a replanning cycle.

715
00:39:23,320 --> 00:39:25,400
 So you plan, but if you ever see something

716
00:39:25,400 --> 00:39:27,960
 that then dramatically changed your view of the world,

717
00:39:27,960 --> 00:39:29,560
 you just replan.

718
00:39:29,560 --> 00:39:33,200
 But that's a great question.

719
00:39:33,200 --> 00:39:36,800
 The particular objective here, just to think about it--

720
00:39:36,800 --> 00:39:40,520
 so instead of representing this as a table of possible

721
00:39:40,520 --> 00:39:42,680
 locations here, the representation here

722
00:39:42,680 --> 00:39:46,280
 was a mean and covariance over possible locations.

723
00:39:46,280 --> 00:39:48,000
 And the goal was to say, I'd like

724
00:39:48,000 --> 00:39:52,600
 to be here where my mean was at the goal,

725
00:39:52,600 --> 00:39:54,900
 but my covariance was as small as possible.

726
00:39:54,900 --> 00:39:56,980
 Find a trajectory, and there was some cost on--

727
00:39:56,980 --> 00:39:59,840
 I should have put in the cost on action, too.

728
00:39:59,840 --> 00:40:02,520
 But it would go across here, and then come back

729
00:40:02,520 --> 00:40:03,600
 with as small as possible.

730
00:40:03,600 --> 00:40:05,140
 And it was better to go into the light

731
00:40:05,140 --> 00:40:08,080
 than to take the straight path.

732
00:40:08,080 --> 00:40:10,000
 OK, so that's still a little abstract.

733
00:40:10,000 --> 00:40:14,160
 Here's a robotics version of it, a manipulation version of it.

734
00:40:14,160 --> 00:40:16,680
 So let's say that you know there's

735
00:40:16,680 --> 00:40:18,480
 going to be two boxes in front of you,

736
00:40:18,480 --> 00:40:21,000
 but you don't know the size or location of the boxes.

737
00:40:21,000 --> 00:40:22,440
 Let me just read it carefully.

738
00:40:22,440 --> 00:40:25,600
 The robot must localize the pose and dimensions of the boxes

739
00:40:25,600 --> 00:40:30,320
 using a laser scanner mounted on the wrist, on the left wrist.

740
00:40:30,320 --> 00:40:32,840
 It's relatively easy when the boxes are separated,

741
00:40:32,840 --> 00:40:35,480
 but when they're squished together like C on the right,

742
00:40:35,480 --> 00:40:37,840
 then it's actually pretty hard.

743
00:40:37,840 --> 00:40:41,480
 So this is a simple example of if the robot is taking

744
00:40:41,480 --> 00:40:44,160
 information gathering actions, it'll actually

745
00:40:44,160 --> 00:40:46,600
 do something different in order to increase

746
00:40:46,600 --> 00:40:48,520
 its confidence of the location of the box

747
00:40:48,520 --> 00:40:50,400
 before it picks it up.

748
00:40:50,400 --> 00:40:52,440
 And you put this into the trajectory optimization

749
00:40:52,440 --> 00:40:57,160
 simulation where you take measurements as laser scanners

750
00:40:57,160 --> 00:40:59,280
 out there, and it actually decides

751
00:40:59,280 --> 00:41:01,440
 to go off and push on the left in order

752
00:41:01,440 --> 00:41:04,120
 to get a better sensor reading of the right.

753
00:41:04,120 --> 00:41:07,840
 And it's tracking a distribution over possible poses

754
00:41:07,840 --> 00:41:10,080
 of the box and the like.

755
00:41:10,080 --> 00:41:13,240
 And it makes the decision, just with trajectory optimization,

756
00:41:13,240 --> 00:41:15,640
 to take that information gathering action

757
00:41:15,640 --> 00:41:17,000
 to reduce its uncertainty.

758
00:41:17,000 --> 00:41:19,160
 And then it goes to pick up the box.

759
00:41:19,160 --> 00:41:22,240
 But that same algorithm, if the boxes started off separate,

760
00:41:22,240 --> 00:41:25,640
 it did its first scan and found it was fairly confident,

761
00:41:25,640 --> 00:41:27,480
 would have just gone in to pick up the box.

762
00:41:27,480 --> 00:41:30,800
 Same thing here.

763
00:41:30,800 --> 00:41:32,260
 You can actually see the-- this is

764
00:41:32,260 --> 00:41:35,260
 a rendering of the distribution over those possible locations.

765
00:41:35,260 --> 00:41:36,960
 Of course, it's a high dimensional thing,

766
00:41:36,960 --> 00:41:40,920
 so it's plotted down in a way that's a little bit hard--

767
00:41:40,920 --> 00:41:42,680
 that's why it's periodic is because it's

768
00:41:42,680 --> 00:41:44,600
 the raft of higher dimensional things plotted

769
00:41:44,600 --> 00:41:46,360
 on a single line.

770
00:41:46,360 --> 00:41:49,880
 And the big robot would make those decisions.

771
00:41:49,880 --> 00:41:55,800
 [INAUDIBLE]

772
00:41:55,800 --> 00:41:58,080
 You're saying local minima of trajectory optimization?

773
00:41:58,080 --> 00:41:58,580
 Yeah.

774
00:41:58,580 --> 00:42:04,600
 It's a really big question.

775
00:42:04,600 --> 00:42:06,880
 Is this kind of trajectory optimization more sensitive

776
00:42:06,880 --> 00:42:09,360
 to local minima, for instance?

777
00:42:09,360 --> 00:42:12,560
 In some way, I actually think it might be less sensitive,

778
00:42:12,560 --> 00:42:14,800
 even though it's solving a harder problem.

779
00:42:14,800 --> 00:42:16,800
 Because for the same reason we talked about with

780
00:42:16,800 --> 00:42:18,520
 the randomized smoothing and the whatever,

781
00:42:18,520 --> 00:42:20,720
 I actually think that putting distributions

782
00:42:20,720 --> 00:42:23,880
 over possible outcomes smooths out

783
00:42:23,880 --> 00:42:26,240
 some of the kinks in the cost landscape,

784
00:42:26,240 --> 00:42:28,800
 and it might be a little less sensitive.

785
00:42:28,800 --> 00:42:30,920
 But it would still have-- the big local minima

786
00:42:30,920 --> 00:42:32,920
 will still be there, but it might get rid

787
00:42:32,920 --> 00:42:34,840
 of some of the small local minima.

788
00:42:34,840 --> 00:42:35,320
 [INAUDIBLE]

789
00:42:35,320 --> 00:42:56,840
 Awesome.

790
00:42:56,840 --> 00:43:00,080
 Awesome.

791
00:43:00,080 --> 00:43:02,280
 Yeah, that's actually the last point I want to make.

792
00:43:02,280 --> 00:43:03,960
 So that's really good.

793
00:43:03,960 --> 00:43:06,840
 So just to be clear, this is doing information gathering.

794
00:43:06,840 --> 00:43:09,000
 That is, that push I would consider

795
00:43:09,000 --> 00:43:11,440
 to be only valuable for the sake of gathering information.

796
00:43:11,440 --> 00:43:14,920
 [INAUDIBLE]

797
00:43:14,920 --> 00:43:15,420
 Nope.

798
00:43:15,420 --> 00:43:19,160
 The goal here is to, with high probability, pick up the box.

799
00:43:19,160 --> 00:43:23,840
 And the only reason it does that is to reduce its uncertainty,

800
00:43:23,840 --> 00:43:25,040
 to gather information.

801
00:43:25,040 --> 00:43:26,840
 OK, but the second part of Leroy's question

802
00:43:26,840 --> 00:43:29,560
 was actually the biggest last point I want to make here,

803
00:43:29,560 --> 00:43:33,040
 which is that this really does have deep connections

804
00:43:33,040 --> 00:43:34,640
 to the state realization questions

805
00:43:34,640 --> 00:43:35,840
 we've been talking about.

806
00:43:35,840 --> 00:43:37,240
 I'll put it in a different slide,

807
00:43:37,240 --> 00:43:41,200
 just so we're not watching that.

808
00:43:41,200 --> 00:43:48,200
 OK, so now imagine I'm doing system ID, right?

809
00:43:48,200 --> 00:43:50,000
 Input output system ID, for instance,

810
00:43:50,000 --> 00:43:51,620
 to try to learn a state representation.

811
00:43:51,620 --> 00:43:55,100
 [WRITING ON BOARD]

812
00:43:55,100 --> 00:44:18,220
 In order to accurately predict my future observations,

813
00:44:18,220 --> 00:44:22,740
 if I'm going to learn a state space model for this,

814
00:44:22,740 --> 00:44:24,980
 in the linear system ID setting, we thought about this

815
00:44:24,980 --> 00:44:27,580
 as we've got a deterministic system we're trying to recover.

816
00:44:27,580 --> 00:44:31,140
 And it did recover the A, B, C, D matrices pretty well.

817
00:44:31,140 --> 00:44:32,780
 But if I have a stochastic system here

818
00:44:32,780 --> 00:44:34,940
 that is trying to recover, and its objective

819
00:44:34,940 --> 00:44:38,420
 is to predict y with its highest confidence possible,

820
00:44:38,420 --> 00:44:41,580
 then the state it has to learn is actually, I think,

821
00:44:41,580 --> 00:44:43,720
 better thought about as a belief state.

822
00:44:43,720 --> 00:44:47,100
 Now again, the belief states are not unique.

823
00:44:47,100 --> 00:44:49,420
 All the stuff we talked about with similarity transforms

824
00:44:49,420 --> 00:44:51,860
 and the like is still present here.

825
00:44:51,860 --> 00:44:54,940
 And it might be that the belief state is not minimal,

826
00:44:54,940 --> 00:45:00,020
 that tracking all the things that the real state might do

827
00:45:00,020 --> 00:45:02,860
 might be more than you need to predict y.

828
00:45:02,860 --> 00:45:05,020
 When we talked about approximate information states,

829
00:45:05,020 --> 00:45:07,980
 that's exactly the idea of finding a state representation

830
00:45:07,980 --> 00:45:12,220
 in here, which is an approximate belief state.

831
00:45:12,220 --> 00:45:17,100
 Similarly, if I'm doing RL, let's say,

832
00:45:17,100 --> 00:45:23,100
 let's say via policy gradient, or if you

833
00:45:23,100 --> 00:45:28,460
 do a policy gradient with a dynamic policy,

834
00:45:28,460 --> 00:45:35,980
 like an LSTM or something, or if you have a value function or a Q

835
00:45:35,980 --> 00:45:45,340
 function that has some dynamic, some states-- by the way,

836
00:45:45,340 --> 00:45:48,180
 I think if you do that, you've walked away from RL theory.

837
00:45:48,180 --> 00:45:50,840
 I think there's-- I mean, people are working on that theory now,

838
00:45:50,840 --> 00:45:53,740
 but that's not the standard thing to do in theory.

839
00:45:53,740 --> 00:45:55,200
 But if people do it in practice now,

840
00:45:55,200 --> 00:45:58,340
 they'll put an LSTM representing the value function or the Q

841
00:45:58,340 --> 00:46:00,620
 function.

842
00:46:00,620 --> 00:46:02,620
 And the states that this thing has

843
00:46:02,620 --> 00:46:05,740
 to acquire in order to accomplish the task-- let's

844
00:46:05,740 --> 00:46:09,220
 say we had an oracular agent that would just

845
00:46:09,220 --> 00:46:12,420
 solve the RL problem and solve the representation along

846
00:46:12,420 --> 00:46:13,740
 with it.

847
00:46:13,740 --> 00:46:17,500
 Then the dynamic, the state in the controller

848
00:46:17,500 --> 00:46:19,300
 is probably best understood as being

849
00:46:19,300 --> 00:46:25,020
 an approximate state of the belief space of the system.

850
00:46:25,020 --> 00:46:27,980
 That's what's required to make optimal decisions.

851
00:46:27,980 --> 00:46:31,620
 Similarly, the value function, if trained--

852
00:46:31,620 --> 00:46:35,860
 so this would be a task-relevant approximate state.

853
00:46:35,860 --> 00:46:38,860
 And this one, similarly, would be just the part

854
00:46:38,860 --> 00:46:41,860
 of the belief space you would need in order

855
00:46:41,860 --> 00:46:45,060
 to accurately predict values.

856
00:46:45,060 --> 00:46:49,260
 So I do think RL is potentially doing this.

857
00:46:49,260 --> 00:46:51,300
 And I think that the language of belief

858
00:46:51,300 --> 00:46:53,100
 is exactly the right way to think about what

859
00:46:53,100 --> 00:46:54,580
 RL is doing in those cases.

860
00:46:59,980 --> 00:47:01,700
 So you should learn more about belief space planning.

861
00:47:01,700 --> 00:47:02,380
 It's good stuff.

862
00:47:02,380 --> 00:47:09,340
 OK, let me step back and just cover the course again

863
00:47:09,340 --> 00:47:11,020
 in a few minutes.

864
00:47:11,020 --> 00:47:13,500
 I think it's really helpful to just sort of connect it

865
00:47:13,500 --> 00:47:15,940
 all together.

866
00:47:15,940 --> 00:47:22,540
 So we've done a lot of things, covered a lot of tools,

867
00:47:22,540 --> 00:47:26,420
 sometimes at a level that I wish I could spend four lectures on.

868
00:47:26,420 --> 00:47:27,580
 We spent less.

869
00:47:28,580 --> 00:47:33,540
 But I hope you came away with a lot of tools.

870
00:47:33,540 --> 00:47:37,980
 And it's been very rewarding for me

871
00:47:37,980 --> 00:47:41,220
 to see you guys hit some of the subtler points in your projects,

872
00:47:41,220 --> 00:47:43,620
 for instance.

873
00:47:43,620 --> 00:47:46,700
 So let's do a kind of a where have we been.

874
00:47:46,700 --> 00:47:51,540
 People also asked in the survey for things

875
00:47:51,540 --> 00:47:55,860
 like predict manipulation 40 years in the future.

876
00:47:55,860 --> 00:47:59,460
 That's hard, but I'll try to say a few things about where

877
00:47:59,460 --> 00:48:00,180
 it's going, too.

878
00:48:00,180 --> 00:48:05,660
 So we started off, after the basic introduction,

879
00:48:05,660 --> 00:48:11,740
 we started off with just basic kinematics, Jacobians,

880
00:48:11,740 --> 00:48:14,780
 stuff like that.

881
00:48:14,780 --> 00:48:18,940
 The multibody notation is something

882
00:48:18,940 --> 00:48:22,460
 that I doubt too many people will move forward

883
00:48:22,460 --> 00:48:26,780
 with as part of your major, part of your life.

884
00:48:26,780 --> 00:48:29,060
 But if you do, you'll be happier.

885
00:48:29,060 --> 00:48:30,780
 I promise.

886
00:48:30,780 --> 00:48:37,300
 I've seen bugs in notebooks that-- I mean, I've made my own.

887
00:48:37,300 --> 00:48:42,140
 If you find yourself frustrated that the Jacobian you got out

888
00:48:42,140 --> 00:48:43,940
 of diff IK is in the wrong frame,

889
00:48:43,940 --> 00:48:47,180
 or your forces somehow seem to be in the wrong space

890
00:48:47,180 --> 00:48:49,700
 or something like this, more careful use

891
00:48:49,700 --> 00:48:52,540
 of multibody notation will save you.

892
00:48:52,540 --> 00:48:54,900
 Consider it.

893
00:48:54,900 --> 00:48:56,900
 It really does help, I think.

894
00:48:56,900 --> 00:48:59,060
 And I think the general view-- I try

895
00:48:59,060 --> 00:49:02,900
 to push sort of less about the mechanics

896
00:49:02,900 --> 00:49:05,100
 of the kinematic equations, which is a slightly more

897
00:49:05,100 --> 00:49:06,100
 standard treatment.

898
00:49:06,100 --> 00:49:08,860
 But I think thinking of it as a spatial algebra

899
00:49:08,860 --> 00:49:12,340
 and understanding the basic operations of how rotations

900
00:49:12,340 --> 00:49:16,620
 affect frames and stuff like this,

901
00:49:16,620 --> 00:49:19,980
 that's a lesson that we came back to multiple times.

902
00:49:19,980 --> 00:49:21,860
 And I really do think a lot of you

903
00:49:21,860 --> 00:49:25,140
 found that the differential IK pipeline became

904
00:49:25,140 --> 00:49:30,980
 a workhorse for your projects, a lot of people using it.

905
00:49:30,980 --> 00:49:35,180
 And some of you really, I think, got to appreciate it.

906
00:49:35,180 --> 00:49:36,260
 Or maybe you're mad at it.

907
00:49:36,260 --> 00:49:40,700
 But a month from now, you'll be really appreciative of it,

908
00:49:40,700 --> 00:49:41,900
 maybe.

909
00:49:41,900 --> 00:49:44,360
 For instance, one of my regrets is

910
00:49:44,360 --> 00:49:47,340
 that a bunch of people copied the IWA Painter notebook.

911
00:49:47,340 --> 00:49:51,100
 And that had only used pseudo-inverse control, not

912
00:49:51,100 --> 00:49:53,180
 the full diff IK.

913
00:49:53,180 --> 00:49:55,300
 Because that was sufficient for that notebook.

914
00:49:55,300 --> 00:49:57,020
 And I hadn't sort of pictured everybody

915
00:49:57,020 --> 00:50:00,020
 copying it and trying to use it for more than it was good for.

916
00:50:00,020 --> 00:50:03,020
 The pseudo-inverse controller can run into singularities.

917
00:50:03,020 --> 00:50:04,300
 And some of you did.

918
00:50:04,300 --> 00:50:05,380
 And it blows up.

919
00:50:05,380 --> 00:50:06,820
 Unfortunately, the way it blows up

920
00:50:06,820 --> 00:50:09,580
 is it causes multibody plant to say, I can't--

921
00:50:09,580 --> 00:50:12,980
 or it says the integrator-- I run into time step

922
00:50:12,980 --> 00:50:14,140
 equals 1e to the minus 14.

923
00:50:14,140 --> 00:50:15,900
 And that's not a very clear message.

924
00:50:15,900 --> 00:50:18,740
 But fine, that was just the pseudo-inverse

925
00:50:18,740 --> 00:50:20,180
 being insufficient.

926
00:50:20,180 --> 00:50:22,780
 And if you switched over to the diff IK, which

927
00:50:22,780 --> 00:50:25,140
 was the least squares interpretation, which

928
00:50:25,140 --> 00:50:28,780
 allowed you to have constraints, then those issues went away.

929
00:50:28,780 --> 00:50:34,540
 So the differential IK, as an optimization,

930
00:50:34,540 --> 00:50:37,780
 I think is a workhorse and a thing I

931
00:50:37,780 --> 00:50:40,900
 hope you feel like you learned.

932
00:50:40,900 --> 00:50:43,020
 We jumped into geometric perception.

933
00:50:43,020 --> 00:50:48,860
 There's a party going on somewhere.

934
00:50:48,860 --> 00:50:58,020
 Of course, we learned iterative closest point and its variance.

935
00:50:58,020 --> 00:51:04,500
 I would say both the kinematics and the ICP kind of work

936
00:51:04,500 --> 00:51:08,340
 helped me start talking about many of these problems,

937
00:51:08,340 --> 00:51:10,540
 kinematics problems as optimizations, too.

938
00:51:10,540 --> 00:51:16,300
 And I think the takeaway that some of you

939
00:51:16,300 --> 00:51:18,900
 are seeing when you're playing with perception on the project

940
00:51:18,900 --> 00:51:21,800
 is that these point cloud processing algorithms

941
00:51:21,800 --> 00:51:27,220
 are very good for refinement if you have a known geometry.

942
00:51:27,220 --> 00:51:28,940
 But they're not great for the global part

943
00:51:28,940 --> 00:51:29,980
 of the perception problem.

944
00:51:29,980 --> 00:51:32,100
 And you really wanted to bring in the deep learning

945
00:51:32,100 --> 00:51:36,940
 pipeline to help with the bigger part of the problem.

946
00:51:36,940 --> 00:51:40,300
 And it requires those require models.

947
00:51:40,300 --> 00:51:50,100
 They're great for accuracy, let's say for refinement,

948
00:51:50,100 --> 00:51:51,740
 if you will.

949
00:51:51,740 --> 00:51:53,020
 But they need an initial guess.

950
00:51:53,020 --> 00:52:01,300
 And remember I said that if you were to take away--

951
00:52:01,300 --> 00:52:04,760
 if you told me I could only have RGB or I could only have depth,

952
00:52:04,760 --> 00:52:06,620
 my answer would have flipped a few years ago.

953
00:52:06,620 --> 00:52:08,500
 And I'd say, take my depth.

954
00:52:08,500 --> 00:52:09,380
 I'll keep my RGB.

955
00:52:09,380 --> 00:52:16,140
 We built up more into the clutter clearing

956
00:52:16,140 --> 00:52:22,820
 was the example that I used for a few reasons.

957
00:52:22,820 --> 00:52:27,260
 We started to talk about perception in clutter, richer

958
00:52:27,260 --> 00:52:30,380
 perception that could handle the occlusions and things

959
00:52:30,380 --> 00:52:35,600
 like that, about more complicated simulation

960
00:52:35,600 --> 00:52:40,320
 mechanics, and about even programming at the task level.

961
00:52:40,320 --> 00:52:42,600
 So this was scaling up the basic recipe

962
00:52:42,600 --> 00:52:45,460
 into a really much more sophisticated version

963
00:52:45,460 --> 00:52:46,080
 of the problem.

964
00:52:46,080 --> 00:52:51,880
 It also helped make the point that we didn't

965
00:52:51,880 --> 00:52:55,120
 need to estimate the pose perfectly in order

966
00:52:55,120 --> 00:52:56,000
 to be successful.

967
00:52:56,000 --> 00:52:58,200
 Because that clutter clearing demo

968
00:52:58,200 --> 00:52:59,720
 was just using antipodal grasps.

969
00:52:59,720 --> 00:53:04,120
 It wasn't even thinking about what objects were.

970
00:53:04,120 --> 00:53:05,120
 And it went pretty far.

971
00:53:05,120 --> 00:53:15,880
 We jumped into deep perception.

972
00:53:15,880 --> 00:53:24,880
 We talked about Mask-RCNN and the like.

973
00:53:27,640 --> 00:53:32,080
 That was the first workhorse.

974
00:53:32,080 --> 00:53:34,520
 If you're starting to do perception in the real world,

975
00:53:34,520 --> 00:53:37,920
 you might very well still be using Mask-RCNN.

976
00:53:37,920 --> 00:53:46,040
 We talked about deep pose estimation,

977
00:53:46,040 --> 00:53:51,840
 the category level versions of this,

978
00:53:51,840 --> 00:53:56,000
 with, for instance, dense descriptors and key points,

979
00:53:56,000 --> 00:54:00,720
 for instance, being an alternative to actually

980
00:54:00,720 --> 00:54:02,280
 estimating the pose.

981
00:54:02,280 --> 00:54:04,840
 Maybe key points are enough, or dense descriptors.

982
00:54:04,840 --> 00:54:16,440
 These are super powerful methods.

983
00:54:16,440 --> 00:54:17,400
 They're getting better.

984
00:54:17,400 --> 00:54:18,280
 They're data hungry.

985
00:54:18,280 --> 00:54:20,640
 I think if there's one thing that we're

986
00:54:20,640 --> 00:54:24,240
 seeing as today's trend that will continue

987
00:54:24,240 --> 00:54:28,240
 is that a lot of the pipelines that started off

988
00:54:28,240 --> 00:54:30,840
 being hugely successful based on supervised learning

989
00:54:30,840 --> 00:54:32,880
 are now turning over into self-supervised learning

990
00:54:32,880 --> 00:54:34,680
 versions of these problems.

991
00:54:34,680 --> 00:54:39,560
 Finding good ways to train a visual representation that's

992
00:54:39,560 --> 00:54:41,920
 sufficient for these kind of downstream tasks

993
00:54:41,920 --> 00:54:46,400
 using unlabeled data is the big new trend.

994
00:54:46,400 --> 00:54:47,480
 Not even that new anymore.

995
00:54:47,480 --> 00:54:51,400
 We did motion planning.

996
00:54:51,400 --> 00:54:53,520
 We covered a lot of stuff.

997
00:54:53,520 --> 00:54:59,200
 We did motion planning, which started with just richer

998
00:54:59,200 --> 00:55:02,040
 spelling of inverse kinematics.

999
00:55:02,040 --> 00:55:03,000
 All the power you use.

1000
00:55:03,000 --> 00:55:06,440
 And a lot of you guys are using inverse kinematics only,

1001
00:55:06,440 --> 00:55:10,720
 actually, I'd say, calling a lot of sequential inverse

1002
00:55:10,720 --> 00:55:12,320
 kinematics calls.

1003
00:55:12,320 --> 00:55:14,480
 And a handful of times I've been saying,

1004
00:55:14,480 --> 00:55:16,840
 maybe you should turn that into a kinematic trajectory

1005
00:55:16,840 --> 00:55:17,440
 optimization.

1006
00:55:22,240 --> 00:55:23,720
 Why?

1007
00:55:23,720 --> 00:55:25,760
 Because solving a bunch of inverse kinematics

1008
00:55:25,760 --> 00:55:29,240
 calls independently is good, but it doesn't actually

1009
00:55:29,240 --> 00:55:32,080
 ask them to be related to each other in any smooth or subtle

1010
00:55:32,080 --> 00:55:33,200
 way.

1011
00:55:33,200 --> 00:55:35,280
 And so I tried to say the kinematic trajectory

1012
00:55:35,280 --> 00:55:39,920
 optimization was just inverse kinematics with the constraint

1013
00:55:39,920 --> 00:55:41,600
 that the inverse kinematics solutions are

1014
00:55:41,600 --> 00:55:42,920
 consistent with each other.

1015
00:55:42,920 --> 00:55:44,720
 They can all be described from one spline.

1016
00:55:44,720 --> 00:55:50,560
 And we talked about sample-based motion planning, too.

1017
00:55:51,560 --> 00:55:56,760
 I'll say sampling-based.

1018
00:55:56,760 --> 00:55:58,760
 Some powerful tools.

1019
00:55:58,760 --> 00:56:01,520
 I threw in some stuff about graph of convex sets there,

1020
00:56:01,520 --> 00:56:02,200
 too, of course.

1021
00:56:02,200 --> 00:56:11,480
 If you remember RRT and PRM, then you've

1022
00:56:11,480 --> 00:56:13,680
 got that basic vocabulary.

1023
00:56:16,640 --> 00:56:21,320
 Do you remember then all of the stuff about the different ways

1024
00:56:21,320 --> 00:56:24,120
 we're doing control on the manipulator?

1025
00:56:24,120 --> 00:56:32,680
 We had our next foray into force control and manipulator

1026
00:56:32,680 --> 00:56:33,180
 control.

1027
00:56:45,640 --> 00:56:49,200
 Can you remember why PID control is good,

1028
00:56:49,200 --> 00:56:51,700
 but inverse dynamics control is better if you have a model?

1029
00:56:51,700 --> 00:56:57,440
 And why we actually use joint stiffness control?

1030
00:56:57,440 --> 00:57:08,640
 In a lot of cases for the robot, we

1031
00:57:08,640 --> 00:57:11,560
 like to think about executing joint trajectories,

1032
00:57:11,560 --> 00:57:14,640
 but relatively with a low stiffness controller,

1033
00:57:14,640 --> 00:57:17,440
 so that if we bump into stuff, we're

1034
00:57:17,440 --> 00:57:21,240
 still compliant enough to keep moving and not

1035
00:57:21,240 --> 00:57:22,700
 break our robot or the environment.

1036
00:57:22,700 --> 00:57:34,240
 But we also talked about direct force control,

1037
00:57:34,240 --> 00:57:37,200
 where you're thinking explicitly about the forces,

1038
00:57:37,200 --> 00:57:44,840
 or indirect force control, like Cartesian impedance control,

1039
00:57:44,840 --> 00:57:46,360
 or Cartesian stiffness control.

1040
00:57:46,360 --> 00:57:52,560
 One of my favorite examples that came up with that, actually.

1041
00:57:52,560 --> 00:57:56,200
 Remember the-- a few people are doing writing projects, right?

1042
00:57:56,200 --> 00:58:00,880
 And I gave the Meshcat painter a little thing that just says,

1043
00:58:00,880 --> 00:58:03,320
 put a chalk, weld it to your hand if you want,

1044
00:58:03,320 --> 00:58:06,560
 and draw some lines.

1045
00:58:06,560 --> 00:58:09,100
 And it was interesting to have the conversations with people,

1046
00:58:09,100 --> 00:58:14,520
 because in the case where the chalk is welded to your finger,

1047
00:58:14,520 --> 00:58:18,800
 the difference between force control and just a Diff IK

1048
00:58:18,800 --> 00:58:21,280
 control, for instance, with joint stiffness

1049
00:58:21,280 --> 00:58:23,960
 controller, or inverse dynamics, is small.

1050
00:58:23,960 --> 00:58:28,740
 Because you just put yourself into a reasonable amount

1051
00:58:28,740 --> 00:58:31,760
 of penetration, you move yourself around,

1052
00:58:31,760 --> 00:58:32,880
 and that's all good.

1053
00:58:32,880 --> 00:58:35,440
 The robot will-- you might have to do a little tuning

1054
00:58:35,440 --> 00:58:37,680
 to not push too hard, because otherwise the chalk will

1055
00:58:37,680 --> 00:58:38,200
 get stuck.

1056
00:58:38,200 --> 00:58:40,280
 If you don't push hard enough, it might not draw.

1057
00:58:40,280 --> 00:58:44,040
 But pretty much, you just tune in once how deep to push.

1058
00:58:44,040 --> 00:58:45,960
 You follow your trajectory.

1059
00:58:45,960 --> 00:58:47,600
 Life is good.

1060
00:58:47,600 --> 00:58:51,480
 But the people who switched from welding it to the finger

1061
00:58:51,480 --> 00:58:55,520
 to holding the chalk had a different experience.

1062
00:58:55,520 --> 00:58:57,220
 So as soon as you push down, the chalk

1063
00:58:57,220 --> 00:58:59,040
 might move in your fingers.

1064
00:58:59,040 --> 00:59:01,640
 As you draw, it might start moving in your fingers.

1065
00:59:01,640 --> 00:59:04,280
 And suddenly there, if you just picked a nice trajectory

1066
00:59:04,280 --> 00:59:07,400
 and started moving around, then you

1067
00:59:07,400 --> 00:59:08,960
 might have drawn for a little while,

1068
00:59:08,960 --> 00:59:11,120
 and now you stopped drawing, because it

1069
00:59:11,120 --> 00:59:12,120
 moved in your fingers.

1070
00:59:12,120 --> 00:59:15,320
 And it's hard to know where the chalk is in your fingers.

1071
00:59:15,320 --> 00:59:17,440
 So actually, this is a beautiful case

1072
00:59:17,440 --> 00:59:19,400
 where, if you think about the space of forces,

1073
00:59:19,400 --> 00:59:20,760
 you just say, I'd like to be pushing down

1074
00:59:20,760 --> 00:59:22,200
 with a certain amount of force.

1075
00:59:22,200 --> 00:59:24,800
 Then even if the chalk moves, the end effector

1076
00:59:24,800 --> 00:59:26,760
 will move for you in order to keep yourself

1077
00:59:26,760 --> 00:59:29,280
 in contact with the table.

1078
00:59:29,280 --> 00:59:40,240
 So we talked about controlling not just the robot,

1079
00:59:40,240 --> 00:59:44,400
 but then the objects in the world.

1080
00:59:44,400 --> 00:59:48,440
 I used the language of visual motor policies

1081
00:59:48,440 --> 00:59:50,400
 to talk about that.

1082
00:59:50,400 --> 00:59:53,360
 I really think something great happened

1083
00:59:53,360 --> 00:59:55,960
 when we started putting cameras at high rate

1084
00:59:55,960 --> 00:59:57,280
 into our controllers.

1085
00:59:57,280 --> 00:59:59,280
 And we need to understand it better.

1086
00:59:59,280 --> 01:00:01,880
 Right now, I'd say our ability to get visual motor policies

1087
01:00:01,880 --> 01:00:03,400
 is still a little weak.

1088
01:00:03,400 --> 01:00:05,240
 We did it with behavior cloning.

1089
01:00:05,240 --> 01:00:12,520
 We talked about it with RL policy search, for instance.

1090
01:00:12,520 --> 01:00:20,120
 But we should have more powerful, reliable ways

1091
01:00:20,120 --> 01:00:21,600
 to get visual motor policies.

1092
01:00:21,600 --> 01:00:23,840
 They're very good.

1093
01:00:23,840 --> 01:00:27,080
 We're still working on it.

1094
01:00:27,080 --> 01:00:29,440
 But this is the stuff that's making the rock star

1095
01:00:29,440 --> 01:00:32,000
 manipulation demos right now.

1096
01:00:32,000 --> 01:00:33,440
 I showed you rolling dough.

1097
01:00:33,440 --> 01:00:35,840
 There's all kinds of things that visual motor policies

1098
01:00:35,840 --> 01:00:36,980
 can do that are surprising.

1099
01:00:36,980 --> 01:00:44,680
 And then we wrapped up with intuitive physics,

1100
01:00:44,680 --> 01:00:54,040
 learning models, task and motion planning,

1101
01:00:54,040 --> 01:00:55,960
 and a little bit of belief space today.

1102
01:00:56,960 --> 01:01:01,680
 So that's a lot of coverage.

1103
01:01:01,680 --> 01:01:04,200
 We've covered a lot of things, some of them more carefully

1104
01:01:04,200 --> 01:01:07,360
 and some of them just quickly at the end.

1105
01:01:07,360 --> 01:01:09,960
 But I think it's a pretty good representation of what's

1106
01:01:09,960 --> 01:01:13,120
 happening in a modern manipulation system.

1107
01:01:13,120 --> 01:01:17,760
 When I reflect on the class, and maybe what I'll do next time,

1108
01:01:17,760 --> 01:01:22,920
 let's say, but the one thing that I

1109
01:01:22,920 --> 01:01:27,360
 think this overly emphasizes, and maybe I

1110
01:01:27,360 --> 01:01:29,280
 wish I would emphasize more, I think

1111
01:01:29,280 --> 01:01:32,680
 I'm going to put mobile manipulation earlier

1112
01:01:32,680 --> 01:01:34,560
 in the class.

1113
01:01:34,560 --> 01:01:38,360
 Because I think it opens up-- I didn't realize-- I mean,

1114
01:01:38,360 --> 01:01:40,840
 I think the tools are actually not that different to solve

1115
01:01:40,840 --> 01:01:42,040
 mobile manipulation.

1116
01:01:42,040 --> 01:01:43,760
 The math is the same.

1117
01:01:43,760 --> 01:01:47,680
 But the ideas you would have for your projects, I think,

1118
01:01:47,680 --> 01:01:49,480
 are going to be different.

1119
01:01:49,480 --> 01:01:52,120
 I think Brian's lecture yesterday really

1120
01:01:52,120 --> 01:01:54,160
 emphasized that, right?

1121
01:01:54,160 --> 01:02:00,400
 You wouldn't ask a chatbot, what should I do with-- go get me

1122
01:02:00,400 --> 01:02:03,280
 a Coke or something like that, if you're limited

1123
01:02:03,280 --> 01:02:06,040
 to the world of your table.

1124
01:02:06,040 --> 01:02:09,840
 And I think the open vocabulary ideas,

1125
01:02:09,840 --> 01:02:12,440
 the anything could happen in the world,

1126
01:02:12,440 --> 01:02:14,960
 you're going to send your robot off and do anything,

1127
01:02:14,960 --> 01:02:16,400
 wheels help you think about that.

1128
01:02:16,400 --> 01:02:21,480
 You could bring a lot of things to yourself in a conveyor belt,

1129
01:02:21,480 --> 01:02:22,880
 and that's not the same.

1130
01:02:22,880 --> 01:02:26,040
 So even though the math is actually very similar,

1131
01:02:26,040 --> 01:02:28,320
 I'm going to probably make a bigger

1132
01:02:28,320 --> 01:02:32,240
 emphasis on mobile manipulation next time.

1133
01:02:32,240 --> 01:02:34,320
 There are some different parts of the math

1134
01:02:34,320 --> 01:02:37,160
 where people think about navigation and mapping

1135
01:02:37,160 --> 01:02:41,720
 and other scene level kind of perception problems

1136
01:02:41,720 --> 01:02:43,240
 that would come along with that.

1137
01:02:43,240 --> 01:02:44,680
 But I think the biggest thing for me

1138
01:02:44,680 --> 01:02:48,560
 is just the needing to think about the open domain,

1139
01:02:48,560 --> 01:02:50,160
 open vocabulary part of the world.

1140
01:02:50,600 --> 01:02:52,040
 [PAPER RUSTLING]

1141
01:02:52,040 --> 01:02:58,160
 For me-- and I'm saying this partly so you can agree with me

1142
01:02:58,160 --> 01:03:01,320
 or disagree with me over anonymous feedback is fine.

1143
01:03:01,320 --> 01:03:04,480
 You can shout it out right now, that's fine.

1144
01:03:04,480 --> 01:03:07,360
 The other thing that I think I want to emphasize--

1145
01:03:07,360 --> 01:03:09,320
 and I said it on Tuesday-- I want

1146
01:03:09,320 --> 01:03:13,040
 to give a few more tools that you could, in your projects,

1147
01:03:13,040 --> 01:03:16,280
 for instance, use for the task level reasoning.

1148
01:03:16,280 --> 01:03:19,520
 I think if you could have just written a Pudittles

1149
01:03:19,520 --> 01:03:22,240
 specification, you might not love writing Pudittles.

1150
01:03:22,240 --> 01:03:23,360
 It's kind of weird.

1151
01:03:23,360 --> 01:03:26,080
 But it's very powerful to be able to think

1152
01:03:26,080 --> 01:03:30,320
 about longer term tasks, more abstract tasks.

1153
01:03:30,320 --> 01:03:34,000
 And I'm thinking that the presentation focused

1154
01:03:34,000 --> 01:03:37,160
 a little bit more on the dexterous part of manipulation

1155
01:03:37,160 --> 01:03:39,840
 and a little less about the world part.

1156
01:03:39,840 --> 01:03:41,980
 But you can leave with a slightly-- knowing

1157
01:03:41,980 --> 01:03:43,020
 that there's other parts.

1158
01:03:43,020 --> 01:03:45,720
 In fact, it's interesting to think about--

1159
01:03:45,720 --> 01:03:51,200
 when I was thinking about that dichotomy,

1160
01:03:51,200 --> 01:04:00,360
 it just happens that at TRI, the org chart is kind of telling.

1161
01:04:00,360 --> 01:04:03,400
 So there's a dexterous manipulation team.

1162
01:04:03,400 --> 01:04:10,600
 But there's also a mobile manipulation team, separate.

1163
01:04:15,040 --> 01:04:17,040
 And it really does bring-- they're complementary.

1164
01:04:17,040 --> 01:04:19,040
 There's a lot of problems that you get into,

1165
01:04:19,040 --> 01:04:21,840
 where you don't need-- the mobile manipulation team,

1166
01:04:21,840 --> 01:04:23,760
 I showed you their grocery store robot.

1167
01:04:23,760 --> 01:04:26,180
 They were happy with a suction gripper for a lot of things.

1168
01:04:26,180 --> 01:04:29,480
 They weren't thinking about the dexterity required.

1169
01:04:29,480 --> 01:04:30,940
 But they're moving through the world

1170
01:04:30,940 --> 01:04:33,160
 and experiencing things that my robot on the table

1171
01:04:33,160 --> 01:04:36,160
 is not experiencing.

1172
01:04:36,160 --> 01:04:37,940
 And once I said that, I realized, OK, well,

1173
01:04:37,940 --> 01:04:39,800
 I haven't said enough about soft robots.

1174
01:04:39,800 --> 01:04:43,000
 There's a soft robotics team.

1175
01:04:43,000 --> 01:04:46,200
 And there's also a human-robot interaction team.

1176
01:04:46,200 --> 01:04:46,960
 I'll write it out.

1177
01:04:46,960 --> 01:04:56,480
 We mentioned soft.

1178
01:04:56,480 --> 01:04:59,680
 And I offered to spend a lecture talking about tactile sensing.

1179
01:04:59,680 --> 01:05:03,520
 But we didn't get to that one.

1180
01:05:03,520 --> 01:05:06,800
 And human-robot interaction is hugely important.

1181
01:05:06,800 --> 01:05:08,920
 It's just not my expertise, really.

1182
01:05:08,920 --> 01:05:19,960
 But any thoughts or questions or anything

1183
01:05:19,960 --> 01:05:22,200
 about that high-level scope stuff?

1184
01:05:22,200 --> 01:05:27,040
 Feedback?

1185
01:05:27,040 --> 01:05:27,540
 Yeah?

1186
01:05:27,540 --> 01:05:29,840
 What would be the next steps?

1187
01:05:29,840 --> 01:05:32,480
 What would be the next steps for you

1188
01:05:32,480 --> 01:05:36,080
 as a manipulator, a robot programmer?

1189
01:05:36,080 --> 01:05:38,800
 [INAUDIBLE]

1190
01:05:38,800 --> 01:05:40,240
 Yeah, for you as students, yeah.

1191
01:05:40,240 --> 01:05:41,800
 There are a lot of really good classes.

1192
01:05:41,800 --> 01:05:43,080
 I don't know which of them you've taken

1193
01:05:43,080 --> 01:05:44,480
 and which of you haven't.

1194
01:05:44,480 --> 01:05:46,020
 I'll be teaching underactuated, which

1195
01:05:46,020 --> 01:05:48,440
 I've advertised a few times in the spring.

1196
01:05:48,440 --> 01:05:51,320
 But there's great classes by Luca Caralone

1197
01:05:51,320 --> 01:05:55,560
 about perception and state estimation and the like.

1198
01:05:55,560 --> 01:05:58,080
 In fact, I could just summarize a list

1199
01:05:58,080 --> 01:06:01,960
 of some of the great classes, maybe on a Piazza post.

1200
01:06:01,960 --> 01:06:04,360
 I'd be happy to do that.

1201
01:06:04,360 --> 01:06:06,040
 We have a lot of good classes on campus.

1202
01:06:06,040 --> 01:06:07,160
 Maybe not enough, actually.

1203
01:06:07,160 --> 01:06:08,080
 I'd love to see more.

1204
01:06:08,080 --> 01:06:11,380
 Yeah?

1205
01:06:11,380 --> 01:06:14,240
 Is this more of a research toolkit

1206
01:06:14,240 --> 01:06:17,040
 rather than what people are applying to robots

1207
01:06:17,040 --> 01:06:20,000
 when they start a robot?

1208
01:06:20,000 --> 01:06:21,000
 That's a great question.

1209
01:06:21,000 --> 01:06:22,520
 Is this a research toolkit?

1210
01:06:22,520 --> 01:06:25,960
 Or is this a, I need a robot to move today

1211
01:06:25,960 --> 01:06:28,360
 to make my startup work kind of toolkit?

1212
01:06:28,360 --> 01:06:32,040
 I think there's a lot of robots that

1213
01:06:32,040 --> 01:06:35,320
 do things that you'd consider to be manipulation that don't

1214
01:06:35,320 --> 01:06:37,320
 use a big part of the stack.

1215
01:06:37,320 --> 01:06:41,960
 But they are the places where the world is more constrained.

1216
01:06:41,960 --> 01:06:44,680
 So the classic example would be a factory room floor

1217
01:06:44,680 --> 01:06:46,640
 where you're welding or something like this.

1218
01:06:46,640 --> 01:06:50,800
 It uses maybe force control, a lot of position programming

1219
01:06:50,800 --> 01:06:51,520
 and the like.

1220
01:06:51,520 --> 01:06:53,000
 But it doesn't need to think about perception.

1221
01:06:53,000 --> 01:06:55,160
 It doesn't need to think about all the uncertainty

1222
01:06:55,160 --> 01:06:56,800
 and complexity or even planning that

1223
01:06:56,800 --> 01:07:00,160
 comes with the fact that the world could be very diverse.

1224
01:07:00,160 --> 01:07:04,640
 And I think in industry, startups, big companies

1225
01:07:04,640 --> 01:07:07,840
 are now investing a lot in the next generation of robots,

1226
01:07:07,840 --> 01:07:11,240
 starting with more flexible manufacturing,

1227
01:07:11,240 --> 01:07:16,280
 flexible logistics, the Amazon problem, the delivery problems.

1228
01:07:16,280 --> 01:07:19,080
 And I think that they are hitting this straight up.

1229
01:07:19,080 --> 01:07:23,240
 This is core material for that kind of a job.

1230
01:07:23,240 --> 01:07:24,800
 And then absolutely, there's research

1231
01:07:24,800 --> 01:07:26,600
 that is taking every one of those

1232
01:07:26,600 --> 01:07:28,480
 and pushing farther.

1233
01:07:28,480 --> 01:07:31,160
 But I think as soon as you start needing

1234
01:07:31,160 --> 01:07:35,640
 to perceive the world in order to do your manipulation,

1235
01:07:35,640 --> 01:07:38,480
 and that's driven by the task, then the old stuff

1236
01:07:38,480 --> 01:07:39,400
 isn't getting it done.

1237
01:07:39,400 --> 01:07:43,640
 And this stuff is bread and butter.

1238
01:07:43,640 --> 01:07:44,440
 Yeah, thank you.

1239
01:07:44,440 --> 01:07:54,360
 So people ask me to predict the future.

1240
01:07:54,360 --> 01:07:55,320
 I can't do that.

1241
01:07:55,320 --> 01:07:57,240
 But I'll give you a few thoughts if you want.

1242
01:07:57,240 --> 01:08:04,040
 So in fact, Rod Brooks, another famous roboticist

1243
01:08:04,040 --> 01:08:08,040
 that got to-- went off and was lab director

1244
01:08:08,040 --> 01:08:11,120
 and then started a company.

1245
01:08:11,120 --> 01:08:13,640
 But I took his class when I was a student,

1246
01:08:13,640 --> 01:08:16,560
 embodied intelligence.

1247
01:08:16,560 --> 01:08:18,840
 I think it was called embodied intelligence, yeah.

1248
01:08:25,280 --> 01:08:29,480
 He always says that people have a tendency-- he reminds us.

1249
01:08:29,480 --> 01:08:30,680
 It's not his quote, I guess.

1250
01:08:30,680 --> 01:08:34,320
 But people have a tendency to overestimate

1251
01:08:34,320 --> 01:08:37,040
 the importance of a new technology in the short term

1252
01:08:37,040 --> 01:08:41,160
 and dramatically underestimate the potential

1253
01:08:41,160 --> 01:08:42,520
 in the long term.

1254
01:08:42,520 --> 01:08:46,720
 So I'm just saying everything I'm about to say is wrong.

1255
01:08:46,720 --> 01:08:50,440
 But I do think there's some huge trends that we've

1256
01:08:50,440 --> 01:08:54,360
 seen enough of to lean into, right?

1257
01:08:54,360 --> 01:08:57,360
 I'd say, actually, Bojan's talk last time

1258
01:08:57,360 --> 01:08:58,680
 is one of the biggest ones.

1259
01:08:58,680 --> 01:09:06,200
 The idea that we could have more common sense, priors,

1260
01:09:06,200 --> 01:09:08,520
 to make decisions with robots, I think,

1261
01:09:08,520 --> 01:09:12,080
 is the biggest change coming to the field in a long time,

1262
01:09:12,080 --> 01:09:12,600
 maybe.

1263
01:09:12,600 --> 01:09:15,800
 And it's starting-- we've always wanted it.

1264
01:09:15,800 --> 01:09:23,600
 And I don't think we're quite there with large language

1265
01:09:23,600 --> 01:09:24,100
 models.

1266
01:09:24,100 --> 01:09:27,080
 But I'd say the large language models

1267
01:09:27,080 --> 01:09:31,640
 and the visual language models and the like that Bojan talked

1268
01:09:31,640 --> 01:09:35,640
 about, that's the first compelling approach

1269
01:09:35,640 --> 01:09:37,720
 to say we're going to get something

1270
01:09:37,720 --> 01:09:43,560
 that smells like an unnatural intelligence common sense.

1271
01:09:43,560 --> 01:09:45,920
 And I don't even know how to measure

1272
01:09:45,920 --> 01:09:47,480
 the potential change in that that's

1273
01:09:47,480 --> 01:09:48,720
 going to happen with that.

1274
01:09:48,720 --> 01:09:51,760
 It's going to be weird.

1275
01:09:51,760 --> 01:09:52,760
 I can guarantee that.

1276
01:09:52,760 --> 01:09:55,160
 That's a high probability prediction, right?

1277
01:09:55,160 --> 01:09:58,040
 But I think it's really one of the biggest things that's

1278
01:09:58,040 --> 01:09:59,460
 going to change what we're doing.

1279
01:09:59,460 --> 01:10:03,840
 It's sort of interesting.

1280
01:10:03,840 --> 01:10:07,200
 The people I talk to about this, they actually

1281
01:10:07,200 --> 01:10:10,360
 say that-- maybe they're just trying to make me feel better.

1282
01:10:10,360 --> 01:10:12,920
 But they say it's interesting because there's

1283
01:10:12,920 --> 01:10:15,440
 so many people are excited about this.

1284
01:10:15,440 --> 01:10:17,800
 And they want to think about how to make robots

1285
01:10:17,800 --> 01:10:20,320
 do these multi-level tasks that, in some ways,

1286
01:10:20,320 --> 01:10:24,680
 it actually puts a premium on motion planning that just works

1287
01:10:24,680 --> 01:10:27,800
 and feedback control and skills and other things.

1288
01:10:27,800 --> 01:10:31,560
 The stuff I did maybe emphasize a lot in the class

1289
01:10:31,560 --> 01:10:36,000
 is suddenly really important because there's

1290
01:10:36,000 --> 01:10:38,600
 engineers everywhere that don't know that yet.

1291
01:10:38,600 --> 01:10:41,680
 And the robots don't work all the time.

1292
01:10:41,680 --> 01:10:46,120
 But if they did, we could do incredible long-term tasks now.

1293
01:10:46,120 --> 01:10:48,520
 So I actually think, in a weird way,

1294
01:10:48,520 --> 01:10:54,440
 this not manipulator equation driven thing

1295
01:10:54,440 --> 01:10:59,800
 is probably going to put a premium on some

1296
01:10:59,800 --> 01:11:03,360
 of the core manipulation skills, let me say.

1297
01:11:03,360 --> 01:11:15,120
 The slightly lower level stuff, including

1298
01:11:15,120 --> 01:11:16,720
 extra style manipulation.

1299
01:11:16,720 --> 01:11:22,560
 I would say that we're going to see--

1300
01:11:22,560 --> 01:11:25,680
 so I obviously like simulation.

1301
01:11:25,680 --> 01:11:29,360
 But I think we've turned a few corners with simulation.

1302
01:11:29,360 --> 01:11:32,520
 And I would expect that the use of simulation is--

1303
01:11:32,520 --> 01:11:34,720
 I think it's just like at the beginning of what we're

1304
01:11:34,720 --> 01:11:36,840
 going to see in this field.

1305
01:11:36,840 --> 01:11:38,840
 And it's going to continue to change rapidly.

1306
01:11:42,400 --> 01:11:45,760
 I think some percentage of the robotics population

1307
01:11:45,760 --> 01:11:48,680
 is converted and says, I believe that if it worked

1308
01:11:48,680 --> 01:11:51,100
 in simulation, I'd have a pretty good chance of it working

1309
01:11:51,100 --> 01:11:51,920
 in reality.

1310
01:11:51,920 --> 01:11:54,200
 The people that are training perception systems

1311
01:11:54,200 --> 01:11:56,960
 on simulated data are pretty convinced, I think.

1312
01:11:56,960 --> 01:11:59,200
 I think less people are convinced about the contact

1313
01:11:59,200 --> 01:12:01,520
 mechanics.

1314
01:12:01,520 --> 01:12:04,800
 I mean, we focused on it more than a lot of people.

1315
01:12:04,800 --> 01:12:08,560
 And there's a sim-to-real gap in the contact mechanics.

1316
01:12:08,560 --> 01:12:12,080
 And you definitely have to be a skilled user of simulation

1317
01:12:12,080 --> 01:12:13,000
 to make that transfer.

1318
01:12:13,000 --> 01:12:15,360
 You could set parameters wrong.

1319
01:12:15,360 --> 01:12:18,760
 But I think if you're a skilled user of simulation,

1320
01:12:18,760 --> 01:12:21,680
 then more and more people believe

1321
01:12:21,680 --> 01:12:24,120
 that you can do your work in simulation.

1322
01:12:24,120 --> 01:12:27,620
 The bottleneck there is content.

1323
01:12:27,620 --> 01:12:30,720
 How do you get your robot, your art assets, your objects

1324
01:12:30,720 --> 01:12:33,480
 that you want to manipulate into simulation?

1325
01:12:33,480 --> 01:12:35,400
 And I think there's going to be just probably

1326
01:12:35,400 --> 01:12:42,040
 a huge change in content.

1327
01:12:42,040 --> 01:12:44,960
 We're already seeing it with-- it's funny.

1328
01:12:44,960 --> 01:12:49,400
 When someone says-- like five years ago, let's say 10 years

1329
01:12:49,400 --> 01:12:54,200
 ago, just to be safe, people said, I built a simulator.

1330
01:12:54,200 --> 01:12:56,280
 They mean they wrote like F equals ma down.

1331
01:12:56,280 --> 01:12:59,400
 And maybe they wrote a renderer that's part of a simulator.

1332
01:12:59,400 --> 01:13:02,080
 But now if someone says, I've got a new simulator,

1333
01:13:02,080 --> 01:13:03,960
 they don't even-- they built something

1334
01:13:03,960 --> 01:13:06,280
 on top of a physics engine.

1335
01:13:06,280 --> 01:13:09,200
 And they don't even cite the physics engine.

1336
01:13:09,200 --> 01:13:14,360
 But they're like now-- there's these, I think,

1337
01:13:14,360 --> 01:13:18,800
 very important content aggregators, people that just

1338
01:13:18,800 --> 01:13:22,320
 say, I've scanned a bunch of houses.

1339
01:13:22,320 --> 01:13:24,040
 And I've put a bunch of different objects

1340
01:13:24,040 --> 01:13:24,800
 in those houses.

1341
01:13:24,800 --> 01:13:26,760
 And that's my new simulator offering.

1342
01:13:26,760 --> 01:13:27,960
 And I think that's valuable.

1343
01:13:27,960 --> 01:13:29,400
 That's hugely valuable.

1344
01:13:29,400 --> 01:13:31,760
 So we're seeing people generate that data

1345
01:13:31,760 --> 01:13:34,760
 in lots of different ways, sometimes with manual effort,

1346
01:13:34,760 --> 01:13:36,800
 sometimes with procedural generation.

1347
01:13:36,800 --> 01:13:40,080
 You can make a program that spits out random living rooms.

1348
01:13:40,080 --> 01:13:41,640
 And increasingly, what we're seeing

1349
01:13:41,640 --> 01:13:44,320
 is real-to-sim kind of work.

1350
01:13:44,320 --> 01:13:50,160
 I think this is just going to be a huge component.

1351
01:13:50,160 --> 01:13:53,280
 The fact that you can drive around Stata

1352
01:13:53,280 --> 01:13:55,120
 with just an RGB camera, come out

1353
01:13:55,120 --> 01:13:59,720
 with a perfect neural radiance field representation of it.

1354
01:13:59,720 --> 01:14:01,440
 And then, so what do you do with that?

1355
01:14:01,440 --> 01:14:04,120
 How do you get that into a simulator?

1356
01:14:04,120 --> 01:14:06,920
 It's not enough, it turns out, to feed the simulator.

1357
01:14:06,920 --> 01:14:08,600
 But people are thinking about this now.

1358
01:14:08,600 --> 01:14:13,080
 How do I just ingest so that the robot, every time it

1359
01:14:13,080 --> 01:14:15,840
 sees something new, it adds it to the simulator.

1360
01:14:15,840 --> 01:14:16,920
 And we build the matrix.

1361
01:14:16,920 --> 01:14:22,440
 I predict that that would be a huge-- that's

1362
01:14:22,440 --> 01:14:25,000
 going to just ramp up more and more and more.

1363
01:14:25,000 --> 01:14:30,680
 I guess along that route, I think--

1364
01:14:30,680 --> 01:14:32,560
 maybe an easy one to say, but let's just

1365
01:14:32,560 --> 01:14:34,120
 think about it for a minute.

1366
01:14:34,120 --> 01:14:36,760
 I think big data hasn't come to robotics yet, but it's coming.

1367
01:14:36,760 --> 01:14:40,920
 It's come through large language models and visual models.

1368
01:14:40,920 --> 01:14:43,920
 But the thing that we're waiting for is, let me say,

1369
01:14:43,920 --> 01:14:51,680
 interaction data, data that has forces.

1370
01:14:51,680 --> 01:14:53,240
 We talked about in the system ID world

1371
01:14:53,240 --> 01:14:55,280
 that if I watched an object fall on YouTube,

1372
01:14:55,280 --> 01:14:57,120
 there's limits to what I can learn about it.

1373
01:14:57,120 --> 01:14:58,920
 I can't learn its math, for instance.

1374
01:14:58,920 --> 01:15:03,080
 And I think we're getting to the world

1375
01:15:03,080 --> 01:15:05,760
 where people are deploying enough robots

1376
01:15:05,760 --> 01:15:09,920
 and thinking seriously about how to aggregate that data.

1377
01:15:09,920 --> 01:15:17,400
 Fleet learning is a huge potential.

1378
01:15:17,400 --> 01:15:21,280
 All the robots as edge nodes pool their understanding,

1379
01:15:21,280 --> 01:15:23,440
 pool their models, pool their data

1380
01:15:23,440 --> 01:15:25,120
 to learn something more about the world

1381
01:15:25,120 --> 01:15:29,600
 than they can learn by surfing the web.

1382
01:15:29,600 --> 01:15:30,480
 And that's coming.

1383
01:15:30,480 --> 01:15:32,440
 But every year we say it's coming,

1384
01:15:32,440 --> 01:15:34,160
 and it's still taking a long time.

1385
01:15:34,160 --> 01:15:36,520
 Considering how important it is, it's taking a long time.

1386
01:15:36,520 --> 01:15:40,880
 It's sort of frustrating that we don't quite have it yet.

1387
01:15:40,880 --> 01:15:42,960
 It's hard to-- because the data that you

1388
01:15:42,960 --> 01:15:45,280
 generate on your robot, that's not exactly the data

1389
01:15:45,280 --> 01:15:47,240
 I want to generate on my robot.

1390
01:15:47,240 --> 01:15:48,880
 And so it's not immediately useful.

1391
01:15:48,880 --> 01:15:51,440
 You have to think about off-policy RL and all these.

1392
01:15:51,440 --> 01:15:56,040
 But even the distribution shift can be really tough.

1393
01:15:56,040 --> 01:15:58,720
 But there's going to be a crossing point where

1394
01:15:58,720 --> 01:16:01,880
 we have enough robots, and they're similar enough,

1395
01:16:01,880 --> 01:16:03,640
 or we have enough copies of the same robot,

1396
01:16:03,640 --> 01:16:06,600
 and maybe we consolidate hardware or something

1397
01:16:06,600 --> 01:16:09,520
 where suddenly I'm going to program my robot completely

1398
01:16:09,520 --> 01:16:13,080
 differently because you generated a lot of data.

1399
01:16:13,080 --> 01:16:14,960
 Also, the same thing, too, is that a lot

1400
01:16:14,960 --> 01:16:16,760
 of the work we're doing here, we're

1401
01:16:16,760 --> 01:16:20,080
 kind of programming the robot as if it's the first time it ever

1402
01:16:20,080 --> 01:16:22,280
 experienced this.

1403
01:16:22,280 --> 01:16:24,400
 And we think a lot about learning as, OK,

1404
01:16:24,400 --> 01:16:28,420
 I started with my policy parameters as random numbers

1405
01:16:28,420 --> 01:16:29,240
 around 0.

1406
01:16:29,240 --> 01:16:30,280
 How do I do that?

1407
01:16:30,280 --> 01:16:32,480
 And that's not the world we're going to be living in.

1408
01:16:32,480 --> 01:16:33,940
 We're going to be living in a world

1409
01:16:33,940 --> 01:16:36,800
 where there are many robots that have already

1410
01:16:36,800 --> 01:16:40,760
 done most of these things, and I should start with their hive

1411
01:16:40,760 --> 01:16:44,800
 mind global model and maybe specialize

1412
01:16:44,800 --> 01:16:47,720
 for my current situation.

1413
01:16:47,720 --> 01:16:52,760
 So that's definitely coming to this neck of the woods, too.

1414
01:16:52,760 --> 01:17:01,120
 And maybe just to say a last one,

1415
01:17:01,120 --> 01:17:03,560
 I think I've said it a few times,

1416
01:17:03,560 --> 01:17:14,080
 but I'm just very optimistic about theory of ML, RL, control

1417
01:17:14,080 --> 01:17:19,000
 coming together with empirical stuff.

1418
01:17:19,000 --> 01:17:24,320
 I think the empirical success of these things raced ahead.

1419
01:17:24,320 --> 01:17:28,040
 But now we have many of the best theorists

1420
01:17:28,040 --> 01:17:31,340
 in the world that are excited about understanding

1421
01:17:31,340 --> 01:17:33,840
 those better, and I think that is just going

1422
01:17:33,840 --> 01:17:35,560
 to be a very harmonious future.

1423
01:17:35,560 --> 01:17:38,520
 I mean, we have Scott Aronson, right,

1424
01:17:38,520 --> 01:17:40,720
 is our quantum computation guy.

1425
01:17:40,720 --> 01:17:43,600
 He saw GPT-3, and now he's open AI for, I think--

1426
01:17:43,600 --> 01:17:45,560
 I hope I'm not wrong, Scott.

1427
01:17:45,560 --> 01:17:47,880
 But the quantum computation people

1428
01:17:47,880 --> 01:17:50,280
 get so excited by these large models

1429
01:17:50,280 --> 01:17:52,280
 that they have to go figure them out.

1430
01:17:52,280 --> 01:17:53,360
 That's good.

1431
01:17:53,360 --> 01:17:54,280
 That's great, right?

1432
01:17:54,280 --> 01:17:57,680
 That's like bringing all the really great people together.

1433
01:17:57,680 --> 01:17:59,040
 And I'm just very--

1434
01:17:59,040 --> 01:18:01,320
 I mean, the controls people are so smart.

1435
01:18:01,320 --> 01:18:03,000
 They're so, so smart.

1436
01:18:03,000 --> 01:18:05,920
 And they now see that some of the things that

1437
01:18:05,920 --> 01:18:10,160
 have happened in RL, and they're moving in that direction.

1438
01:18:10,160 --> 01:18:11,600
 And I'm very optimistic about that

1439
01:18:11,600 --> 01:18:15,000
 and how that changes things.

1440
01:18:15,000 --> 01:18:20,560
 So if I were to just, at a meta level,

1441
01:18:20,560 --> 01:18:23,000
 try to convince you of something,

1442
01:18:23,000 --> 01:18:25,600
 it's maybe-- I think it's in this space, which is--

1443
01:18:25,600 --> 01:18:27,560
 and I said it on day one, and I'll say it again

1444
01:18:27,560 --> 01:18:30,600
 to close this off here.

1445
01:18:30,600 --> 01:18:34,000
 I mean, for me, this class, even, and the notes

1446
01:18:34,000 --> 01:18:38,040
 as they slowly evolve, and the way I think about it,

1447
01:18:38,040 --> 01:18:42,920
 I think because the systems we're building here

1448
01:18:42,920 --> 01:18:48,560
 are so complicated, we have to think rigorously about them.

1449
01:18:48,560 --> 01:18:52,320
 And I think having a foundation of the things we know

1450
01:18:52,320 --> 01:18:54,640
 and rigorous thinking about the things

1451
01:18:54,640 --> 01:18:59,000
 that we're still inventing is just so important.

1452
01:18:59,000 --> 01:19:01,840
 And I think if you talk to the best empirical machine

1453
01:19:01,840 --> 01:19:06,040
 learning people and the most influential papers,

1454
01:19:06,040 --> 01:19:07,840
 and you look at the authors, or you look

1455
01:19:07,840 --> 01:19:14,600
 at the style of the papers, they're extremely rigorous.

1456
01:19:14,600 --> 01:19:16,440
 I think people get the impression

1457
01:19:16,440 --> 01:19:19,400
 that you can put a quick algorithm together,

1458
01:19:19,400 --> 01:19:21,360
 you can make some curves, and you're good.

1459
01:19:21,360 --> 01:19:24,460
 But those aren't the papers that are having massive impact.

1460
01:19:24,460 --> 01:19:28,560
 And so I really want us to take the time

1461
01:19:28,560 --> 01:19:30,960
 to think deeply about these problems

1462
01:19:30,960 --> 01:19:36,640
 and build a foundation across these complicated disciplines.

1463
01:19:36,640 --> 01:19:39,760
 And I think that's what's going to push the field forward.

1464
01:19:39,760 --> 01:19:43,760
 Maybe more now than in some other times.

1465
01:19:43,760 --> 01:19:46,680
 There's just been such a bubbling up of ideas.

1466
01:19:46,680 --> 01:19:48,720
 And it feels to me like it's time

1467
01:19:48,720 --> 01:19:51,600
 to consolidate a little bit and then push forward again.

1468
01:19:51,600 --> 01:19:55,480
 Good.

1469
01:19:55,480 --> 01:19:59,280
 OK, so that's it for me.

1470
01:19:59,280 --> 01:20:00,520
 It's your turn.

1471
01:20:00,520 --> 01:20:05,200
 So Anthony sent out the logistics for Tuesday.

1472
01:20:05,200 --> 01:20:08,480
 But basically, I think-- and his text

1473
01:20:08,480 --> 01:20:10,800
 is the gospel, if I say anything different right now.

1474
01:20:10,800 --> 01:20:12,840
 But the basic gist is please come.

1475
01:20:12,840 --> 01:20:15,480
 Come at 2, because it's going to take longer than an hour

1476
01:20:15,480 --> 01:20:16,800
 and a half to do it.

1477
01:20:16,800 --> 01:20:18,320
 If you come at 2.30, that's fine.

1478
01:20:18,320 --> 01:20:20,400
 But if you can come at 2, it's great.

1479
01:20:20,400 --> 01:20:23,240
 It really is like the best part of the class.

1480
01:20:23,240 --> 01:20:25,000
 And please, when you're presenting or making

1481
01:20:25,000 --> 01:20:28,440
 your videos, think about what you

1482
01:20:28,440 --> 01:20:30,840
 learned that you wish other people knew.

1483
01:20:30,840 --> 01:20:31,840
 That's the value.

1484
01:20:31,840 --> 01:20:35,480
 I get tons of value out of that, of learning about the things

1485
01:20:35,480 --> 01:20:39,720
 that you thought were going to work that didn't work.

1486
01:20:39,720 --> 01:20:41,640
 Algorithm you tried that we haven't covered

1487
01:20:41,640 --> 01:20:43,180
 or I haven't thought about that much.

1488
01:20:43,180 --> 01:20:44,800
 I hear your experiences.

1489
01:20:44,800 --> 01:20:47,200
 And I understand things better because of them.

1490
01:20:47,200 --> 01:20:49,160
 And so I think you can all get that out

1491
01:20:49,160 --> 01:20:53,600
 of each other next week.

1492
01:20:53,600 --> 01:20:55,880
 The goal is-- so once you put your name on the sheet,

1493
01:20:55,880 --> 01:20:57,380
 we're going to march down the sheet.

1494
01:20:57,380 --> 01:21:00,160
 People in the room will put up first.

1495
01:21:00,160 --> 01:21:02,080
 We've had a few times where someone would sit

1496
01:21:02,080 --> 01:21:03,680
 in the room for a really long time watching

1497
01:21:03,680 --> 01:21:04,720
 people who aren't there.

1498
01:21:04,720 --> 01:21:06,840
 And so if you're here in the room,

1499
01:21:06,840 --> 01:21:10,880
 and you've marked your video as public,

1500
01:21:10,880 --> 01:21:12,440
 so when you upload to YouTube, you

1501
01:21:12,440 --> 01:21:14,400
 can make it public or unlisted.

1502
01:21:14,400 --> 01:21:16,400
 The public videos means we're going to show it.

1503
01:21:16,400 --> 01:21:18,640
 And we'll show it even on the live stream,

1504
01:21:18,640 --> 01:21:21,400
 because some people will watch remotely.

1505
01:21:21,400 --> 01:21:23,400
 If it's unlisted, it's still on the spreadsheet.

1506
01:21:23,400 --> 01:21:24,960
 So you can watch everybody's videos.

1507
01:21:24,960 --> 01:21:29,540
 I mean, the class-- your members of the class

1508
01:21:29,540 --> 01:21:34,060
 are your audience, whether you're unlisted or public.

1509
01:21:34,060 --> 01:21:36,180
 But the broader world is only your audience

1510
01:21:36,180 --> 01:21:39,420
 if you mark your video public.

1511
01:21:39,420 --> 01:21:43,020
 And I've got a room till 5.

1512
01:21:43,020 --> 01:21:43,940
 We'll see what we do.

1513
01:21:43,940 --> 01:21:45,860
 We're going to march through as many as we can.

1514
01:21:45,860 --> 01:21:47,700
 Try to give a little space for questions.

1515
01:21:47,700 --> 01:21:49,280
 There's a lot of you, and it takes time

1516
01:21:49,280 --> 01:21:51,060
 to march through the videos.

1517
01:21:51,060 --> 01:21:52,940
 But please come.

1518
01:21:52,940 --> 01:21:54,780
 It's really, really a fun part of the class.

1519
01:21:54,780 --> 01:21:56,840
 I know there's people all over that

1520
01:21:56,840 --> 01:21:59,920
 are going to be watching, because they've

1521
01:21:59,920 --> 01:22:02,400
 seen awesome projects in the past.

1522
01:22:02,400 --> 01:22:05,760
 And I think they're going to see awesome projects this time.

1523
01:22:05,760 --> 01:22:08,240
 And it's not about how well your robot works.

1524
01:22:08,240 --> 01:22:10,200
 It's about how much you learned and how

1525
01:22:10,200 --> 01:22:13,080
 you can communicate that.

1526
01:22:13,080 --> 01:22:13,920
 OK, good.

1527
01:22:13,920 --> 01:22:15,040
 See you Tuesday.

1528
01:22:15,040 --> 01:22:16,760
 I'm excited.

