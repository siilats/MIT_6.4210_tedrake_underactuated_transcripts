1
00:00:00,000 --> 00:00:03,440
 - Hey, thank you guys for coming early.

2
00:00:03,440 --> 00:00:05,660
 I'm gonna start pretty much right off

3
00:00:05,660 --> 00:00:07,920
 because we have a lot of you

4
00:00:07,920 --> 00:00:10,340
 and a lot of good projects to get through.

5
00:00:10,340 --> 00:00:13,040
 If you, just one logistical note.

6
00:00:13,040 --> 00:00:17,020
 I hoped it was clear.

7
00:00:17,020 --> 00:00:19,600
 We said if your video was public,

8
00:00:19,600 --> 00:00:21,280
 that's in the queue right now.

9
00:00:21,280 --> 00:00:22,560
 If it's unlisted, that means you don't want me

10
00:00:22,560 --> 00:00:23,460
 to show it today.

11
00:00:23,460 --> 00:00:24,800
 I think a bunch of people,

12
00:00:24,800 --> 00:00:28,680
 based on emails you've sent in the last 20 minutes,

13
00:00:29,560 --> 00:00:31,320
 might have marked your video unlisted

14
00:00:31,320 --> 00:00:32,640
 and expected it to be shown today.

15
00:00:32,640 --> 00:00:34,280
 By default, that's not happening.

16
00:00:34,280 --> 00:00:37,280
 You can talk to Bojan and we can see if we can figure it out.

17
00:00:37,280 --> 00:00:43,560
 I have queued up all of the videos marked public

18
00:00:43,560 --> 00:00:46,120
 and we're gonna play them.

19
00:00:46,120 --> 00:00:48,320
 Because we have so many of them,

20
00:00:48,320 --> 00:00:49,800
 and it's kind of nice actually,

21
00:00:49,800 --> 00:00:53,540
 I can just tell YouTube to play it exactly the allotted time

22
00:00:53,540 --> 00:00:55,120
 and it stops.

23
00:00:55,120 --> 00:00:56,040
 And every once in a while,

24
00:00:56,040 --> 00:00:58,640
 there's gonna be one that's about to show the results

25
00:00:58,640 --> 00:01:00,120
 and then it stops.

26
00:01:00,120 --> 00:01:01,680
 But I'm not a bad guy.

27
00:01:01,680 --> 00:01:05,240
 I just automatically, I programmed the embedded URLs

28
00:01:05,240 --> 00:01:06,740
 to do this, okay?

29
00:01:06,740 --> 00:01:12,680
 Okay, so thank you guys for all the hard work.

30
00:01:12,680 --> 00:01:15,380
 And without further ado, let's start.

31
00:01:15,380 --> 00:01:21,760
 Oh, we have to figure out, yeah.

32
00:01:21,760 --> 00:01:26,080
 You know what?

33
00:01:26,080 --> 00:01:29,840
 How am I gonna get you to hear the audio well up there?

34
00:01:29,840 --> 00:01:32,740
 That's something we haven't worried about too much before.

35
00:01:32,740 --> 00:01:43,320
 I don't think this is actually making it louder

36
00:01:43,320 --> 00:01:44,480
 up in the room, is it?

37
00:01:44,480 --> 00:01:53,280
 Oh.

38
00:01:54,680 --> 00:01:59,040
 This is not exactly the high tech I was hoping for,

39
00:01:59,040 --> 00:02:01,120
 but I'm gonna put the mic kind of close to the computer.

40
00:02:01,120 --> 00:02:02,320
 We'll see how that goes.

41
00:02:02,320 --> 00:02:04,940
 You think there's one here?

42
00:02:04,940 --> 00:02:11,360
 That doesn't look promising, it's on the PC.

43
00:02:11,360 --> 00:02:16,320
 Audio.

44
00:02:16,320 --> 00:02:22,440
 - If you go into the system settings,

45
00:02:22,440 --> 00:02:24,520
 you'll notice that you get the option to output

46
00:02:24,520 --> 00:02:26,680
 the (mumbles)

47
00:02:26,680 --> 00:02:30,940
 (audience member speaking faintly)

48
00:02:30,940 --> 00:02:47,480
 - As well as the limitations of the robot

49
00:02:47,480 --> 00:02:49,840
 to achieve the perpetual juggling motion.

50
00:02:49,840 --> 00:02:52,240
 With some additional work, we could even load this program

51
00:02:52,240 --> 00:02:55,000
 onto a real Evo robot and have it juggle in real life.

52
00:02:55,000 --> 00:02:56,360
 - Nice.

53
00:02:56,360 --> 00:02:58,200
 - To start, we need to choose the point

54
00:02:58,200 --> 00:03:00,360
 at which we're going to throw and catch.

55
00:03:00,360 --> 00:03:03,360
 To start, we need to choose the point

56
00:03:03,360 --> 00:03:13,080
 at which we're going to throw and catch.

57
00:03:13,080 --> 00:03:14,840
 The person that juggles.

58
00:03:14,840 --> 00:03:18,600
 We modeled everything in the simulator with physics,

59
00:03:18,600 --> 00:03:21,300
 like gravity and friction, as well as the limitations

60
00:03:21,300 --> 00:03:24,360
 of the robot to achieve the perpetual juggling motion.

61
00:03:24,360 --> 00:03:26,760
 With some additional work, we could even load this program

62
00:03:26,760 --> 00:03:29,560
 onto a real Evo robot and have it juggle in real life.

63
00:03:29,560 --> 00:03:32,720
 To start, we need to choose a point

64
00:03:32,720 --> 00:03:35,140
 at which we're going to throw and catch.

65
00:03:35,140 --> 00:03:37,440
 The approach we use to find such two positions

66
00:03:37,440 --> 00:03:39,040
 is forward kinematics.

67
00:03:39,040 --> 00:03:42,360
 We first decide on a set of comfortable joint angles of Evo

68
00:03:42,360 --> 00:03:44,400
 and then calculate the corresponding spatial position

69
00:03:44,400 --> 00:03:45,800
 of the gripper.

70
00:03:45,800 --> 00:03:48,080
 So now that we have our throw and catch positions

71
00:03:48,080 --> 00:03:50,320
 and their comfortable joint configurations,

72
00:03:50,320 --> 00:03:52,240
 we'll use a formula of projectile motions

73
00:03:52,240 --> 00:03:54,420
 to calculate the start and end velocity,

74
00:03:54,420 --> 00:03:56,980
 given the max height of the trajectory.

75
00:03:56,980 --> 00:03:59,740
 We also want to get corresponding joint velocities.

76
00:03:59,740 --> 00:04:01,140
 We first calculate Jacobian matrix

77
00:04:01,140 --> 00:04:04,600
 for translation of velocities at the joint configuration.

78
00:04:04,600 --> 00:04:06,780
 By multiplying the pseudo-inverse of Jacobian

79
00:04:06,780 --> 00:04:09,600
 with the spatial velocities, we're able to obtain

80
00:04:09,600 --> 00:04:12,460
 the joint velocity for both catch and throw.

81
00:04:12,460 --> 00:04:14,220
 And now it's time for the cool part.

82
00:04:14,220 --> 00:04:16,680
 How do we get our robot to go from being here

83
00:04:16,680 --> 00:04:19,280
 with this velocity to here with this velocity

84
00:04:19,280 --> 00:04:22,960
 at exactly 0.639 seconds later?

85
00:04:22,960 --> 00:04:25,600
 We can frame this as an optimization problem.

86
00:04:25,600 --> 00:04:27,800
 We want to minimize the length of the path,

87
00:04:27,800 --> 00:04:30,320
 subject to the following constraints.

88
00:04:30,320 --> 00:04:32,940
 First, we constrain the positions and velocities

89
00:04:32,940 --> 00:04:34,760
 at the throw and catch points.

90
00:04:34,760 --> 00:04:37,520
 Second, the entire duration of the arm trajectory

91
00:04:37,520 --> 00:04:40,780
 needs to be exactly the duration the ball is in the air for.

92
00:04:40,780 --> 00:04:43,520
 And third, we ensure that the joints on the Evo robot

93
00:04:43,520 --> 00:04:45,560
 don't exceed their position, velocity,

94
00:04:45,560 --> 00:04:47,400
 or acceleration limits.

95
00:04:47,400 --> 00:04:49,520
 At this point, we've done a lot of the groundwork.

96
00:04:49,520 --> 00:04:50,360
 We're super excited.

97
00:04:50,360 --> 00:04:52,720
 We put it on our robot, and this is what we see.

98
00:04:52,720 --> 00:04:53,960
 Everything is going great.

99
00:04:53,960 --> 00:04:56,460
 Then it lets go of the ball at completely the wrong position

100
00:04:56,460 --> 00:04:57,880
 with the wrong velocity.

101
00:04:57,880 --> 00:05:00,240
 Graphing the desired positions and velocities,

102
00:05:00,240 --> 00:05:02,760
 as well as the measured positions and velocities,

103
00:05:02,760 --> 00:05:06,000
 we see that the measured values lag behind the desired values

104
00:05:06,000 --> 00:05:09,680
 by 0.0538 seconds.

105
00:05:09,680 --> 00:05:12,120
 Originally, we were sending our desired positions

106
00:05:12,120 --> 00:05:14,600
 to a system that interpolates the desired state

107
00:05:14,600 --> 00:05:16,560
 with discrete derivatives.

108
00:05:16,560 --> 00:05:19,000
 The desired state is then fed into the inverse dynamics

109
00:05:19,000 --> 00:05:20,920
 controller, which spits out the force

110
00:05:20,920 --> 00:05:22,600
 commands to center the arm.

111
00:05:22,600 --> 00:05:25,360
 Note that this system doesn't feed desired acceleration

112
00:05:25,360 --> 00:05:28,600
 to the inverse dynamics controller at all.

113
00:05:28,600 --> 00:05:31,520
 To improve tracking, we remove the state interpolator

114
00:05:31,520 --> 00:05:34,840
 and took analytical derivatives on our position trajectory

115
00:05:34,840 --> 00:05:37,340
 to get our desired velocities and accelerations

116
00:05:37,340 --> 00:05:41,580
 and fed those into the inverse dynamics controller directly.

117
00:05:41,580 --> 00:05:43,280
 We can see that with these changes,

118
00:05:43,280 --> 00:05:46,000
 the measured values now converge to the desired values.

119
00:05:46,000 --> 00:05:51,280
 We put our new controller on a robot,

120
00:05:51,280 --> 00:05:54,400
 but somehow it is still not able to catch the ball.

121
00:05:54,400 --> 00:05:55,960
 Gripper is trying to catch the ball

122
00:05:55,960 --> 00:05:58,240
 where we thought it was going to be instead

123
00:05:58,240 --> 00:06:00,600
 of where it actually is.

124
00:06:00,600 --> 00:06:02,880
 To account for this, we need to change our planner

125
00:06:02,880 --> 00:06:04,480
 so that it updates the trajectory based

126
00:06:04,480 --> 00:06:07,060
 on the actual position and velocity of throwing right

127
00:06:07,060 --> 00:06:08,640
 after releasing the ball.

128
00:06:08,640 --> 00:06:11,360
 We put this on a robot, and after some debugging,

129
00:06:11,360 --> 00:06:12,280
 finally it works.

130
00:06:12,280 --> 00:06:16,240
 [CHEERING]

131
00:06:16,240 --> 00:06:17,960
 We tried a bunch of different heights

132
00:06:17,960 --> 00:06:20,640
 and found that it can juggle up to two meters high.

133
00:06:20,640 --> 00:06:22,800
 To make the jump to juggling multiple balls,

134
00:06:22,800 --> 00:06:24,600
 we had to modify our planner.

135
00:06:24,600 --> 00:06:27,240
 When trying to juggle two balls, after throwing the first ball,

136
00:06:27,240 --> 00:06:29,680
 you have to catch and throw the second ball all in the time

137
00:06:29,680 --> 00:06:31,440
 that the first ball is in the air.

138
00:06:31,440 --> 00:06:34,020
 So we have much less time to move between the throw and catch

139
00:06:34,020 --> 00:06:35,080
 positions.

140
00:06:35,080 --> 00:06:37,560
 We can keep most of our planner exactly the same,

141
00:06:37,560 --> 00:06:39,560
 but after a ball is thrown, instead

142
00:06:39,560 --> 00:06:42,040
 of replanning the trajectory based on the ball you just

143
00:06:42,040 --> 00:06:44,720
 threw, we update it with the position and velocity

144
00:06:44,720 --> 00:06:46,200
 of the ball that we want to catch.

145
00:06:46,200 --> 00:06:47,540
 And this is what it looks like.

146
00:06:47,540 --> 00:06:53,080
 One thing we had to do was to warm start the arm

147
00:06:53,080 --> 00:06:55,560
 with the desired joint velocities at time 0,

148
00:06:55,560 --> 00:06:57,680
 because unlike in the one ball case,

149
00:06:57,680 --> 00:06:59,920
 it didn't have enough time for the actual velocities

150
00:06:59,920 --> 00:07:01,940
 to converge on the desired velocities

151
00:07:01,940 --> 00:07:05,200
 before it had to throw the ball.

152
00:07:05,200 --> 00:07:07,040
 Next, we tried juggling three balls,

153
00:07:07,040 --> 00:07:09,280
 but kept running into the problem of the balls colliding

154
00:07:09,280 --> 00:07:11,120
 in the air with each other, because now there

155
00:07:11,120 --> 00:07:12,880
 are two balls in the air at the same time

156
00:07:12,880 --> 00:07:15,240
 and because of the slight inaccuracies in the throw.

157
00:07:15,240 --> 00:07:16,900
 We were about to give up when we decided

158
00:07:16,900 --> 00:07:20,160
 to try making the balls smaller, and it worked.

159
00:07:20,160 --> 00:07:22,320
 On one hand, this feels a bit like cheating,

160
00:07:22,320 --> 00:07:23,860
 but on the other hand, it's also cool

161
00:07:23,860 --> 00:07:25,520
 because our controller is precise enough

162
00:07:25,520 --> 00:07:27,160
 to juggle something so small.

163
00:07:27,160 --> 00:07:29,120
 This was the limit, though.

164
00:07:29,120 --> 00:07:30,920
 The kinematic trajectory optimization

165
00:07:30,920 --> 00:07:32,880
 kept failing when we tried to juggle four balls.

166
00:07:37,680 --> 00:07:40,080
 [MUSIC PLAYING]

167
00:07:40,080 --> 00:07:44,840
 And finally, to put it on the real robot,

168
00:07:44,840 --> 00:07:46,520
 we need a perception system.

169
00:07:46,520 --> 00:07:48,920
 In the simulator, we can always know the exact position

170
00:07:48,920 --> 00:07:51,680
 and velocity of everything, but in the real world,

171
00:07:51,680 --> 00:07:54,000
 we'll need to find a way to measure that.

172
00:07:54,000 --> 00:07:56,680
 We simulated three Intel RealSense depth cameras,

173
00:07:56,680 --> 00:07:59,560
 where we plan to combine the point cloud from each camera,

174
00:07:59,560 --> 00:08:02,560
 then subtract the gripper from the point cloud.

175
00:08:02,560 --> 00:08:05,040
 By best fitting a sphere to the leftover point cloud,

176
00:08:05,040 --> 00:08:07,520
 we will be able to estimate the center position of the ball

177
00:08:07,520 --> 00:08:09,880
 from the fitted sphere, where we would then

178
00:08:09,880 --> 00:08:13,120
 be able to feed the position to the kinematic--

179
00:08:13,120 --> 00:08:15,600
 [AUDIO OUT]

180
00:08:15,600 --> 00:08:39,920
 That's pretty good.

181
00:08:39,920 --> 00:08:41,560
 I think we talked about it a little bit

182
00:08:41,560 --> 00:08:43,060
 in the manipulator control lecture,

183
00:08:43,060 --> 00:08:45,840
 but that's a beautiful way to get nice tracking.

184
00:08:45,840 --> 00:08:49,080
 Very nicely done.

185
00:08:49,080 --> 00:08:50,440
 OK, let's keep going.

186
00:08:50,440 --> 00:08:53,120
 I do have a schedule, an optimistic schedule

187
00:08:53,120 --> 00:08:55,560
 on the spreadsheet of about when we think

188
00:08:55,560 --> 00:08:57,080
 videos are going to come up.

189
00:08:57,080 --> 00:08:58,280
 We'll see how it goes.

190
00:08:58,280 --> 00:09:01,520
 Stage of extracting the point clouds from the cameras,

191
00:09:01,520 --> 00:09:04,200
 but haven't yet been able to subtract the arm or fit

192
00:09:04,200 --> 00:09:05,560
 the sphere.

193
00:09:05,560 --> 00:09:06,280
 Hey, everyone.

194
00:09:06,280 --> 00:09:08,560
 Our project is titled Implicit Neural Representations

195
00:09:08,560 --> 00:09:09,880
 for Deformable Objects.

196
00:09:09,880 --> 00:09:11,880
 Our fundamental aim has been trying to find a smarter way

197
00:09:11,880 --> 00:09:13,160
 to model deformable objects.

198
00:09:13,160 --> 00:09:14,700
 Ultimately, we could use these models

199
00:09:14,700 --> 00:09:17,000
 to do downstream tasks like keypoint estimation.

200
00:09:17,000 --> 00:09:19,240
 Keypoint estimation is difficult on deformable objects

201
00:09:19,240 --> 00:09:21,880
 due to the many different deformation configurations.

202
00:09:21,880 --> 00:09:24,820
 As we will show, generalized implicit neural representations

203
00:09:24,820 --> 00:09:26,880
 are naturally invariant to object deformations,

204
00:09:26,880 --> 00:09:29,600
 since they rely on topological features of the object.

205
00:09:29,600 --> 00:09:31,680
 What are implicit neural representations?

206
00:09:31,680 --> 00:09:33,900
 INRs are functions that parametrize conventionally

207
00:09:33,900 --> 00:09:35,780
 discrete signals, like the pixels of an image

208
00:09:35,780 --> 00:09:38,040
 are points in a point cloud, and learn a continuous function

209
00:09:38,040 --> 00:09:39,580
 that maps the domain of the signal.

210
00:09:39,580 --> 00:09:40,960
 These functions are not dependent

211
00:09:40,960 --> 00:09:42,960
 on the spatial resolution of the original signal,

212
00:09:42,960 --> 00:09:45,360
 unlike standard ML models, which provide an estimate

213
00:09:45,360 --> 00:09:48,360
 of each pixel within an image.

214
00:09:48,360 --> 00:09:51,160
 The caveat is that most INRs are trained on data residing

215
00:09:51,160 --> 00:09:52,480
 strictly in Euclidean space.

216
00:09:52,480 --> 00:09:54,120
 So even though INRs learn functions

217
00:09:54,120 --> 00:09:55,960
 that aren't dependent on spatial resolution,

218
00:09:55,960 --> 00:09:57,960
 they're still tied to some Euclidean space.

219
00:09:57,960 --> 00:09:59,520
 Modeling deformable objects means

220
00:09:59,520 --> 00:10:02,480
 we can't be tied to this kind of space.

221
00:10:02,480 --> 00:10:05,120
 Generalized INRs can be thought of as applying INRs

222
00:10:05,120 --> 00:10:06,400
 in non-Euclidean domains.

223
00:10:06,400 --> 00:10:08,680
 Given the assumption that a continuous signal exists

224
00:10:08,680 --> 00:10:10,440
 on some unknown topological space,

225
00:10:10,440 --> 00:10:12,480
 we can sample a discrete graph from the space.

226
00:10:12,480 --> 00:10:13,920
 We then compute a spectral embedding

227
00:10:13,920 --> 00:10:15,120
 for each node in the graph.

228
00:10:15,120 --> 00:10:17,280
 This embedding allows us to approximate each node's

229
00:10:17,280 --> 00:10:19,240
 location, since there's no coordinate system.

230
00:10:19,240 --> 00:10:20,780
 We can then use a neural net to learn

231
00:10:20,780 --> 00:10:23,120
 the mappings between the spectral embedding of each node

232
00:10:23,120 --> 00:10:24,760
 to some predicted signal.

233
00:10:24,760 --> 00:10:27,040
 In our case, this part is a classification problem

234
00:10:27,040 --> 00:10:31,280
 where we wish to output one of n labels per node.

235
00:10:31,280 --> 00:10:33,880
 Here are the first three components of the computed GINR

236
00:10:33,880 --> 00:10:35,480
 embeddings for the Stanford bunny.

237
00:10:35,480 --> 00:10:37,280
 We see that the components vary smoothly

238
00:10:37,280 --> 00:10:38,360
 over the bunny's surface.

239
00:10:38,360 --> 00:10:42,600
 Because GINR embeddings only depend

240
00:10:42,600 --> 00:10:44,040
 on the topology of an object, they

241
00:10:44,040 --> 00:10:45,880
 are independent of a coordinate frame.

242
00:10:45,880 --> 00:10:48,120
 Thus, we can use them for learning representations

243
00:10:48,120 --> 00:10:49,920
 of deformable objects.

244
00:10:49,920 --> 00:10:52,840
 Here, we investigate GINRs for key point prediction.

245
00:10:52,840 --> 00:10:57,480
 Here is a Stanford bunny with a few key points

246
00:10:57,480 --> 00:11:00,760
 labeled corresponding to the right ear, the right eye,

247
00:11:00,760 --> 00:11:03,800
 and the tail.

248
00:11:03,800 --> 00:11:06,480
 Our project seeks to answer the following key questions.

249
00:11:06,480 --> 00:11:09,020
 Are GINR embeddings informative enough for key point prediction

250
00:11:09,020 --> 00:11:09,960
 on objects?

251
00:11:09,960 --> 00:11:12,840
 Can they generalize to unseen deformations of an object?

252
00:11:12,840 --> 00:11:15,960
 And finally, can they generalize to undersampled/oversampled

253
00:11:15,960 --> 00:11:18,440
 mesh representations?

254
00:11:18,440 --> 00:11:21,320
 Our experimental setup consists of manually labeling key points

255
00:11:21,320 --> 00:11:24,280
 on an undeformed or canonical object of interest.

256
00:11:24,280 --> 00:11:25,920
 Here, we choose the Stanford bunny

257
00:11:25,920 --> 00:11:28,440
 with five classes of key points.

258
00:11:28,440 --> 00:11:30,700
 Note that the number of labeled key points

259
00:11:30,700 --> 00:11:33,560
 is much lesser than the number of total points in the mesh.

260
00:11:36,400 --> 00:11:38,640
 We create deformations of the canonical bunny

261
00:11:38,640 --> 00:11:41,840
 by simulating impact with the ground and manual deformation.

262
00:11:41,840 --> 00:11:47,360
 Here is an animation showing the impact simulation

263
00:11:47,360 --> 00:11:50,440
 of the bunny with the ground.

264
00:11:50,440 --> 00:11:52,200
 To perform the simulation, we first

265
00:11:52,200 --> 00:11:55,560
 deteralized 2D surface mesh to get a 3D volume mesh.

266
00:11:55,560 --> 00:11:57,400
 And we then simulated it in PyDrake

267
00:11:57,400 --> 00:11:58,960
 by dropping the bunny from a height

268
00:11:58,960 --> 00:12:00,660
 and recording the positions after impact.

269
00:12:00,660 --> 00:12:06,080
 Once we have these deformations, we

270
00:12:06,080 --> 00:12:07,960
 consider additional modifications,

271
00:12:07,960 --> 00:12:10,440
 such as random addition of new points and edges,

272
00:12:10,440 --> 00:12:14,760
 oversampling, or undersampling the resultant mesh.

273
00:12:14,760 --> 00:12:17,760
 All of these modifications affect the underlying topology

274
00:12:17,760 --> 00:12:19,020
 of the object.

275
00:12:19,020 --> 00:12:21,320
 Next, we will present the results from our experiments.

276
00:12:21,320 --> 00:12:22,860
 Here, we show an example of the training data

277
00:12:22,860 --> 00:12:24,860
 for the canonical bunny mesh with labeled points

278
00:12:24,860 --> 00:12:26,280
 colored by their class.

279
00:12:26,280 --> 00:12:28,640
 On the bottom, we show an example deformation

280
00:12:28,640 --> 00:12:30,680
 of the bunny with predicted classes.

281
00:12:30,680 --> 00:12:33,200
 Notably, we found our model struggled with false positives

282
00:12:33,200 --> 00:12:34,580
 but was able to correctly classify

283
00:12:34,580 --> 00:12:35,800
 all the labeled points.

284
00:12:35,800 --> 00:12:37,920
 We also observed that once our model performed well

285
00:12:37,920 --> 00:12:40,200
 on the training example, it achieved identical results

286
00:12:40,200 --> 00:12:42,440
 on the deformed example, since their mesh structure

287
00:12:42,440 --> 00:12:45,360
 was the same, which demonstrates the utility of GINR's

288
00:12:45,360 --> 00:12:47,720
 inherent invariance to coordinate changes.

289
00:12:47,720 --> 00:12:49,520
 Additionally, we evaluated how our model

290
00:12:49,520 --> 00:12:52,480
 was able to classify key points when the meshes were modified.

291
00:12:52,480 --> 00:12:54,880
 We perturbed the meshes by randomly adding 10% new

292
00:12:54,880 --> 00:12:56,280
 vertices to the mesh.

293
00:12:56,280 --> 00:12:58,680
 We trained our GINR using 15 sample meshes

294
00:12:58,680 --> 00:13:00,120
 and evaluated it on five.

295
00:13:00,120 --> 00:13:02,840
 We saw that the GINRs did not require many examples

296
00:13:02,840 --> 00:13:04,600
 to generalize well to perturb meshes,

297
00:13:04,600 --> 00:13:06,080
 but having a smaller embedding size

298
00:13:06,080 --> 00:13:08,000
 acted like a regularizer for our model.

299
00:13:08,000 --> 00:13:09,680
 We also tested the ability of our model

300
00:13:09,680 --> 00:13:12,160
 to generalize to oversampled and undersampled meshes.

301
00:13:12,160 --> 00:13:14,800
 We trained our model on a single canonical example

302
00:13:14,800 --> 00:13:17,120
 and observed that it was relatively robust to uniform

303
00:13:17,120 --> 00:13:19,440
 oversampling, but significantly underperformed

304
00:13:19,440 --> 00:13:20,920
 on undersampled meshes.

305
00:13:20,920 --> 00:13:22,640
 We hypothesized that the undersampling

306
00:13:22,640 --> 00:13:24,720
 has a greater influence on the spectral embeddings,

307
00:13:24,720 --> 00:13:27,400
 leading to this erroneous behavior.

308
00:13:27,400 --> 00:13:29,640
 Summary, we show that GINRs are useful for learning

309
00:13:29,640 --> 00:13:32,280
 implicit representations of deformed objects

310
00:13:32,280 --> 00:13:34,040
 due to their natural invariance properties.

311
00:13:34,040 --> 00:13:36,680
 We see that they are able to generalize to some variations,

312
00:13:36,680 --> 00:13:38,480
 but have improved robustness when trained

313
00:13:38,480 --> 00:13:40,920
 on various mesh perturbations.

314
00:13:40,920 --> 00:13:42,960
 In the future, we would like to extend this study

315
00:13:42,960 --> 00:13:44,600
 by investigating how our model performs

316
00:13:44,600 --> 00:13:47,120
 on meshes which are inferred from unstructured point cloud

317
00:13:47,120 --> 00:13:47,720
 data.

318
00:13:47,720 --> 00:13:49,840
 We foresee that these meshes will contain a greater

319
00:13:49,840 --> 00:13:51,440
 degree of variability and would like

320
00:13:51,440 --> 00:13:54,440
 to see how GINRs are able to perform under more

321
00:13:54,440 --> 00:13:56,040
 naturalistic conditions.

322
00:13:56,040 --> 00:13:57,480
 Lastly, we are interested in how we

323
00:13:57,480 --> 00:13:59,920
 were able to use the topological structure of the mesh

324
00:13:59,920 --> 00:14:02,840
 to register known object shapes onto partially observed data.

325
00:14:02,840 --> 00:14:13,840
 Maybe I'll just ask a basic one.

326
00:14:13,840 --> 00:14:16,920
 Instead of volumetric, the spectral decomposition

327
00:14:16,920 --> 00:14:18,960
 is parametrizing the surface of the mesh.

328
00:14:18,960 --> 00:14:19,920
 Is that right?

329
00:14:19,920 --> 00:14:24,360
 So how would it behave under a large deformation mode?

330
00:14:24,360 --> 00:14:27,800
 Is that decomposition robust to really large changes

331
00:14:27,800 --> 00:14:29,880
 if you really squish the bunny?

332
00:14:29,880 --> 00:14:32,080
 Yeah, but in our problem, we have

333
00:14:32,080 --> 00:14:34,280
 a view that we have a mesh structure of things.

334
00:14:34,280 --> 00:14:34,780
 Yeah.

335
00:14:34,780 --> 00:14:36,200
 Otherwise, we have a model that's

336
00:14:36,200 --> 00:14:38,200
 like the surface is a structure of objects.

337
00:14:38,200 --> 00:14:38,520
 I see.

338
00:14:38,520 --> 00:14:40,120
 So the vertices are going to get you--

339
00:14:40,120 --> 00:14:42,120
 the fact that you know where the vertices have deformed to

340
00:14:42,120 --> 00:14:43,400
 already tells you the topology.

341
00:14:43,400 --> 00:14:44,900
 It doesn't depend on the connection.

342
00:14:44,900 --> 00:14:46,600
 It doesn't depend on the number of points

343
00:14:46,600 --> 00:14:47,700
 or the actual coordinates.

344
00:14:47,700 --> 00:14:50,240
 You can actually find it by 10 times.

345
00:14:50,240 --> 00:14:50,800
 Awesome.

346
00:14:50,800 --> 00:14:51,300
 Thank you.

347
00:14:51,300 --> 00:14:57,160
 I'm leaving a little space for questions

348
00:14:57,160 --> 00:15:00,360
 if anybody has any.

349
00:15:00,360 --> 00:15:01,720
 OK, we have to keep going.

350
00:15:01,720 --> 00:15:02,280
 That's great.

351
00:15:02,280 --> 00:15:04,920
 Thank you.

352
00:15:04,920 --> 00:15:08,280
 An essential aspect of human life is social interaction.

353
00:15:08,280 --> 00:15:10,920
 In many cultures, it is common to interact with people

354
00:15:10,920 --> 00:15:14,520
 physically via interactions such as high fives and fist bumps.

355
00:15:14,520 --> 00:15:17,600
 As robots are becoming more integrated with human lives,

356
00:15:17,600 --> 00:15:19,180
 there's an increased need for robots

357
00:15:19,180 --> 00:15:20,600
 to become more human-like.

358
00:15:20,600 --> 00:15:23,320
 That is, for humans to feel safer around robots

359
00:15:23,320 --> 00:15:25,960
 and see robots more as human-like counterparts

360
00:15:25,960 --> 00:15:28,960
 rather than these mechanistic machines that have no capacity

361
00:15:28,960 --> 00:15:30,680
 for social interaction.

362
00:15:30,680 --> 00:15:33,200
 However, there are many associated challenges

363
00:15:33,200 --> 00:15:36,000
 with essentially humanizing a robotical system.

364
00:15:36,000 --> 00:15:38,840
 In the first place, it's hard to describe what constitutes

365
00:15:38,840 --> 00:15:40,080
 a human-like motion.

366
00:15:40,080 --> 00:15:42,400
 But then it's also challenging to then translate

367
00:15:42,400 --> 00:15:45,680
 those descriptions into raw code.

368
00:15:45,680 --> 00:15:47,160
 Thus, we present HandBot.

369
00:15:47,160 --> 00:15:49,840
 We are Miranda Kai, Jordan Ren, and Aaron Zhu.

370
00:15:49,840 --> 00:15:52,120
 And today, we will be presenting our final project

371
00:15:52,120 --> 00:15:53,360
 called HandBot.

372
00:15:53,360 --> 00:15:55,280
 HandBot is a simulation system that

373
00:15:55,280 --> 00:15:57,480
 serves as a proof of concept to construct

374
00:15:57,480 --> 00:16:00,240
 a basic human-like system without the use of learning,

375
00:16:00,240 --> 00:16:02,960
 which is a contrast as many other systems use

376
00:16:02,960 --> 00:16:06,040
 learning in trying to design human-like systems.

377
00:16:06,040 --> 00:16:08,200
 Specifically, HandBot is a simulation

378
00:16:08,200 --> 00:16:10,760
 that is able to recognize and respond to certain hand

379
00:16:10,760 --> 00:16:14,160
 gestures, specifically high fives and fist bumps,

380
00:16:14,160 --> 00:16:16,600
 using only the basics of perception and motion

381
00:16:16,600 --> 00:16:20,400
 planning in as human-like way as possible.

382
00:16:20,400 --> 00:16:22,560
 HandBot has three main systems that allow

383
00:16:22,560 --> 00:16:24,640
 it to respond to human gestures.

384
00:16:24,640 --> 00:16:26,720
 First, the perception system of the robot

385
00:16:26,720 --> 00:16:30,320
 will use a camera to get scene points of a hand or a fist

386
00:16:30,320 --> 00:16:32,120
 that is floating in front of it, then

387
00:16:32,120 --> 00:16:35,040
 use ICP to determine if it is a fist or not

388
00:16:35,040 --> 00:16:37,000
 and its location in the world.

389
00:16:37,000 --> 00:16:39,360
 Second, the robot will perform motion planning

390
00:16:39,360 --> 00:16:42,480
 to set key frames of where the robot hand should be.

391
00:16:42,480 --> 00:16:44,440
 Finally, we use various controllers

392
00:16:44,440 --> 00:16:49,000
 to control the robot and bring it to the target hand.

393
00:16:49,000 --> 00:16:50,640
 When setting up our environment, we

394
00:16:50,640 --> 00:16:52,680
 need to construct a robot that can effectively

395
00:16:52,680 --> 00:16:56,120
 high five or fist bump a floating hand.

396
00:16:56,120 --> 00:16:59,520
 We opted to use the Lego hand that is provided by Drake,

397
00:16:59,520 --> 00:17:03,000
 as well as the KUKA IWA 7-Link arm.

398
00:17:03,000 --> 00:17:06,720
 We then welded the hand onto the last joint of the IWA arm.

399
00:17:06,720 --> 00:17:09,080
 In addition, because the hand's configuration

400
00:17:09,080 --> 00:17:12,040
 would not change after it matches the target hands,

401
00:17:12,040 --> 00:17:14,680
 we fixed the hand joints on the robot arm

402
00:17:14,680 --> 00:17:17,520
 such that it has no degrees of freedom.

403
00:17:17,520 --> 00:17:19,200
 To further simplify our environment,

404
00:17:19,200 --> 00:17:21,280
 we decided to keep it clutter-free,

405
00:17:21,280 --> 00:17:23,280
 as receiving objects in the scene

406
00:17:23,280 --> 00:17:24,960
 and constraining motion planning would

407
00:17:24,960 --> 00:17:28,120
 increase the time it takes for the robot to respond.

408
00:17:28,120 --> 00:17:30,840
 And in many cases, there are no obstructions

409
00:17:30,840 --> 00:17:34,720
 in the way of the high five or fist bump.

410
00:17:34,720 --> 00:17:37,800
 In order to identify which configuration the hand is in,

411
00:17:37,800 --> 00:17:41,720
 as well as its location, we make use of ICP.

412
00:17:41,720 --> 00:17:44,440
 In our initial iteration, we generated the model points

413
00:17:44,440 --> 00:17:47,560
 by concatenating point clouds from multiple camera angles

414
00:17:47,560 --> 00:17:49,680
 around the hand, getting the full model

415
00:17:49,680 --> 00:17:52,080
 view of the hand or fist.

416
00:17:52,080 --> 00:17:55,280
 However, since ICP tends to get stuck in local minima

417
00:17:55,280 --> 00:17:58,360
 if the model points are not in a configuration similar to the scene

418
00:17:58,360 --> 00:18:01,320
 points, we instead opted to generate model points

419
00:18:01,320 --> 00:18:03,840
 from angles close to the front of the hand,

420
00:18:03,840 --> 00:18:06,480
 as seen on the figure in the left.

421
00:18:06,480 --> 00:18:09,000
 This makes it so that the scene points almost always

422
00:18:09,000 --> 00:18:12,520
 match the correct portion of the model points.

423
00:18:12,520 --> 00:18:16,040
 We then obtain scene points from a camera next to the IWA arm

424
00:18:16,040 --> 00:18:20,000
 and perform ICP for both the fist and hand points.

425
00:18:20,000 --> 00:18:22,000
 Whichever ICP error is the smallest

426
00:18:22,000 --> 00:18:24,280
 gives us the hand configuration to use

427
00:18:24,280 --> 00:18:27,800
 and the transform of the hand to approach.

428
00:18:27,800 --> 00:18:30,680
 After learning the orientation and position of the target

429
00:18:30,680 --> 00:18:34,000
 hand, we can construct the robot arm's planned trajectory

430
00:18:34,000 --> 00:18:36,960
 by interpolating 10 frames between the robot's

431
00:18:36,960 --> 00:18:40,000
 initial state and the desired final state.

432
00:18:40,000 --> 00:18:42,960
 We decided to use a linear trajectory for simplification

433
00:18:42,960 --> 00:18:46,480
 reasons, which worked well over the short distances being

434
00:18:46,480 --> 00:18:49,080
 traveled to reach the target hand.

435
00:18:49,080 --> 00:18:51,560
 Once we have this trajectory, the system

436
00:18:51,560 --> 00:18:54,680
 will use a differential inverse kinematics controller

437
00:18:54,680 --> 00:18:57,120
 to translate these poses into joint positions

438
00:18:57,120 --> 00:19:01,160
 to execute the simulation.

439
00:19:01,160 --> 00:19:04,160
 As you can see, we have designed a robotic system

440
00:19:04,160 --> 00:19:06,660
 that is able to respond to human gestures

441
00:19:06,660 --> 00:19:09,080
 like high fives and fist bumps.

442
00:19:09,080 --> 00:19:12,060
 However, while we attempted to create our system

443
00:19:12,060 --> 00:19:14,640
 in an as human-like way as possible,

444
00:19:14,640 --> 00:19:16,320
 we acknowledge that our system is far

445
00:19:16,320 --> 00:19:18,680
 from being perfectly natural.

446
00:19:18,680 --> 00:19:21,080
 While our system may not be perfect,

447
00:19:21,080 --> 00:19:24,560
 the results do serve as a stepping stone for future work

448
00:19:24,560 --> 00:19:25,360
 on related tasks.

449
00:19:25,360 --> 00:19:36,880
 Since people have come in, now let me just--

450
00:19:36,880 --> 00:19:37,560
 quick logistics.

451
00:19:37,560 --> 00:19:39,000
 So I went through the spreadsheet.

452
00:19:39,000 --> 00:19:39,800
 You guys are awesome.

453
00:19:39,800 --> 00:19:43,100
 You're like saying, I could be there between 3:30 and 3:33

454
00:19:43,100 --> 00:19:44,460
 or whatever.

455
00:19:44,460 --> 00:19:45,860
 I did my best to sort of--

456
00:19:45,860 --> 00:19:47,140
 I basically kept the order, but I

457
00:19:47,140 --> 00:19:50,020
 shuffled people who said I can be here only until 3:30

458
00:19:50,020 --> 00:19:50,780
 a little bit.

459
00:19:50,780 --> 00:19:51,900
 And it's all in the spreadsheet now.

460
00:19:51,900 --> 00:19:53,100
 We're going to go through it.

461
00:19:53,100 --> 00:19:54,060
 And if you didn't--

462
00:19:54,060 --> 00:19:56,620
 if you thought-- if you marked yourself unlisted,

463
00:19:56,620 --> 00:19:58,780
 but we're hoping to be shown here,

464
00:19:58,780 --> 00:20:03,580
 tell Bo Yen quick, because the plan was to just show

465
00:20:03,580 --> 00:20:04,420
 the public videos.

466
00:20:04,420 --> 00:20:06,620
 That was the way you tell us you want it to be shown.

467
00:20:07,240 --> 00:20:10,720
 [INAUDIBLE]

468
00:20:10,720 --> 00:20:12,400
 Oh, pfft.

469
00:20:12,400 --> 00:20:15,440
 I will do that from my phone while the next video is playing.

470
00:20:15,440 --> 00:20:16,960
 And just one other comment.

471
00:20:16,960 --> 00:20:20,080
 I think there's people in the class that

472
00:20:20,080 --> 00:20:22,480
 are in their final year of the PhD working on robotics.

473
00:20:22,480 --> 00:20:24,940
 And there's people in the class that this is the first time

474
00:20:24,940 --> 00:20:26,280
 they've ever touched robotics.

475
00:20:26,280 --> 00:20:27,840
 So I think it's--

476
00:20:27,840 --> 00:20:31,280
 just appreciate the diversity of what people have done.

477
00:20:31,280 --> 00:20:34,720
 And it's really awesome to see the whole spectrum of--

478
00:20:34,720 --> 00:20:38,060
 and I have just been watching how much people--

479
00:20:38,060 --> 00:20:39,640
 how much effort people are putting in,

480
00:20:39,640 --> 00:20:42,100
 how much learning they've done on that part.

481
00:20:42,100 --> 00:20:44,060
 So appreciate the whole spectrum with me.

482
00:20:44,060 --> 00:20:48,980
 Bo Yen, access.

483
00:20:48,980 --> 00:20:53,320
 Hi.

484
00:20:53,320 --> 00:20:55,740
 Today we're presenting to you the Hanoi EWO robot,

485
00:20:55,740 --> 00:20:58,780
 which is a robot that can solve the Tower of Hanoi.

486
00:20:58,780 --> 00:21:00,260
 My name is Ning Shan Ma.

487
00:21:00,260 --> 00:21:01,300
 I'm Zi Tong Cheng.

488
00:21:01,300 --> 00:21:04,700
 And we're two juniors studying computer science at the MIT.

489
00:21:04,700 --> 00:21:07,200
 EECS department.

490
00:21:07,200 --> 00:21:10,220
 Today we'll be going through four parts of our project.

491
00:21:10,220 --> 00:21:12,800
 The first is introduction and problem definition.

492
00:21:12,800 --> 00:21:15,460
 Basically, why should we care about this problem?

493
00:21:15,460 --> 00:21:15,960
 Yeah.

494
00:21:15,960 --> 00:21:19,160
 Well, so simply to start, to help people in their daily lives,

495
00:21:19,160 --> 00:21:21,560
 robots today must be able to make intelligent decisions

496
00:21:21,560 --> 00:21:23,500
 while performing manipulation tasks.

497
00:21:23,500 --> 00:21:25,620
 And Tower of Hanoi is a perfect example,

498
00:21:25,620 --> 00:21:28,260
 challenging the robot both to reason cognitively

499
00:21:28,260 --> 00:21:30,320
 and also to perform strategic motion planning,

500
00:21:30,320 --> 00:21:32,780
 and finally, of course, to manipulate the object.

501
00:21:32,780 --> 00:21:35,300
 Yeah, previously we've seen robots assisting in household

502
00:21:35,300 --> 00:21:38,020
 tasks, as well as robots playing other intelligent puzzles,

503
00:21:38,020 --> 00:21:39,860
 such as Rubik's Cube or Jenga.

504
00:21:39,860 --> 00:21:42,380
 So we thought of it, applied it to the Tower of Hanoi,

505
00:21:42,380 --> 00:21:45,460
 which is another intellectual puzzle.

506
00:21:45,460 --> 00:21:46,780
 And basically, the Tower of Hanoi

507
00:21:46,780 --> 00:21:48,780
 is a classic mathematical puzzle,

508
00:21:48,780 --> 00:21:50,940
 where the goal is to transport a stack of disks

509
00:21:50,940 --> 00:21:54,620
 from one peg to the other, usually from the leftmost peg

510
00:21:54,620 --> 00:21:55,780
 to the rightmost peg.

511
00:21:55,780 --> 00:21:57,860
 And the constraints or the rules of this game

512
00:21:57,860 --> 00:22:00,180
 are that you can only move one disk at a time,

513
00:22:00,180 --> 00:22:03,620
 and that no larger disk can be placed on top of a smaller disk.

514
00:22:03,620 --> 00:22:06,820
 And we want to minimize the time or the steps required

515
00:22:06,820 --> 00:22:09,500
 for this task.

516
00:22:09,500 --> 00:22:11,260
 And we'll be going through the approach

517
00:22:11,260 --> 00:22:15,100
 or how we made the robot play the game.

518
00:22:15,100 --> 00:22:17,700
 First, we had the whole system set up

519
00:22:17,700 --> 00:22:21,300
 in Drake, which is a simulation environment from our class,

520
00:22:21,300 --> 00:22:24,100
 where we represented the disk as box instances.

521
00:22:24,100 --> 00:22:28,220
 And we represented the pegs as flat cylindrical bases.

522
00:22:28,220 --> 00:22:32,380
 And the robot arm we used is the KUKA LBR EWAR arm.

523
00:22:32,380 --> 00:22:34,700
 The second step, of course, is to design our algorithm,

524
00:22:34,700 --> 00:22:36,580
 break it down, and finally develop it.

525
00:22:36,580 --> 00:22:39,380
 So here is a basic flowchart of our program.

526
00:22:39,380 --> 00:22:41,300
 We break it down mainly into three separate steps.

527
00:22:41,300 --> 00:22:45,620
 The first is to use a recursive generation algorithm for us

528
00:22:45,620 --> 00:22:47,260
 to actually figure out what step to do

529
00:22:47,260 --> 00:22:48,860
 to solve the Tower of Hanoi.

530
00:22:48,860 --> 00:22:51,940
 The second is to import those steps into the robot gripper,

531
00:22:51,940 --> 00:22:54,180
 and for the gripper to iterate through each step

532
00:22:54,180 --> 00:22:59,340
 to move each disk from target to the desired location.

533
00:22:59,340 --> 00:23:01,140
 And the third step, of course, is

534
00:23:01,140 --> 00:23:03,180
 to once each step is completed, as well as

535
00:23:03,180 --> 00:23:04,740
 once all steps are completed, we need

536
00:23:04,740 --> 00:23:07,340
 to check for those conditions and tell the robot to stop,

537
00:23:07,340 --> 00:23:10,340
 because that's where we end the program.

538
00:23:10,340 --> 00:23:12,500
 And during this process, we definitely

539
00:23:12,500 --> 00:23:14,540
 run into some difficulties and challenges,

540
00:23:14,540 --> 00:23:16,300
 which we'll talk about here.

541
00:23:16,300 --> 00:23:19,460
 Well, first thing is that we figure out we cannot actually

542
00:23:19,460 --> 00:23:20,660
 grab the disk from top.

543
00:23:20,660 --> 00:23:23,260
 So what happens is that they might be just simply too large.

544
00:23:23,260 --> 00:23:25,140
 So a simple solution, we decrease

545
00:23:25,140 --> 00:23:27,740
 the size of the different disks.

546
00:23:27,740 --> 00:23:29,380
 A second difficulty we ran into was

547
00:23:29,380 --> 00:23:31,060
 with the geometry of the disk.

548
00:23:31,060 --> 00:23:33,900
 So as you can see here in the demonstration,

549
00:23:33,900 --> 00:23:36,180
 when they're all cylinders, the robot arm

550
00:23:36,180 --> 00:23:38,500
 slips when it was grabbing the disk.

551
00:23:38,500 --> 00:23:40,740
 And this is due to the antipodal grasp, which

552
00:23:40,740 --> 00:23:43,020
 with a curved surface, it's harder.

553
00:23:43,020 --> 00:23:45,220
 So we change it to using boxes, that it

554
00:23:45,220 --> 00:23:48,860
 doesn't need to be specifically at the two diameter points.

555
00:23:48,860 --> 00:23:49,420
 Right.

556
00:23:49,420 --> 00:23:51,340
 And the third thing is that we figure out that,

557
00:23:51,340 --> 00:23:54,460
 because we are using time to keep track of our simulation,

558
00:23:54,460 --> 00:23:57,220
 sometimes the same step, which should be only executed once,

559
00:23:57,220 --> 00:23:58,740
 is executed repeatedly.

560
00:23:58,740 --> 00:24:00,740
 So here, that's why we incorporate a checking

561
00:24:00,740 --> 00:24:03,900
 condition to determine, has the gripper got to the beginning

562
00:24:03,900 --> 00:24:04,460
 position?

563
00:24:04,460 --> 00:24:05,980
 Has it got to the end position?

564
00:24:05,980 --> 00:24:08,060
 If both, yes, then we said the step is complete,

565
00:24:08,060 --> 00:24:10,540
 and we execute the step.

566
00:24:10,540 --> 00:24:12,660
 And this is the moment that you've been waiting for.

567
00:24:12,660 --> 00:24:14,780
 How well can our robot play the game?

568
00:24:14,780 --> 00:24:20,180
 So as you can see here, this is a 15 times speeded up

569
00:24:20,180 --> 00:24:21,900
 version of our robot playing the game.

570
00:24:21,900 --> 00:24:23,740
 It actually manipulates pretty effectively.

571
00:24:23,740 --> 00:24:25,020
 There's no waste of time.

572
00:24:25,020 --> 00:24:27,780
 And also, you can see the stacking is pretty accurate.

573
00:24:27,780 --> 00:24:30,860
 And you can see at the end, all disks

574
00:24:30,860 --> 00:24:33,140
 have been transported from the leftmost peg

575
00:24:33,140 --> 00:24:34,140
 to the rightmost peg.

576
00:24:34,140 --> 00:24:36,940
 And since this is also a three layer model,

577
00:24:36,940 --> 00:24:39,420
 we can easily generalize it to higher layers.

578
00:24:39,420 --> 00:24:43,580
 And some future work.

579
00:24:43,580 --> 00:24:46,380
 So what areas can be improved about our robot?

580
00:24:46,380 --> 00:24:47,660
 The first is perception.

581
00:24:47,660 --> 00:24:49,580
 Right now, our robot is basically

582
00:24:49,580 --> 00:24:51,220
 getting the information about location.

583
00:24:51,220 --> 00:24:56,220
 Good.

584
00:24:56,220 --> 00:24:56,720
 Very nice.

585
00:24:56,720 --> 00:25:00,220
 [APPLAUSE]

586
00:25:00,220 --> 00:25:02,540
 Now, I remember I talked to you guys not too long ago,

587
00:25:02,540 --> 00:25:03,780
 and the robot was barely moving.

588
00:25:03,780 --> 00:25:05,780
 And then all of a sudden, it's doing the whole Tower of Hanoi.

589
00:25:05,780 --> 00:25:08,220
 I thought that was incredible how fast.

590
00:25:08,220 --> 00:25:09,660
 That was good.

591
00:25:09,660 --> 00:25:10,180
 I love it.

592
00:25:10,180 --> 00:25:12,860
 I love it.

593
00:25:12,860 --> 00:25:13,460
 Any questions?

594
00:25:13,460 --> 00:25:13,960
 No?

595
00:25:14,880 --> 00:25:18,360
 [SIDE CONVERSATION]

596
00:25:18,360 --> 00:25:24,200
 In this project, we seek to enable a robotic arm

597
00:25:24,200 --> 00:25:26,680
 to catch a free-flying spherical projectile using only

598
00:25:26,680 --> 00:25:31,800
 an RGBD sensor and a bin welded to the robot's end effector.

599
00:25:31,800 --> 00:25:33,320
 We study this problem because it is

600
00:25:33,320 --> 00:25:36,200
 an example of time-constrained robotic manipulation.

601
00:25:36,200 --> 00:25:38,000
 All perception, planning, and control

602
00:25:38,000 --> 00:25:41,000
 must take place within the time of flight of the object.

603
00:25:41,000 --> 00:25:43,800
 This is in stark contrast to many other manipulation tasks,

604
00:25:43,800 --> 00:25:46,440
 such as pick and place.

605
00:25:46,440 --> 00:25:49,400
 This presents challenges in both perception and control,

606
00:25:49,400 --> 00:25:51,800
 as we must localize the object in real time

607
00:25:51,800 --> 00:25:53,760
 and plan an efficient catch motion in order

608
00:25:53,760 --> 00:25:57,000
 to be successful.

609
00:25:57,000 --> 00:25:59,280
 Our system is split into three phases--

610
00:25:59,280 --> 00:26:02,040
 first, perception, where we localize the object;

611
00:26:02,040 --> 00:26:04,760
 then planning, where we select an interception point based

612
00:26:04,760 --> 00:26:07,760
 on our estimated trajectory; finally, control,

613
00:26:07,760 --> 00:26:08,960
 where we execute the catch.

614
00:26:11,920 --> 00:26:14,560
 To localize the object, we note that any two points

615
00:26:14,560 --> 00:26:18,040
 in the RGBD point cloud must be equidistant from the object

616
00:26:18,040 --> 00:26:19,800
 center.

617
00:26:19,800 --> 00:26:21,440
 This constraint can be formulated

618
00:26:21,440 --> 00:26:23,720
 as a quadratic optimization cost.

619
00:26:23,720 --> 00:26:26,280
 And then, after summing this optimization cost

620
00:26:26,280 --> 00:26:29,280
 over all pairs of points, we can use quadratic programming

621
00:26:29,280 --> 00:26:31,280
 to find an optimal center.

622
00:26:31,280 --> 00:26:33,640
 We find that the center is robust to noise typically

623
00:26:33,640 --> 00:26:35,400
 present in modern-day RGBD sensors.

624
00:26:35,400 --> 00:26:40,880
 Given multiple of these object poses,

625
00:26:40,880 --> 00:26:42,760
 we can estimate trajectory by correcting

626
00:26:42,760 --> 00:26:46,320
 for the quadratic term in the projectile motion equations

627
00:26:46,320 --> 00:26:49,520
 and fitting a line to the corrected data points

628
00:26:49,520 --> 00:26:53,000
 in order to predict the initial object position and velocity.

629
00:26:53,000 --> 00:26:58,360
 To select an interception point, we simply

630
00:26:58,360 --> 00:27:00,440
 take the closest point on the trajectory

631
00:27:00,440 --> 00:27:01,640
 to the current bin position.

632
00:27:01,640 --> 00:27:07,080
 By using inverse kinematics to command the bin

633
00:27:07,080 --> 00:27:09,520
 to the interception point, we can already

634
00:27:09,520 --> 00:27:11,880
 achieve limited success in the catching task,

635
00:27:11,880 --> 00:27:15,560
 specifically when the object's arc is very high and slow

636
00:27:15,560 --> 00:27:18,000
 moving.

637
00:27:18,000 --> 00:27:20,640
 However, for fast-moving projectiles,

638
00:27:20,640 --> 00:27:22,840
 the object tends to either bounce out or roll out

639
00:27:22,840 --> 00:27:25,480
 of the bin after impact.

640
00:27:25,480 --> 00:27:27,640
 To correct for this, we introduce a compensation

641
00:27:27,640 --> 00:27:30,560
 trajectory consisting of two parts--

642
00:27:30,560 --> 00:27:32,840
 first, a linear path minimizing the relative motion

643
00:27:32,840 --> 00:27:35,560
 between the projectile and the bin, and second, a tilt

644
00:27:35,560 --> 00:27:38,960
 meant to prevent the object from rolling out of the bin.

645
00:27:38,960 --> 00:27:40,880
 With the compensation trajectory,

646
00:27:40,880 --> 00:27:44,320
 our system is able to adapt to varying object poses,

647
00:27:44,320 --> 00:27:45,720
 velocities, and angles.

648
00:27:45,720 --> 00:27:54,520
 However, our system is not perfect.

649
00:27:54,520 --> 00:27:57,760
 The two primary failure modes are significant error

650
00:27:57,760 --> 00:28:00,680
 in the estimated pose of the object,

651
00:28:00,680 --> 00:28:03,480
 resulting in a complete miss.

652
00:28:03,480 --> 00:28:06,760
 The next is IK failure during the compensation trajectory,

653
00:28:06,760 --> 00:28:09,160
 which prevents us from compensating the object motion

654
00:28:09,160 --> 00:28:12,160
 properly.

655
00:28:12,160 --> 00:28:14,680
 To conclude, our system achieves reasonable success

656
00:28:14,680 --> 00:28:18,000
 on the catching task and simulation.

657
00:28:18,000 --> 00:28:21,280
 Future work could be done to improve both the robustness--

658
00:28:21,280 --> 00:28:29,800
 So did you solve IK?

659
00:28:29,800 --> 00:28:32,440
 You said you said IK from here to here, and then

660
00:28:32,440 --> 00:28:35,040
 did you linearly interpolate between them?

661
00:28:35,040 --> 00:28:35,800
 Yeah, yeah.

662
00:28:35,800 --> 00:28:36,160
 Cool.

663
00:28:36,160 --> 00:28:36,680
 No, it's good.

664
00:28:36,680 --> 00:28:37,640
 You got one.

665
00:28:37,640 --> 00:28:38,360
 I really love it.

666
00:28:38,360 --> 00:28:38,840
 Yeah.

667
00:28:38,840 --> 00:28:41,800
 [INAUDIBLE]

668
00:28:41,800 --> 00:28:57,200
 Very good.

669
00:28:57,200 --> 00:29:00,480
 OK.

670
00:29:05,080 --> 00:29:07,160
 This is a summary of my final project

671
00:29:07,160 --> 00:29:09,920
 on planning prehensile pushing of a horizontal plane

672
00:29:09,920 --> 00:29:13,760
 using rapidly expanding random trees and motion cones.

673
00:29:13,760 --> 00:29:16,160
 Pushing is a deceptively complex task,

674
00:29:16,160 --> 00:29:18,840
 and I chose to focus on a simplified case,

675
00:29:18,840 --> 00:29:21,480
 pushing a predefined object in a horizontal plane

676
00:29:21,480 --> 00:29:23,800
 with a point contact, i.e.

677
00:29:23,800 --> 00:29:26,480
 pushing an object while it rests on the ground.

678
00:29:26,480 --> 00:29:29,080
 We can split this task into two main steps,

679
00:29:29,080 --> 00:29:30,800
 developing a friction-aware planner that

680
00:29:30,800 --> 00:29:32,840
 can find a series of feasible pushes

681
00:29:32,840 --> 00:29:35,040
 that move a square block from its initial position

682
00:29:35,040 --> 00:29:37,400
 to some desired goal, and implementing

683
00:29:37,400 --> 00:29:39,360
 this trajectory on a simulated robot

684
00:29:39,360 --> 00:29:41,640
 in order to evaluate performance.

685
00:29:41,640 --> 00:29:44,080
 We limit the positions around the edge of a block

686
00:29:44,080 --> 00:29:46,240
 at which a push can be applied to it.

687
00:29:46,240 --> 00:29:49,280
 As we learned in class, a pusher at each of these locations

688
00:29:49,280 --> 00:29:51,080
 can exert a force on the block that

689
00:29:51,080 --> 00:29:53,040
 lies within a friction cone determined

690
00:29:53,040 --> 00:29:55,080
 by the coefficient of friction between the pusher

691
00:29:55,080 --> 00:29:55,960
 and the block.

692
00:29:55,960 --> 00:29:57,760
 If the pusher attempts to apply a force

693
00:29:57,760 --> 00:30:01,840
 on the boundary of this cone, then it will slip.

694
00:30:01,840 --> 00:30:05,160
 Motion cones can be viewed as an extension of friction cones.

695
00:30:05,160 --> 00:30:07,920
 We use them to represent the set of possible twists

696
00:30:07,920 --> 00:30:11,400
 that can be induced on an object by an external wrench.

697
00:30:11,400 --> 00:30:13,280
 This diagram shows the general process

698
00:30:13,280 --> 00:30:16,440
 for determining the motion cones for our particular task.

699
00:30:16,440 --> 00:30:19,000
 In the case of pushing in the horizontal plane,

700
00:30:19,000 --> 00:30:21,520
 the motion cone is composed of two wrenches,

701
00:30:21,520 --> 00:30:24,280
 one from the pusher and one from the supporting surface,

702
00:30:24,280 --> 00:30:25,920
 i.e. the ground.

703
00:30:25,920 --> 00:30:28,000
 The pusher-wrench can be found by transforming

704
00:30:28,000 --> 00:30:30,680
 the set of feasible pushes to the block zone coordinate

705
00:30:30,680 --> 00:30:31,560
 frame.

706
00:30:31,560 --> 00:30:33,480
 The set of possible support wrenches

707
00:30:33,480 --> 00:30:36,320
 can then be represented by an ellipsoidal approximation

708
00:30:36,320 --> 00:30:38,400
 of a limit surface.

709
00:30:38,400 --> 00:30:40,080
 The limit surface can be visualized

710
00:30:40,080 --> 00:30:42,240
 as the capability of the ground to resist

711
00:30:42,240 --> 00:30:45,200
 loads applied to the object.

712
00:30:45,200 --> 00:30:47,200
 This is itself a product of friction

713
00:30:47,200 --> 00:30:49,280
 between the ground and the object.

714
00:30:49,280 --> 00:30:51,560
 In this video, we can see that the roll of tape

715
00:30:51,560 --> 00:30:53,560
 doesn't move until the load being applied

716
00:30:53,560 --> 00:30:55,120
 exceeds some threshold.

717
00:30:55,120 --> 00:30:57,560
 This is indication of a support wrench resisting

718
00:30:57,560 --> 00:30:59,120
 our efforts to move it.

719
00:30:59,120 --> 00:31:02,280
 Once we have a set of possible pusher and support wrenches,

720
00:31:02,280 --> 00:31:05,640
 we can combine them to find the direction of possible motions.

721
00:31:05,640 --> 00:31:07,480
 In order for our block to move, we

722
00:31:07,480 --> 00:31:09,200
 must apply a pusher-wrench with respect

723
00:31:09,200 --> 00:31:11,720
 to friction constraints, and this pusher-wrench

724
00:31:11,720 --> 00:31:14,240
 must be sufficient to overcome the corresponding support

725
00:31:14,240 --> 00:31:15,160
 wrench.

726
00:31:15,160 --> 00:31:17,840
 We can find the motions that result from this process

727
00:31:17,840 --> 00:31:19,800
 through the vectors perpendicular to the limit

728
00:31:19,800 --> 00:31:23,440
 surface at the locations where the pusher-wrench intersects.

729
00:31:23,440 --> 00:31:25,360
 The set of all of these possible vectors

730
00:31:25,360 --> 00:31:27,640
 forms a motion cone, which we can finally

731
00:31:27,640 --> 00:31:30,280
 represent in the world frame.

732
00:31:30,280 --> 00:31:32,920
 Using motion cones to validate potential pushes,

733
00:31:32,920 --> 00:31:36,040
 I successfully implemented a simple RRT planner.

734
00:31:36,040 --> 00:31:38,720
 Note that due to the stochastic nature of this approach,

735
00:31:38,720 --> 00:31:40,760
 there are occasions when the planner fails to find

736
00:31:40,760 --> 00:31:42,640
 a valid path towards the goal.

737
00:31:42,640 --> 00:31:45,080
 I introduced an acceptable tolerance of misalignment

738
00:31:45,080 --> 00:31:47,600
 with the goal in order to improve the chances of finding

739
00:31:47,600 --> 00:31:49,720
 a feasible trajectory.

740
00:31:49,720 --> 00:31:52,240
 Trying to follow the planned trajectories in the simulator

741
00:31:52,240 --> 00:31:53,920
 led to promising results.

742
00:31:53,920 --> 00:31:56,160
 Due to the feedforward nature of our controller,

743
00:31:56,160 --> 00:32:01,000
 we observe an accumulation of error for longer trajectories.

744
00:32:01,000 --> 00:32:03,760
 These errors result from inaccurate pusher positions.

745
00:32:03,760 --> 00:32:14,120
 Is there like some time we really

746
00:32:14,120 --> 00:32:15,960
 must watch the last little bit?

747
00:32:15,960 --> 00:32:16,460
 It's fine.

748
00:32:16,460 --> 00:32:18,160
 If it's double-edged, it doesn't work.

749
00:32:18,160 --> 00:32:18,660
 Oh.

750
00:32:18,660 --> 00:32:22,840
 Nice, OK.

751
00:32:22,840 --> 00:32:25,600
 So that is a really nice-- that paper that you built off of

752
00:32:25,600 --> 00:32:28,160
 is a really nice example of planning with that contact

753
00:32:28,160 --> 00:32:29,960
 that we didn't have that lecture.

754
00:32:29,960 --> 00:32:33,000
 But for planning with dynamic constraints of friction

755
00:32:33,000 --> 00:32:35,120
 and everything like that, that's a really nice paper

756
00:32:35,120 --> 00:32:36,240
 to build off of.

757
00:32:36,240 --> 00:32:36,840
 Yeah, thanks.

758
00:32:36,840 --> 00:32:39,400
 Go on back.

759
00:32:39,400 --> 00:32:40,840
 OK, since we do have a few people

760
00:32:40,840 --> 00:32:43,680
 that we have to add to my already full schedule,

761
00:32:43,680 --> 00:32:45,680
 what I'm going to do is I'm going to prioritize people

762
00:32:45,680 --> 00:32:46,840
 that are in the room, right?

763
00:32:46,840 --> 00:32:49,840
 So if we go and the presenter is not here,

764
00:32:49,840 --> 00:32:51,960
 then we can come back if someone shows up.

765
00:32:51,960 --> 00:32:55,120
 But I think we should prioritize the people that

766
00:32:55,120 --> 00:32:58,200
 are in the room, yeah?

767
00:32:58,200 --> 00:32:58,920
 OK, so--

768
00:32:58,920 --> 00:33:02,400
 [AUDIO OUT]

769
00:33:02,400 --> 00:33:06,320
 - You go to the grocery store just to see a huge line.

770
00:33:06,320 --> 00:33:08,240
 The alternative is self-checkout.

771
00:33:08,240 --> 00:33:10,160
 But that comes with its own set of issues,

772
00:33:10,160 --> 00:33:11,760
 like Cody faces over here.

773
00:33:11,760 --> 00:33:15,200
 - Please place the item on the screen and wait.

774
00:33:15,200 --> 00:33:16,760
 Please wait for a minute.

775
00:33:16,760 --> 00:33:17,260
 - Why?

776
00:33:17,260 --> 00:33:19,760
 What did I do?

777
00:33:19,760 --> 00:33:23,400
 - Please place the item on the screen and wait.

778
00:33:23,400 --> 00:33:24,960
 Please wait for a minute.

779
00:33:24,960 --> 00:33:26,920
 - Oh, goodness.

780
00:33:26,920 --> 00:33:30,080
 - We wanted to make this a far better experience.

781
00:33:30,080 --> 00:33:31,240
 I'm Sonia.

782
00:33:31,240 --> 00:33:32,200
 - And I'm Vishnu.

783
00:33:32,200 --> 00:33:33,880
 And we created Checkoutbot, a robot

784
00:33:33,880 --> 00:33:36,840
 to automate the checkout process at grocery stores.

785
00:33:36,840 --> 00:33:39,040
 - Here's the scenario we envision.

786
00:33:39,040 --> 00:33:41,320
 A shopper will fill their cart with items

787
00:33:41,320 --> 00:33:43,640
 and push it right next to the robot.

788
00:33:43,640 --> 00:33:46,400
 The robot will then pick up the items one by one,

789
00:33:46,400 --> 00:33:48,520
 put them on the counter for scanning,

790
00:33:48,520 --> 00:33:50,800
 and then finally pick up the scanned item

791
00:33:50,800 --> 00:33:52,960
 and put them in the final cart for the shopper

792
00:33:52,960 --> 00:33:54,880
 to push out of the store.

793
00:33:54,880 --> 00:33:56,720
 Now, creating the simulation required

794
00:33:56,720 --> 00:34:00,000
 us to create the models for several different items.

795
00:34:00,000 --> 00:34:02,080
 And here we have the individual grocery items

796
00:34:02,080 --> 00:34:03,600
 that we modeled ourselves.

797
00:34:03,600 --> 00:34:06,360
 It's a box with packaging on most sides,

798
00:34:06,360 --> 00:34:08,960
 and then a QR code on the final side.

799
00:34:08,960 --> 00:34:11,440
 And we repeated this process for several different grocery

800
00:34:11,440 --> 00:34:12,600
 items.

801
00:34:12,600 --> 00:34:15,520
 Lastly, we imported a model for a shopping cart,

802
00:34:15,520 --> 00:34:17,440
 like you can see on the right, to better

803
00:34:17,440 --> 00:34:21,360
 create a feel of a shopping environment in our simulation.

804
00:34:21,360 --> 00:34:25,240
 Now, here's a video demo of our robot in action.

805
00:34:25,240 --> 00:34:27,440
 It comes to this first shopping cart,

806
00:34:27,440 --> 00:34:30,080
 and it picks up each item one by one.

807
00:34:30,080 --> 00:34:32,280
 So here we have Raisin Bran.

808
00:34:32,280 --> 00:34:35,440
 And it'll position it right in between these three cameras

809
00:34:35,440 --> 00:34:38,080
 that you see at the top.

810
00:34:38,080 --> 00:34:40,320
 It has a QR code on this side.

811
00:34:40,320 --> 00:34:42,680
 And here's a view that the camera sees,

812
00:34:42,680 --> 00:34:46,040
 where it says, scan Raisin Bran, at which point

813
00:34:46,040 --> 00:34:48,440
 it'll proceed to pick up the box again

814
00:34:48,440 --> 00:34:52,080
 and deposit it in the final cart.

815
00:34:52,080 --> 00:34:55,440
 Now, this is our robot working at real speed.

816
00:34:55,440 --> 00:34:57,440
 But now we'll speed it up so you can see it scan

817
00:34:57,440 --> 00:34:59,840
 the other objects as well.

818
00:34:59,840 --> 00:35:02,360
 Here we have Eggo Waffles, where it repeats the process.

819
00:35:02,360 --> 00:35:07,400
 And then here in the camera view,

820
00:35:07,400 --> 00:35:09,320
 you'll be able to see Scan Eggo Waffles when

821
00:35:09,320 --> 00:35:11,680
 it recognized the QR code.

822
00:35:11,680 --> 00:35:13,160
 And I'll just fast forward the video

823
00:35:13,160 --> 00:35:15,920
 so that you can also see how it picks up the Cheerios over

824
00:35:15,920 --> 00:35:20,560
 here, drops it on the counter, picks it up after scanning,

825
00:35:20,560 --> 00:35:23,880
 and then lastly says, scan Cheerios.

826
00:35:23,880 --> 00:35:26,520
 Now, finally, at the end, it'll also

827
00:35:26,520 --> 00:35:29,640
 output an itemized receipt of everything that it scanned,

828
00:35:29,640 --> 00:35:31,920
 as well as its final price.

829
00:35:31,920 --> 00:35:34,080
 Now I'll pass it over to Vishnu to talk more

830
00:35:34,080 --> 00:35:35,120
 about the technical details.

831
00:35:35,120 --> 00:35:40,440
 So in the control flow for a simulation,

832
00:35:40,440 --> 00:35:42,840
 we had several points of emphasis.

833
00:35:42,840 --> 00:35:44,960
 One was retrying based on mode of failure,

834
00:35:44,960 --> 00:35:47,480
 whether it was dropped while picking up from the counter

835
00:35:47,480 --> 00:35:49,560
 or dropped from picking up from the shopper's cart.

836
00:35:49,560 --> 00:35:52,600
 We have code to retry several times.

837
00:35:52,600 --> 00:35:54,280
 And one of the main parts of our project

838
00:35:54,280 --> 00:35:56,000
 was the scanning of the QR code.

839
00:35:56,000 --> 00:35:58,520
 So as Sonia explained, we applied a QR code texture

840
00:35:58,520 --> 00:36:00,040
 to each of the objects we created,

841
00:36:00,040 --> 00:36:02,120
 and these are Aruko QR codes.

842
00:36:02,120 --> 00:36:04,280
 And we used OpenCV's library to be

843
00:36:04,280 --> 00:36:06,840
 able to do both the detection and the identification

844
00:36:06,840 --> 00:36:07,680
 of these codes.

845
00:36:07,680 --> 00:36:10,960
 And we kept a mapping from codes to their object items.

846
00:36:10,960 --> 00:36:14,360
 And we store these items.

847
00:36:14,360 --> 00:36:17,360
 We also had code to reset the robot angles when it got stuck,

848
00:36:17,360 --> 00:36:21,640
 such as when it got stuck by hitting the camera.

849
00:36:21,640 --> 00:36:24,000
 And we had end of simulation behavior,

850
00:36:24,000 --> 00:36:25,400
 such as generation of the receipt.

851
00:36:25,400 --> 00:36:27,640
 And what we did is we took the information

852
00:36:27,640 --> 00:36:30,040
 we stored from before, scanning the QR codes,

853
00:36:30,040 --> 00:36:31,680
 and we generated an itemized receipt

854
00:36:31,680 --> 00:36:35,400
 with a final total for the users.

855
00:36:35,400 --> 00:36:37,880
 So in summary, we created a control flow for a checkout

856
00:36:37,880 --> 00:36:40,160
 bot, and we had some effective methods

857
00:36:40,160 --> 00:36:43,360
 for scanning the shopping items using OpenCV and Aruko QR

858
00:36:43,360 --> 00:36:45,080
 codes.

859
00:36:45,080 --> 00:36:46,680
 Future work would include speeding up

860
00:36:46,680 --> 00:36:49,320
 the robot in transition, adding collision geometry

861
00:36:49,320 --> 00:36:51,320
 to items such as the shopping cart,

862
00:36:51,320 --> 00:36:53,560
 and using a more appropriate robot arming gripper

863
00:36:53,560 --> 00:36:55,280
 to deal with the more diverse set of items

864
00:36:55,280 --> 00:36:58,000
 we would deal with in a grocery store.

865
00:36:58,000 --> 00:37:00,320
 Thank you.

866
00:37:00,320 --> 00:37:03,800
 [APPLAUSE]

867
00:37:03,800 --> 00:37:08,160
 How frustrating was the-- look, it was DiffIK, you said, right?

868
00:37:08,160 --> 00:37:10,040
 And I saw it go through itself a couple of times.

869
00:37:10,040 --> 00:37:11,240
 It even went around itself.

870
00:37:11,240 --> 00:37:14,600
 Was that a major bottleneck for you, or did you power through

871
00:37:14,600 --> 00:37:15,100
 it?

872
00:37:15,100 --> 00:37:16,080
 Yeah, of course.

873
00:37:16,080 --> 00:37:20,040
 It was just dealing with when you generate the source code,

874
00:37:20,040 --> 00:37:21,520
 you have to use the monotype.

875
00:37:21,520 --> 00:37:24,720
 It was a saving flow, so that it would not

876
00:37:24,720 --> 00:37:27,080
 say, oh, I'm using the product, and it's not valid.

877
00:37:27,080 --> 00:37:29,520
 So it was kind of a--

878
00:37:29,520 --> 00:37:31,920
 And the kinematic reachability of the EWO

879
00:37:31,920 --> 00:37:33,480
 was pretty limited to get into that.

880
00:37:33,480 --> 00:37:36,000
 So you had to kind of drop the cart.

881
00:37:36,000 --> 00:37:37,640
 I love it.

882
00:37:37,640 --> 00:37:41,000
 And I love the textureless shopper.

883
00:37:41,000 --> 00:37:42,840
 I couldn't read what it said in the banner.

884
00:37:42,840 --> 00:37:44,280
 What did it say in the banner?

885
00:37:44,280 --> 00:37:46,760
 [INAUDIBLE]

886
00:37:46,760 --> 00:37:48,120
 That's awesome.

887
00:37:48,120 --> 00:37:49,120
 So good.

888
00:37:49,120 --> 00:37:50,120
 I have a quick question.

889
00:37:50,120 --> 00:37:51,120
 Of course, yeah.

890
00:37:51,120 --> 00:37:53,600
 How do you know what the key offset is?

891
00:37:53,600 --> 00:37:57,600
 If we are using the box?

892
00:37:57,600 --> 00:38:02,080
 So in DiffIK, we rarely end up using the box,

893
00:38:02,080 --> 00:38:06,080
 where the impact of the simulation and what it tells

894
00:38:06,080 --> 00:38:11,080
 us didn't actually line up with what [INAUDIBLE]

895
00:38:11,080 --> 00:38:16,080
 So even though that simulation is showing the faces of the box

896
00:38:16,080 --> 00:38:18,080
 and the source code on one, the RPCensor

897
00:38:18,080 --> 00:38:21,560
 actually saw the source code on all faces,

898
00:38:21,560 --> 00:38:23,040
 because the [INAUDIBLE]

899
00:38:23,040 --> 00:38:25,520
 [LAUGHTER]

900
00:38:25,520 --> 00:38:28,520
 Oh, man.

901
00:38:28,520 --> 00:38:30,000
 That's like the texture coordinates

902
00:38:30,000 --> 00:38:31,680
 were different or something.

903
00:38:31,680 --> 00:38:32,400
 OK.

904
00:38:32,400 --> 00:38:33,280
 Send that to me after.

905
00:38:33,280 --> 00:38:33,880
 We'll fix that.

906
00:38:33,880 --> 00:38:36,320
 [LAUGHTER]

907
00:38:36,320 --> 00:38:41,320
 Hi.

908
00:38:41,320 --> 00:38:43,640
 For our 14.12 final project, we developed

909
00:38:43,640 --> 00:38:46,000
 TetrisBot, which is an end-to-end robotic system that

910
00:38:46,000 --> 00:38:47,920
 plays Tetris.

911
00:38:47,920 --> 00:38:50,080
 The motivation behind developing TetrisBot

912
00:38:50,080 --> 00:38:53,120
 is that we've seen that the combination of unrealistic game

913
00:38:53,120 --> 00:38:55,560
 elements that require a complicated simulation

914
00:38:55,560 --> 00:38:58,800
 and fine-grained pick and place is pretty rarely explored.

915
00:38:58,800 --> 00:39:00,920
 And we choose Tetris in order to explore this,

916
00:39:00,920 --> 00:39:04,280
 because it satisfies both of these criteria.

917
00:39:04,280 --> 00:39:08,040
 Things like row clearing and piece teleportation

918
00:39:08,040 --> 00:39:11,320
 require complex simulation.

919
00:39:11,320 --> 00:39:13,760
 And at the same time, we need highly precise manipulation

920
00:39:13,760 --> 00:39:16,640
 in order for pieces to be placed pretty--

921
00:39:16,640 --> 00:39:18,080
 right next to each other.

922
00:39:18,080 --> 00:39:22,440
 And Tetris has the potential to impact a lot of robotic games

923
00:39:22,440 --> 00:39:25,640
 and interactive simulators, such as things like Connect4, Chess,

924
00:39:25,640 --> 00:39:26,720
 or Jenga.

925
00:39:26,720 --> 00:39:29,360
 And as far as we're aware, this is the first Tetris-playing

926
00:39:29,360 --> 00:39:31,600
 robotic system that we've seen.

927
00:39:31,600 --> 00:39:33,200
 And so as you've seen in this class,

928
00:39:33,200 --> 00:39:34,760
 there is a wide range of projects

929
00:39:34,760 --> 00:39:37,560
 being explored in this space of robotic manipulation.

930
00:39:37,560 --> 00:39:40,000
 So we just wanted to highlight some that we felt also touched

931
00:39:40,000 --> 00:39:42,440
 on some similar aspects, as TetrisBot did.

932
00:39:42,440 --> 00:39:44,320
 So as we mentioned, as we learned in class,

933
00:39:44,320 --> 00:39:46,920
 we learned about the dishwasher loader robot, which

934
00:39:46,920 --> 00:39:49,400
 we saw those aspects of object identification,

935
00:39:49,400 --> 00:39:52,240
 graph determination, and trajectory optimization.

936
00:39:52,240 --> 00:39:55,640
 In the shopping cart example, along with the localization,

937
00:39:55,640 --> 00:39:58,040
 there was also this aspect of optimal piece placement, which

938
00:39:58,040 --> 00:39:59,680
 is very important in Tetris.

939
00:39:59,680 --> 00:40:01,720
 And finally, looking at more of the gameplay side,

940
00:40:01,720 --> 00:40:03,720
 you have Jenga with that adversarial answer

941
00:40:03,720 --> 00:40:06,000
 in gameplay, which is a very important part of Tetris

942
00:40:06,000 --> 00:40:07,520
 as well.

943
00:40:07,520 --> 00:40:11,080
 So here's a brief overview of the methodology of TetrisBot.

944
00:40:11,080 --> 00:40:13,680
 We first construct and teleport a random piece.

945
00:40:13,680 --> 00:40:15,480
 And then we detect said piece using

946
00:40:15,480 --> 00:40:16,960
 a convolutional neural network.

947
00:40:16,960 --> 00:40:20,680
 We pass in whatever our detection algorithm returns

948
00:40:20,680 --> 00:40:22,800
 to the gameplay algorithm, which returns

949
00:40:22,800 --> 00:40:24,560
 a rotation in a position.

950
00:40:24,560 --> 00:40:26,600
 We transform that into board coordinates

951
00:40:26,600 --> 00:40:28,800
 and then place that piece at a desired location.

952
00:40:28,800 --> 00:40:31,680
 And then we update our board state if necessary.

953
00:40:31,680 --> 00:40:33,360
 Going into a little bit more detail,

954
00:40:33,360 --> 00:40:35,800
 in order to construct and teleport a new piece,

955
00:40:35,800 --> 00:40:37,560
 we require a simulation loop, which

956
00:40:37,560 --> 00:40:39,600
 is pretty crucial for this entire project.

957
00:40:39,600 --> 00:40:41,640
 And that's because certain aspects of Tetris

958
00:40:41,640 --> 00:40:44,240
 can't be simulated using things like the physics engine, which

959
00:40:44,240 --> 00:40:47,000
 is what we've traditionally used in class so far.

960
00:40:47,000 --> 00:40:51,280
 Things like being able to spawn a piece at a given location

961
00:40:51,280 --> 00:40:53,560
 or replace every piece once we put it

962
00:40:53,560 --> 00:40:56,920
 on the board with a series of unit cubes in the same location

963
00:40:56,920 --> 00:40:58,960
 to make things like line clearing easier

964
00:40:58,960 --> 00:41:01,280
 are things that we can't do without breaking out

965
00:41:01,280 --> 00:41:03,280
 of the simulation.

966
00:41:03,280 --> 00:41:05,560
 Once we have that randomly generated piece,

967
00:41:05,560 --> 00:41:07,400
 we use a convolutional neural network

968
00:41:07,400 --> 00:41:10,560
 to detect which piece we've teleported in.

969
00:41:10,560 --> 00:41:14,680
 And we use a mobile net CNN in order as our base network.

970
00:41:14,680 --> 00:41:17,040
 And we train a couple of layers on top of that

971
00:41:17,040 --> 00:41:18,640
 to determine which of the seven pieces

972
00:41:18,640 --> 00:41:20,280
 we've randomly generated.

973
00:41:20,280 --> 00:41:22,040
 Once we've figured out what piece we have,

974
00:41:22,040 --> 00:41:24,080
 we can then use our gameplay algorithm, which

975
00:41:24,080 --> 00:41:26,680
 will basically determine what the optimal position is

976
00:41:26,680 --> 00:41:28,280
 to place that piece on the board.

977
00:41:28,280 --> 00:41:32,840
 And that's using heuristics like the height of the board

978
00:41:32,840 --> 00:41:34,760
 after any line clears after that placement

979
00:41:34,760 --> 00:41:37,880
 or the actual placement on the board of that piece,

980
00:41:37,880 --> 00:41:39,840
 like how low that piece can go.

981
00:41:39,840 --> 00:41:42,120
 And these are heuristics that are used by players that

982
00:41:42,120 --> 00:41:43,040
 are playing Tetris.

983
00:41:43,040 --> 00:41:45,160
 And we felt that they would be effective at teaching

984
00:41:45,160 --> 00:41:47,360
 the robot how to play Tetris effectively.

985
00:41:47,360 --> 00:41:50,280
 For the final two steps, we go a bit more into detail

986
00:41:50,280 --> 00:41:52,160
 in the next two slides.

987
00:41:52,160 --> 00:41:55,080
 So for picking and placing, we plan

988
00:41:55,080 --> 00:41:57,400
 a trajectory for each piece.

989
00:41:57,400 --> 00:42:00,040
 So what that does is it composes four key poses

990
00:42:00,040 --> 00:42:01,840
 to construct this trajectory, which

991
00:42:01,840 --> 00:42:05,400
 are the initial pose here, a grasp pose, an intermediate

992
00:42:05,400 --> 00:42:07,200
 pose, and finally a drop pose.

993
00:42:07,200 --> 00:42:09,240
 The grasp pose is fixed for all the pieces

994
00:42:09,240 --> 00:42:10,960
 and is able to pick all of them up.

995
00:42:10,960 --> 00:42:12,800
 And the drop pose incorporates information

996
00:42:12,800 --> 00:42:14,160
 from the gameplay algorithm, which

997
00:42:14,160 --> 00:42:17,680
 is the rotation of the piece, the desired row and column

998
00:42:17,680 --> 00:42:19,760
 to drop it in.

999
00:42:19,760 --> 00:42:22,200
 After dropping the piece, we update the board state.

1000
00:42:22,200 --> 00:42:24,160
 So what that involves is populating cubes

1001
00:42:24,160 --> 00:42:26,000
 in the location of that piece.

1002
00:42:26,000 --> 00:42:29,000
 And also, if any rows are full, we clear that row.

1003
00:42:29,000 --> 00:42:34,040
 And in these images, we see that before and after a row clear.

1004
00:42:34,040 --> 00:42:37,200
 Here's a demo of our Tetris bot in action.

1005
00:42:37,200 --> 00:42:39,280
 We can see that we have a successful end-to-end--

1006
00:42:39,280 --> 00:42:42,720
 [AUDIO OUT]

1007
00:42:42,720 --> 00:42:43,760
 Oh, there's three of you.

1008
00:42:43,760 --> 00:42:45,440
 I get the wrong time.

1009
00:42:45,440 --> 00:42:46,560
 You guys get in the middle.

1010
00:42:46,560 --> 00:42:48,880
 I'm sorry.

1011
00:42:48,880 --> 00:42:49,600
 --that row.

1012
00:42:49,600 --> 00:42:54,240
 And here's a demo of our Tetris bot in action.

1013
00:42:54,240 --> 00:42:56,320
 We can see that we have a successful end-to-end--

1014
00:42:56,320 --> 00:43:04,520
 [AUDIO OUT]

1015
00:43:04,520 --> 00:43:06,560
 --grasping and placing of each block.

1016
00:43:06,560 --> 00:43:09,440
 And each one lands where desired on the board.

1017
00:43:09,440 --> 00:43:12,320
 We can see that all the pieces are being identified correctly

1018
00:43:12,320 --> 00:43:13,240
 and placed correctly.

1019
00:43:13,240 --> 00:43:16,840
 And the gameplay is pretty reasonable.

1020
00:43:16,840 --> 00:43:17,680
 Yeah.

1021
00:43:17,680 --> 00:43:19,760
 And so as we were working on this project,

1022
00:43:19,760 --> 00:43:21,160
 there were a couple of extensions

1023
00:43:21,160 --> 00:43:22,400
 that we were thinking about.

1024
00:43:22,400 --> 00:43:26,720
 So obviously, we want to make a more robust, more optimal

1025
00:43:26,720 --> 00:43:28,840
 gameplay algorithm using elements such as reinforcement

1026
00:43:28,840 --> 00:43:29,760
 learning.

1027
00:43:29,760 --> 00:43:31,920
 Hopefully, we thought it would be interesting to make

1028
00:43:31,920 --> 00:43:33,680
 a multiplayer version of Tetris that

1029
00:43:33,680 --> 00:43:35,760
 has multiple EWAs playing against each other

1030
00:43:35,760 --> 00:43:37,040
 on the same board.

1031
00:43:37,040 --> 00:43:39,680
 And also, we'd like to make a more true-to-life board that

1032
00:43:39,680 --> 00:43:43,200
 has either a vertical or slanted board so that we can utilize

1033
00:43:43,200 --> 00:43:46,880
 that aspect of gravity and simulate actual gameplay

1034
00:43:46,880 --> 00:43:49,080
 with increasing gravity, perhaps.

1035
00:43:49,080 --> 00:43:52,080
 Thank you.

1036
00:43:52,080 --> 00:43:56,040
 [APPLAUSE]

1037
00:43:56,040 --> 00:43:57,440
 Questions for the Tetris folks?

1038
00:43:57,440 --> 00:44:05,320
 We need to now simulate exploding blocks,

1039
00:44:05,320 --> 00:44:07,920
 some fracture mechanics or something like this.

1040
00:44:07,920 --> 00:44:08,440
 Yeah?

1041
00:44:08,440 --> 00:44:09,160
 That's awesome.

1042
00:44:09,160 --> 00:44:14,360
 So what was the hardest-- what was the most surprising part

1043
00:44:14,360 --> 00:44:15,600
 of that whole pipeline?

1044
00:44:15,600 --> 00:44:17,040
 What was the part that you thought

1045
00:44:17,040 --> 00:44:19,240
 was going to work better that didn't actually work?

1046
00:44:19,240 --> 00:44:42,720
 [INAUDIBLE]

1047
00:44:42,720 --> 00:44:43,220
 Super nice.

1048
00:44:44,100 --> 00:44:56,740
 [INAUDIBLE]

1049
00:44:56,740 --> 00:44:58,740
 Interesting.

1050
00:44:58,740 --> 00:44:59,980
 We could speed that part up.

1051
00:44:59,980 --> 00:45:01,220
 But that's good.

1052
00:45:01,220 --> 00:45:02,020
 That's good.

1053
00:45:02,020 --> 00:45:03,220
 No, that's super insightful.

1054
00:45:03,220 --> 00:45:03,720
 I like that.

1055
00:45:03,720 --> 00:45:10,100
 Hi, this is Michael.

1056
00:45:10,100 --> 00:45:11,340
 And this is Nico.

1057
00:45:11,340 --> 00:45:15,380
 For our final project, we tried to get the IWA to skip a rock.

1058
00:45:15,380 --> 00:45:18,140
 Rock skipping is a relatively simple dynamic task

1059
00:45:18,140 --> 00:45:21,260
 that can be done by many people, even the kid in this picture.

1060
00:45:21,260 --> 00:45:22,720
 We thought it would be pretty cool

1061
00:45:22,720 --> 00:45:25,900
 to see if we could get a robot to do the same thing.

1062
00:45:25,900 --> 00:45:27,300
 The first challenge of our project

1063
00:45:27,300 --> 00:45:29,500
 was to model the dynamics of skipping.

1064
00:45:29,500 --> 00:45:31,820
 Skipping occurs when a rock collides with water

1065
00:45:31,820 --> 00:45:34,700
 and a surface force propels the rock upwards.

1066
00:45:34,700 --> 00:45:37,380
 We modeled this collision using a drag force outlined

1067
00:45:37,380 --> 00:45:38,900
 in a previous paper.

1068
00:45:38,900 --> 00:45:41,420
 This force is directly proportional to the contact

1069
00:45:41,420 --> 00:45:45,580
 area of the rock and favors a flat geometry and velocity.

1070
00:45:45,580 --> 00:45:48,900
 This is intuitive for how we understand skipping.

1071
00:45:48,900 --> 00:45:51,540
 For our simulation setup, we have the IWA arm positioned

1072
00:45:51,540 --> 00:45:53,660
 next to the water surface and table.

1073
00:45:53,660 --> 00:45:56,220
 Based on the dynamics, we chose a flat hockey puck shape

1074
00:45:56,220 --> 00:45:58,740
 for our rock and centered it on top of the table.

1075
00:45:58,740 --> 00:46:00,740
 Behind the scenes, we implemented a force system

1076
00:46:00,740 --> 00:46:03,740
 to apply the spatial force from the water.

1077
00:46:03,740 --> 00:46:05,420
 Here is the general flow of our system

1078
00:46:05,420 --> 00:46:08,140
 where we specify parameters, such as the known rock

1079
00:46:08,140 --> 00:46:11,180
 location, desired throw velocity, and release height.

1080
00:46:11,180 --> 00:46:13,220
 We then manipulate the arm to pick up the rock

1081
00:46:13,220 --> 00:46:15,340
 and throw it with the desired parameters.

1082
00:46:15,340 --> 00:46:18,740
 Lastly, we simulate the skipping dynamics.

1083
00:46:18,740 --> 00:46:20,460
 Our setup consists of a state machine

1084
00:46:20,460 --> 00:46:22,580
 to switch how we command the arm.

1085
00:46:22,580 --> 00:46:23,960
 For each of these states, we have

1086
00:46:23,960 --> 00:46:27,460
 a different underlying control method, as shown.

1087
00:46:27,460 --> 00:46:30,540
 For the pickup state, we utilize simple kinematic planning

1088
00:46:30,540 --> 00:46:32,540
 and differential inverse kinematics

1089
00:46:32,540 --> 00:46:34,820
 to drag the rock to the edge of the table.

1090
00:46:34,820 --> 00:46:37,380
 This enables us to get an antipodal grasp on the rock,

1091
00:46:37,380 --> 00:46:39,060
 as shown in the video.

1092
00:46:39,060 --> 00:46:41,100
 Once you pick up the rock, we load the rock

1093
00:46:41,100 --> 00:46:42,860
 to an ideal initial throwing position.

1094
00:46:42,860 --> 00:46:49,220
 We initially thought to try a similar approach

1095
00:46:49,220 --> 00:46:51,620
 with kinematic throwing by creating radial poses,

1096
00:46:51,620 --> 00:46:52,980
 as seen in the picture.

1097
00:46:52,980 --> 00:46:55,100
 But we were unable to throw the rock any faster

1098
00:46:55,100 --> 00:46:56,940
 than 6 meters per second.

1099
00:46:56,940 --> 00:47:00,420
 This occurred since differential inverse kinematics only

1100
00:47:00,420 --> 00:47:02,660
 utilizes the very next pose, which

1101
00:47:02,660 --> 00:47:05,460
 would lead to unadvantageous joint positions.

1102
00:47:05,460 --> 00:47:07,740
 To overcome this, we switched to kinematic trajectory

1103
00:47:07,740 --> 00:47:10,180
 optimization.

1104
00:47:10,180 --> 00:47:13,460
 We knew we wanted a similar radial throwing trajectory.

1105
00:47:13,460 --> 00:47:15,180
 Specifically, we wanted the trajectory

1106
00:47:15,180 --> 00:47:17,460
 to pass through a release pose and a final pose,

1107
00:47:17,460 --> 00:47:18,940
 as shown in the image.

1108
00:47:18,940 --> 00:47:21,580
 Since the release of the rock matters the most when throwing,

1109
00:47:21,580 --> 00:47:23,780
 we added orientation and velocity constraints

1110
00:47:23,780 --> 00:47:25,180
 at this point.

1111
00:47:25,180 --> 00:47:27,340
 In the image, you can see these poses, as well as

1112
00:47:27,340 --> 00:47:31,500
 the yellow trajectory line created from the optimization.

1113
00:47:31,500 --> 00:47:33,820
 Here is a video of the arm throwing the rock.

1114
00:47:34,820 --> 00:47:38,620
 And here is a video of the rock skipping after being thrown.

1115
00:47:38,620 --> 00:47:44,940
 We ran our implementation over a desired set

1116
00:47:44,940 --> 00:47:46,740
 of throwing velocities.

1117
00:47:46,740 --> 00:47:47,980
 The throws were not perfect.

1118
00:47:47,980 --> 00:47:49,980
 The actual release velocity of the rock

1119
00:47:49,980 --> 00:47:51,820
 was much less than expected.

1120
00:47:51,820 --> 00:47:54,220
 This is likely due to the rock colliding with the gripper

1121
00:47:54,220 --> 00:47:56,340
 on an imperfect release.

1122
00:47:56,340 --> 00:47:57,940
 We just used the two-finger gripper,

1123
00:47:57,940 --> 00:48:00,700
 which is not necessarily optimal for skipping a rock.

1124
00:48:00,700 --> 00:48:02,940
 However, we still did get the rock to skip,

1125
00:48:02,940 --> 00:48:06,740
 and saw that higher velocities led to more skips.

1126
00:48:06,740 --> 00:48:09,060
 We can also see the contact force decreases

1127
00:48:09,060 --> 00:48:11,340
 with each impact.

1128
00:48:11,340 --> 00:48:15,020
 After many long days, we actually did get it to skip.

1129
00:48:15,020 --> 00:48:18,140
 However, we ran into a lot of problems along the way.

1130
00:48:18,140 --> 00:48:20,660
 We saw from our results earlier that rock velocities

1131
00:48:20,660 --> 00:48:22,860
 below 10 meters per second would not skip.

1132
00:48:22,860 --> 00:48:26,900
 But we kept running into problems

1133
00:48:26,900 --> 00:48:28,940
 where the rock would fly out of the gripper

1134
00:48:28,940 --> 00:48:31,940
 at desired velocities over 25 meters per second.

1135
00:48:31,940 --> 00:48:33,540
 We tried a few things to prevent this,

1136
00:48:33,540 --> 00:48:34,980
 but none of them helped.

1137
00:48:34,980 --> 00:48:38,140
 We tried different grasps, increasing grip force,

1138
00:48:38,140 --> 00:48:42,260
 decreasing the simulation DT, and even increasing friction.

1139
00:48:42,260 --> 00:48:43,420
 Still, none of this helped.

1140
00:48:43,420 --> 00:48:50,060
 And we saw the rock slip out as shown.

1141
00:48:50,060 --> 00:48:51,620
 Outside of problems with the gripper,

1142
00:48:51,620 --> 00:48:53,060
 we had some problems planning.

1143
00:48:53,060 --> 00:48:55,140
 Originally, we hoped to test how release height

1144
00:48:55,140 --> 00:48:56,580
 may affect skipping.

1145
00:48:56,580 --> 00:48:58,820
 The dynamics would tell us that a lower release height

1146
00:48:58,820 --> 00:49:01,220
 is best, because this leads to a faster release

1147
00:49:01,220 --> 00:49:03,940
 height, because this leads to a flatter velocity.

1148
00:49:03,940 --> 00:49:07,180
 But our trajectory optimizer had a hard time solving

1149
00:49:07,180 --> 00:49:08,540
 for various release heights.

1150
00:49:08,540 --> 00:49:17,940
 These are the guys that we're asking on Piazza constantly,

1151
00:49:17,940 --> 00:49:21,620
 like, how do I remove every possible limit from the IWA

1152
00:49:21,620 --> 00:49:23,140
 model, right?

1153
00:49:23,140 --> 00:49:24,340
 I don't want force limits.

1154
00:49:24,340 --> 00:49:25,700
 I don't want acceleration limits.

1155
00:49:25,700 --> 00:49:27,300
 I don't want torque limits.

1156
00:49:27,300 --> 00:49:29,660
 So what was your biggest surprise in the end?

1157
00:49:29,980 --> 00:49:32,940
 Honestly, I think we got really, really deep.

1158
00:49:32,940 --> 00:49:35,940
 [INAUDIBLE]

1159
00:49:35,940 --> 00:49:36,940
 Yeah.

1160
00:49:36,940 --> 00:49:40,420
 [INAUDIBLE]

1161
00:49:40,420 --> 00:49:45,900
 That's robotics.

1162
00:49:45,900 --> 00:49:46,380
 Yeah.

1163
00:49:46,380 --> 00:49:47,380
 Hey.

1164
00:49:47,380 --> 00:49:50,380
 [INAUDIBLE]

1165
00:49:50,380 --> 00:49:53,860
 [INAUDIBLE]

1166
00:49:54,860 --> 00:49:57,860
 [INAUDIBLE]

1167
00:49:57,860 --> 00:50:23,340
 I think IWA is not meant for throwing at high speeds.

1168
00:50:23,340 --> 00:50:25,220
 But awesome.

1169
00:50:25,220 --> 00:50:33,380
 Design choices for dual-arm robotic manipulator control.

1170
00:50:33,380 --> 00:50:36,060
 I'm Alex, and I'm working with Marcel and Idan.

1171
00:50:36,060 --> 00:50:37,700
 So why dual-arm robots?

1172
00:50:37,700 --> 00:50:39,860
 You have access to a bunch of more diverse tasks

1173
00:50:39,860 --> 00:50:41,860
 that require cooperation between both arms,

1174
00:50:41,860 --> 00:50:43,420
 as well as increased throughput.

1175
00:50:43,420 --> 00:50:46,260
 The challenge is you have more complexity

1176
00:50:46,260 --> 00:50:47,960
 having twice as many degrees of freedom,

1177
00:50:47,960 --> 00:50:50,460
 plus you have to plan around the other arm.

1178
00:50:50,460 --> 00:50:52,040
 So there are three possible approaches

1179
00:50:52,040 --> 00:50:53,040
 to tackling this problem.

1180
00:50:53,040 --> 00:50:55,460
 You have two independent controllers, one for each arm,

1181
00:50:55,460 --> 00:50:56,940
 two independent controllers with a communication

1182
00:50:56,940 --> 00:50:59,980
 channel between the two arms, and a single unified controller.

1183
00:50:59,980 --> 00:51:02,580
 So here's a demo of two independent controllers.

1184
00:51:02,580 --> 00:51:06,020
 Both controllers don't have any notion

1185
00:51:06,020 --> 00:51:07,660
 of what the other arm is doing.

1186
00:51:07,660 --> 00:51:10,220
 They're working completely independently and asynchronously.

1187
00:51:10,220 --> 00:51:12,700
 So one arm picks up objects from one side of the bin,

1188
00:51:12,700 --> 00:51:16,100
 and the other picks up objects in the other side of the bin

1189
00:51:16,100 --> 00:51:18,580
 and places them into the target bin.

1190
00:51:18,580 --> 00:51:21,940
 So here is a more detailed diagram of how we did this.

1191
00:51:21,940 --> 00:51:23,980
 We geofenced the robots.

1192
00:51:23,980 --> 00:51:28,300
 So the robot on the left picks up an object

1193
00:51:28,300 --> 00:51:31,580
 from the bottom half of the bin, goes to clearance frame one,

1194
00:51:31,580 --> 00:51:33,240
 and then from clearance frame one

1195
00:51:33,240 --> 00:51:35,860
 follows a fixed path to the drop-off location.

1196
00:51:35,860 --> 00:51:39,180
 Now the robot on the right picks up objects from the top

1197
00:51:39,180 --> 00:51:42,140
 and goes to clearance frame two, and then

1198
00:51:42,140 --> 00:51:44,140
 follows a fixed path down to the drop-off location.

1199
00:51:44,140 --> 00:51:48,980
 The advantages of this method is that it's

1200
00:51:48,980 --> 00:51:51,420
 very simple to implement, and it works completely

1201
00:51:51,420 --> 00:51:52,800
 asynchronously, so you don't have

1202
00:51:52,800 --> 00:51:54,260
 to wait for the other robot.

1203
00:51:54,260 --> 00:51:56,140
 But the disadvantages are that you

1204
00:51:56,140 --> 00:52:01,460
 have to manually define a fixed path for each new scenario,

1205
00:52:01,460 --> 00:52:04,180
 as well as the path is not ideal because you

1206
00:52:04,180 --> 00:52:06,820
 have to move out of the way to make the clearance requirements

1207
00:52:06,820 --> 00:52:09,660
 to not hit the other robot.

1208
00:52:09,660 --> 00:52:11,300
 And one failure modality is that there

1209
00:52:11,300 --> 00:52:15,180
 is a grasp boundary where no robot can get to,

1210
00:52:15,180 --> 00:52:19,380
 because the grippers have some finite width,

1211
00:52:19,380 --> 00:52:21,340
 making it so that some objects may not

1212
00:52:21,340 --> 00:52:24,740
 be grasped if they're right on the grasp boundary.

1213
00:52:24,740 --> 00:52:26,940
 The next approach, we kept the controllers

1214
00:52:26,940 --> 00:52:29,500
 separate from each other, but added a communication channel

1215
00:52:29,500 --> 00:52:31,020
 between them.

1216
00:52:31,020 --> 00:52:33,140
 To study this approach, we took the task

1217
00:52:33,140 --> 00:52:36,060
 of passing an object from one robotic task to another

1218
00:52:36,060 --> 00:52:38,300
 without putting it on the ground.

1219
00:52:38,300 --> 00:52:40,260
 The communication channel carries

1220
00:52:40,260 --> 00:52:42,500
 relatively simple information.

1221
00:52:42,500 --> 00:52:45,500
 After the first arm grasps the object

1222
00:52:45,500 --> 00:52:47,260
 and carries it to the location, it

1223
00:52:47,260 --> 00:52:50,140
 sends a message to the other arm that it's ready

1224
00:52:50,140 --> 00:52:51,340
 and its location.

1225
00:52:51,340 --> 00:52:54,420
 The other arm grasps the object and then sends a message

1226
00:52:54,420 --> 00:52:56,700
 that the first arm should release it.

1227
00:52:56,700 --> 00:52:58,420
 The main technical challenge here

1228
00:52:58,420 --> 00:53:02,540
 is finding a good grasping pose in the air.

1229
00:53:02,540 --> 00:53:04,980
 Unlike the algorithm that was taught in the class,

1230
00:53:04,980 --> 00:53:08,180
 here the point cloud includes both the object and the gripper,

1231
00:53:08,180 --> 00:53:10,940
 and we needed to separate them before coming up

1232
00:53:10,940 --> 00:53:14,540
 with a good antipodal grasp proposal.

1233
00:53:14,540 --> 00:53:19,180
 Overall, this approach allows us to have simple controllers,

1234
00:53:19,180 --> 00:53:21,820
 but that still have some cooperation between them.

1235
00:53:21,820 --> 00:53:24,620
 On the other hand, it's good only

1236
00:53:24,620 --> 00:53:26,340
 when the chance of collision is low

1237
00:53:26,340 --> 00:53:29,580
 and is not suitable for every task.

1238
00:53:29,580 --> 00:53:32,140
 The previous methods could solve a limited number of tasks

1239
00:53:32,140 --> 00:53:33,700
 and were not generalizable.

1240
00:53:33,700 --> 00:53:36,420
 Next, we'll tackle this problem by designing a single controller

1241
00:53:36,420 --> 00:53:39,820
 and proposing a new algorithm for path planning optimization

1242
00:53:39,820 --> 00:53:41,460
 for both arms at the same time.

1243
00:53:41,460 --> 00:53:43,980
 In this case, the benchmark consists in picking and placing

1244
00:53:43,980 --> 00:53:46,020
 multiple objects from one bin to the other.

1245
00:53:46,020 --> 00:53:47,740
 This task becomes hard because the trajectories

1246
00:53:47,740 --> 00:53:50,460
 between both arms are constantly crossing each other.

1247
00:53:50,460 --> 00:53:52,980
 In the video, you can see we successfully achieved this task,

1248
00:53:52,980 --> 00:53:54,860
 but let's look at how we solved it.

1249
00:53:54,860 --> 00:53:56,740
 The first idea was to design a state machine

1250
00:53:56,740 --> 00:53:58,820
 that forces synchronization on both arms,

1251
00:53:58,820 --> 00:54:01,020
 meaning that each arm will have to wait for the other

1252
00:54:01,020 --> 00:54:04,420
 to finish its subtask before starting the next sequence.

1253
00:54:04,420 --> 00:54:07,500
 The second idea consisted in designing two collision-free paths

1254
00:54:07,500 --> 00:54:08,860
 from one bin to the other.

1255
00:54:08,860 --> 00:54:11,620
 We propose a novel algorithm that consists in recursively

1256
00:54:11,620 --> 00:54:13,500
 finding two frames, one for each arm,

1257
00:54:13,500 --> 00:54:15,100
 so that they are not colliding,

1258
00:54:15,100 --> 00:54:17,500
 as we can see in the visualization.

1259
00:54:17,500 --> 00:54:19,780
 The main advantage of using a single controller

1260
00:54:19,780 --> 00:54:21,900
 is that now we can achieve any feasible task,

1261
00:54:21,900 --> 00:54:23,260
 and it is space efficient.

1262
00:54:23,260 --> 00:54:25,180
 However, there are some disadvantages too.

1263
00:54:25,180 --> 00:54:26,940
 The controller becomes harder to design,

1264
00:54:26,940 --> 00:54:28,580
 the optimization becomes harder too,

1265
00:54:28,580 --> 00:54:32,100
 and synchronous movements might slow down the time to success.

1266
00:54:32,100 --> 00:54:35,140
 Nevertheless, we have to say that this method is still not perfect,

1267
00:54:35,140 --> 00:54:36,660
 and we find two main issues.

1268
00:54:36,660 --> 00:54:38,820
 First is that we only constrain the end effector

1269
00:54:38,820 --> 00:54:40,100
 positioned to not collide,

1270
00:54:40,100 --> 00:54:42,620
 so we can still have collisions on the rest of the arm.

1271
00:54:42,620 --> 00:54:44,620
 Second, the arm gets sometimes tangled with itself

1272
00:54:44,620 --> 00:54:46,500
 due to the differential IK solver.

1273
00:54:46,500 --> 00:54:48,340
 The solution for both of these problems

1274
00:54:48,340 --> 00:54:50,700
 would be to add constraints and optimize in joint space

1275
00:54:50,700 --> 00:54:52,340
 instead of end effector space.

1276
00:54:52,340 --> 00:54:54,780
 Our algorithm could perfectly incorporate this improvement

1277
00:54:54,780 --> 00:54:56,740
 and we leave it as future work.

1278
00:54:56,740 --> 00:54:58,220
 To conclude, in our project,

1279
00:54:58,220 --> 00:55:01,820
 we presented three different design choices of controllers of dual-arm robots.

1280
00:55:01,820 --> 00:55:03,980
 Independent controllers are similar to program,

1281
00:55:03,980 --> 00:55:06,900
 they can work asynchronously, but are space and time inefficient.

1282
00:55:06,900 --> 00:55:09,340
 Adding a communication channel allows to have more correlation

1283
00:55:09,340 --> 00:55:11,220
 between the arms and solve more tasks,

1284
00:55:11,220 --> 00:55:14,140
 but the number of tasks is still limited.

1285
00:55:14,140 --> 00:55:17,740
 Finally, with a single controller, we should be able to solve any task,

1286
00:55:17,740 --> 00:55:19,900
 but the optimization problem becomes harder

1287
00:55:19,900 --> 00:55:21,740
 due to the curse of dimensionality.

1288
00:55:21,740 --> 00:55:23,740
 Thank you for listening.

1289
00:55:23,740 --> 00:55:30,940
 Okay, so what surprised you?

1290
00:55:30,940 --> 00:55:37,740
 I was surprised that the two-by-two space didn't work as well as we thought.

1291
00:55:37,740 --> 00:55:38,620
 Yeah.

1292
00:55:38,620 --> 00:55:48,620
 [inaudible]

1293
00:55:48,620 --> 00:55:52,620
 Did you use the diff IK or did you use the pseudo-inverse Jacobian?

1294
00:55:52,620 --> 00:55:53,620
 [inaudible]

1295
00:55:53,620 --> 00:55:54,620
 Yeah, yeah.

1296
00:55:54,620 --> 00:55:57,620
 Good, okay.

1297
00:55:57,620 --> 00:55:58,620
 Any other questions?

1298
00:55:58,620 --> 00:56:03,620
 I do think dual-arm planning is still hard.

1299
00:56:03,620 --> 00:56:07,420
 People, most, if you see a dual-arm demo in the world,

1300
00:56:07,420 --> 00:56:10,420
 there's typically some hacks that make it work.

1301
00:56:10,420 --> 00:56:12,420
 It's still hard to solve the real problem,

1302
00:56:12,420 --> 00:56:14,420
 collision-free motion planning.

1303
00:56:14,420 --> 00:56:17,420
 All right.

1304
00:56:17,420 --> 00:56:25,420
 Hi, I'm Pixi, and alongside me is Hanshi and Arif.

1305
00:56:25,420 --> 00:56:27,420
 Today, we're going to discuss our project,

1306
00:56:27,420 --> 00:56:31,420
 which is a physics-based throwing using inverse dynamics control.

1307
00:56:31,420 --> 00:56:35,420
 So the motivation for our work stems from the fact

1308
00:56:35,420 --> 00:56:38,420
 that throwing objects is a very important skill for robots,

1309
00:56:38,420 --> 00:56:40,420
 as it is a prerequisite for many more complex

1310
00:56:40,420 --> 00:56:42,420
 and dynamic robotic manipulation tasks.

1311
00:56:42,420 --> 00:56:45,420
 Throwing can increase the efficiency of manipulation

1312
00:56:45,420 --> 00:56:47,420
 as well as expand the robot's workspace.

1313
00:56:47,420 --> 00:56:53,420
 So our goal is to construct a method for robots

1314
00:56:53,420 --> 00:56:56,420
 to successfully throw objects between one another.

1315
00:56:56,420 --> 00:56:59,420
 This problem involves solving the problems of perception,

1316
00:56:59,420 --> 00:57:02,420
 grasping, trajectory planning, inverse kinematics,

1317
00:57:02,420 --> 00:57:04,420
 as well as inverse dynamics control.

1318
00:57:05,420 --> 00:57:09,420
 And here's an example of our system and work.

1319
00:57:09,420 --> 00:57:15,420
 So first, the robots select the grasp, execute the grasp,

1320
00:57:15,420 --> 00:57:20,420
 and then it proceeds to plan a trajectory to execute the throw.

1321
00:57:20,420 --> 00:57:23,420
 As you can see, this is the trajectory that they're following,

1322
00:57:23,420 --> 00:57:25,420
 that it has planned in real time.

1323
00:57:25,420 --> 00:57:33,420
 And then this throw is executed, and this time it was successful.

1324
00:57:33,420 --> 00:57:37,420
 And then this process keeps repeating itself over and over again.

1325
00:57:37,420 --> 00:57:52,420
 As you can see, it is not always successful,

1326
00:57:52,420 --> 00:57:55,420
 but we'll go more into detail later why.

1327
00:57:55,420 --> 00:58:00,420
 So first, let's talk about the system design.

1328
00:58:00,420 --> 00:58:03,420
 At a high level, there is a task planner or a state machine.

1329
00:58:03,420 --> 00:58:06,420
 And in this, it's broken up into different states.

1330
00:58:06,420 --> 00:58:10,420
 The first state is perception and grasp selection,

1331
00:58:10,420 --> 00:58:13,420
 where it determines grasp, executes those grasp,

1332
00:58:13,420 --> 00:58:16,420
 and then it plans a throwing trajectory.

1333
00:58:16,420 --> 00:58:21,420
 These trajectories are then fed into an inverse kinematics solver

1334
00:58:21,420 --> 00:58:23,420
 to convert 3D poses into joint angles,

1335
00:58:23,420 --> 00:58:26,420
 in which the joint angles are then fed into an inverse dynamics controller

1336
00:58:26,420 --> 00:58:28,420
 to achieve precise trajectory control.

1337
00:58:29,420 --> 00:58:32,420
 So first off, starting with the perception and grasp planning,

1338
00:58:32,420 --> 00:58:36,420
 point clouds are sampled from cameras and then combined and downsampled.

1339
00:58:36,420 --> 00:58:38,420
 From this downsampled combined point cloud,

1340
00:58:38,420 --> 00:58:42,420
 normals are obtained and oriented towards the camera.

1341
00:58:42,420 --> 00:58:48,420
 With this, grasp can be selected and scored,

1342
00:58:48,420 --> 00:58:51,420
 and then the best quality grasp is then taken

1343
00:58:51,420 --> 00:58:56,420
 as the final grasp pose position for the robot to execute towards.

1344
00:58:57,420 --> 00:59:01,420
 For trajectory planning, the complete throwing trajectory consists of three parts.

1345
00:59:01,420 --> 00:59:05,420
 The rotation part, the linear movement part, and the circular motion part.

1346
00:59:05,420 --> 00:59:09,420
 The robot arm will rotate to the oriented patient direction first,

1347
00:59:09,420 --> 00:59:14,420
 and then it will linearly move to the starting position of the throwing trajectory.

1348
00:59:14,420 --> 00:59:17,420
 And then we will generate the last segment of the throwing trajectory

1349
00:59:17,420 --> 00:59:21,420
 using our self-designed circular throwing trajectory method.

1350
00:59:22,420 --> 00:59:26,420
 And for the projected problem analysis,

1351
00:59:26,420 --> 00:59:30,420
 since our robots are throwing the objects in the 3D world,

1352
00:59:30,420 --> 00:59:34,420
 we can reduce this throwing behavior into two-dimensional problems.

1353
00:59:34,420 --> 00:59:36,420
 As you can see in the right figure,

1354
00:59:36,420 --> 00:59:38,420
 when the robot is heading towards the target point,

1355
00:59:38,420 --> 00:59:42,420
 we can set the throwing trajectory into the direction of the target point.

1356
00:59:42,420 --> 00:59:46,420
 And for the trajectory planning,

1357
00:59:46,420 --> 00:59:48,420
 we use the minimized energy consumption trajectory,

1358
00:59:48,420 --> 00:59:51,420
 which is a circular arc trajectory design idea.

1359
00:59:51,420 --> 00:59:53,420
 Consider the left picture.

1360
00:59:53,420 --> 00:59:56,420
 Since we know the starting position of the throwing trajectory,

1361
00:59:56,420 --> 01:00:00,420
 and the core of our solution is to find the throwing position and velocity correctly,

1362
01:00:00,420 --> 01:00:04,420
 instead, we focus on the radius of the arc and the ledge angle.

1363
01:00:04,420 --> 01:00:07,420
 And the right side is the cost function and the related constraints.

1364
01:00:07,420 --> 01:00:13,420
 To achieve our precise control, we use the inverse time controller,

1365
01:00:13,420 --> 01:00:16,420
 because throwing involves large accelerations and velocities,

1366
01:00:16,420 --> 01:00:20,420
 and also the robot needs to achieve a certain velocity to achieve a good throw.

1367
01:00:20,420 --> 01:00:25,420
 So, naive controllers that are not aware of velocities and feedforward accelerations can lag behind.

1368
01:00:25,420 --> 01:00:28,420
 These controllers are differential inverse kinematics and position controllers.

1369
01:00:28,420 --> 01:00:31,420
 So, we need to incorporate velocity and acceleration,

1370
01:00:31,420 --> 01:00:34,420
 which motivates us to use the inverse dynamics controller,

1371
01:00:34,420 --> 01:00:37,420
 because we can use the known dynamics of the system.

1372
01:00:37,420 --> 01:00:43,420
 We use inverse kinematics to generate joint angles in joint space from trajectories in 3D,

1373
01:00:43,420 --> 01:00:47,420
 and then we use shape-preserving interpolation between these to generate a continuous trajectory.

1374
01:00:47,420 --> 01:00:53,420
 Then we can differentiate the trajectory to generate velocities and accelerations to feed into the inverse dynamics controller.

1375
01:00:53,420 --> 01:01:00,420
 One metric we can use to evaluate our controller is trajectory tracking performance.

1376
01:01:00,420 --> 01:01:04,420
 We use this because this is closely related to the throwing performance.

1377
01:01:04,420 --> 01:01:07,420
 On the left, we plot the tracking error over time,

1378
01:01:07,420 --> 01:01:12,420
 and we can see that the tracking error is highest near 21 seconds, which is when the throwing happens.

1379
01:01:12,420 --> 01:01:17,420
 We also plot the desired accelerations, which are commanded to the inverse dynamics controller over time.

1380
01:01:17,420 --> 01:01:22,420
 On the right, we see that high accelerations are the most common.

1381
01:01:22,420 --> 01:01:31,420
 Did you see the same sliding effect that rock skippers saw?

1382
01:01:31,420 --> 01:01:35,420
 Do you think when you were getting to high velocities?

1383
01:01:35,420 --> 01:01:46,420
 Yeah, especially when we did these high-speed dynamics to bring the unit to size,

1384
01:01:46,420 --> 01:01:54,420
 we can use our control system to make the acceleration scale.

1385
01:01:54,420 --> 01:01:59,420
 And we can vary it a lot of times to make it more efficient,

1386
01:01:59,420 --> 01:02:05,420
 but we can't do it all the time.

1387
01:02:05,420 --> 01:02:10,420
 So, we have to use a lot of time to make it more efficient.

1388
01:02:10,420 --> 01:02:17,420
 We can't do it all the time, but we can vary it a lot of times.

1389
01:02:17,420 --> 01:02:22,420
 That's the way we can get the maximum performance.

1390
01:02:22,420 --> 01:02:28,420
 [INAUDIBLE]

1391
01:02:28,420 --> 01:02:30,420
 Makes sense. Very nice.

1392
01:02:30,420 --> 01:02:34,420
 I feel like I need to find a high-ly end effector for everybody,

1393
01:02:34,420 --> 01:02:38,420
 or get some demos of an allegro hand throwing or something like that.

1394
01:02:38,420 --> 01:02:39,420
 Yeah?

1395
01:02:39,420 --> 01:02:55,420
 [INAUDIBLE]

1396
01:02:55,420 --> 01:02:57,420
 Did you guys look at the collision geometry?

1397
01:02:57,420 --> 01:03:02,420
 I guess maybe for the rock skipping and even the original, the first video throwing?

1398
01:03:02,420 --> 01:03:31,420
 [INAUDIBLE]

1399
01:03:31,420 --> 01:03:36,420
 I think on the collision geometry, I'm pretty sure that the model that everybody would have found

1400
01:03:36,420 --> 01:03:38,420
 is the one that has multiple collision points.

1401
01:03:38,420 --> 01:03:41,420
 I think when David took the class, I might have given out a collision model

1402
01:03:41,420 --> 01:03:44,420
 that had only one contact point per finger,

1403
01:03:44,420 --> 01:03:47,420
 and I had some regret about that, but I fixed it for this year.

1404
01:03:47,420 --> 01:03:50,420
 So, I think you guys must have had a few contact points,

1405
01:03:50,420 --> 01:03:54,420
 but maybe it could have been better.

1406
01:03:54,420 --> 01:03:58,420
 You guys need lots of friction and force.

1407
01:03:58,420 --> 01:04:00,420
 Hi, everyone. My name is McCoy Becker.

1408
01:04:00,420 --> 01:04:01,420
 I'll be--

1409
01:04:01,420 --> 01:04:15,420
 [AUDIO OUT]

1410
01:04:15,420 --> 01:04:17,420
 Hello. We are Kami, Raul, and Michael,

1411
01:04:17,420 --> 01:04:20,420
 and today we will be presenting our robotic manipulation final project,

1412
01:04:20,420 --> 01:04:25,420
 the teleoperation of the allegro hand via kinematic hand synergies in Drake.

1413
01:04:25,420 --> 01:04:30,420
 The motivation of our study came from the learning dexterity project conducted by OpenAI.

1414
01:04:30,420 --> 01:04:35,420
 They used reinforcement learning to learn how to reorient the cube using the dexterous shadow robot hand.

1415
01:04:35,420 --> 01:04:38,420
 However, we were curious if the problem could be simplified.

1416
01:04:38,420 --> 01:04:41,420
 Specifically, we hypothesized that knowledge of human hand motor control

1417
01:04:41,420 --> 01:04:45,420
 could lead to easier learning of dexterous manipulation via behavior cloning.

1418
01:04:45,420 --> 01:04:47,420
 But before we could test this hypothesis,

1419
01:04:47,420 --> 01:04:52,420
 we needed to develop a teleoperation system that can be used to study human hand manipulation.

1420
01:04:52,420 --> 01:04:54,420
 This ultimately became the goal of our project.

1421
01:04:54,420 --> 01:04:57,420
 Specifically, this goal was broken into three subgoals.

1422
01:04:57,420 --> 01:05:01,420
 The first subgoal was to teleoperate the allegro hand in a simulated environment.

1423
01:05:01,420 --> 01:05:05,420
 The second subgoal comes from the idea that while the hand has many degrees of freedom,

1424
01:05:05,420 --> 01:05:07,420
 humans rarely control each joint individually.

1425
01:05:07,420 --> 01:05:11,420
 Rather, they control many of the joints simultaneously in a coordinated fashion.

1426
01:05:11,420 --> 01:05:13,420
 This idea is known as a synergy.

1427
01:05:13,420 --> 01:05:20,420
 In fact, reducing the control degree of freedom can greatly simplify the control input required to learn a manipulation task.

1428
01:05:20,420 --> 01:05:25,420
 Thus, our second goal was to teleoperate the allegro hand using known kinematic hand synergies.

1429
01:05:25,420 --> 01:05:30,420
 And with those first two subgoals, our third subgoal was to grasp various objects using teleoperation of the hand

1430
01:05:30,420 --> 01:05:32,420
 with and without synergistic control.

1431
01:05:32,420 --> 01:05:38,420
 This work is important to the design of prosthetics, robotic rehabilitation devices, and dexterous manipulation.

1432
01:05:38,420 --> 01:05:46,420
 The environment we are controlling contains the allegro hand, the IWA arm, and various objects.

1433
01:05:46,420 --> 01:05:52,420
 Our system was designed such that the allegro hand and IWA were controlled based on separate pipelines.

1434
01:05:52,420 --> 01:05:59,420
 Starting from the hand, first the hand video was captured with a single, uncalibrated RGB camera.

1435
01:05:59,420 --> 01:06:06,420
 Using MediaPipe Hands, a software that can track hand kinematics, we obtained joint positions of the operator's hand.

1436
01:06:06,420 --> 01:06:10,420
 This is passed into the desired states of the IWA.

1437
01:06:10,420 --> 01:06:17,420
 Then we pass those states in the inverse dynamics controller to output torque for each of the controllable joints of the hand.

1438
01:06:17,420 --> 01:06:23,420
 Once this is passed into the plant, the simulator shows the allegro hand moving like this.

1439
01:06:23,420 --> 01:06:29,420
 On the IWA side, we came up with a set of poses that we wanted its end effector to follow.

1440
01:06:29,420 --> 01:06:34,420
 Using inverse kinematics, we find the joint positions of the arm.

1441
01:06:34,420 --> 01:06:40,420
 The same workflow as the hand is repeated to obtain the simulation of the arm's motion.

1442
01:06:40,420 --> 01:06:49,420
 With our teleoperation pipeline, we were able to accurately track the positions of our hand joints and command the allegro hand to follow our motions,

1443
01:06:49,420 --> 01:06:54,420
 composing itself into a peace sign, a fist, or other desired configurations.

1444
01:06:54,420 --> 01:07:00,420
 We controlled the system in real time, but our controller occasionally overshot its intended target before stabilizing.

1445
01:07:00,420 --> 01:07:07,420
 Each digit can be commanded individually, and inputs from the pinky are ignored because the allegro hand only has four fingers.

1446
01:07:07,420 --> 01:07:17,420
 We were also able to control the position and orientation of the hand by using sliders to command the IWA positions using an inverse kinematics solver,

1447
01:07:17,420 --> 01:07:20,420
 which we used during our grasping tests.

1448
01:07:27,420 --> 01:07:33,420
 Here we show that we were able to successfully implement teleoperation using kinematic hand synergies.

1449
01:07:33,420 --> 01:07:36,420
 We see the human teleoperating the hand to make a peace sign.

1450
01:07:36,420 --> 01:07:41,420
 However, in the case where there is one synergy, all the joints are moving in a coupled manner simultaneously.

1451
01:07:41,420 --> 01:07:48,420
 As we increase the number of synergies in the controller, we see that the allegro hand begins to look more like the peace sign that the human hand is prescribing.

1452
01:07:48,420 --> 01:07:54,420
 That is because as we add synergies, we are effectively adding more individual control degrees of freedom.

1453
01:07:55,420 --> 01:08:01,420
 Through teleoperation, we were able to maintain a firm grip on a cube resting in the palm of the allegro hand.

1454
01:08:01,420 --> 01:08:07,420
 However, without accurate force feedback from the simulation, we often relied on exaggerated movements to achieve our desired grip,

1455
01:08:07,420 --> 01:08:11,420
 fully forming a fist instead of a gentle grip on the cube, for example.

1456
01:08:11,420 --> 01:08:21,420
 Unfortunately, when we attempted to grasp objects placed on a table, we had serious difficulties forming a stable grip,

1457
01:08:21,420 --> 01:08:26,420
 and often inadvertently applied very large forces to our test objects when trying to grab them.

1458
01:08:26,420 --> 01:08:31,420
 This would cause the simulation to become unstable, sometimes to the point of crashing the system.

1459
01:08:31,420 --> 01:08:38,420
 We showed our implementation of teleoperating a multi-finger robotic hand in Drake.

1460
01:08:38,420 --> 01:08:42,420
 We used human demonstration captured by an RGB camera.

1461
01:08:42,420 --> 01:08:47,420
 We used synergies to simplify the degrees of freedom to control the robotic hand.

1462
01:08:47,420 --> 01:08:51,420
 Although some aspects of our implementation were unsuccessful,

1463
01:08:51,420 --> 01:08:57,420
 we think that this project paves the promising way forward to understand robotic control for dexterous manipulation.

1464
01:08:57,420 --> 01:09:01,420
 We want to improve our contact simulation to achieve stable graphs,

1465
01:09:01,420 --> 01:09:08,420
 and ultimately we want to apply this work to conduct behavior cloning of manipulation tasks based on human demonstration.

1466
01:09:15,420 --> 01:09:18,420
 I watched most of these this morning.

1467
01:09:18,420 --> 01:09:23,420
 But what was the thing, when the Allegro went unstable, and the simulation shot off,

1468
01:09:23,420 --> 01:09:25,420
 something flew into the real image.

1469
01:09:25,420 --> 01:09:27,420
 What was that?

1470
01:09:27,420 --> 01:09:30,420
 [LAUGHTER]

1471
01:09:30,420 --> 01:09:34,420
 There was like, someone threw a t-shirt through the camera or something.

1472
01:09:34,420 --> 01:09:40,420
 [LAUGHTER]

1473
01:09:40,420 --> 01:09:43,420
 So what do you think would be the thing that would give you more,

1474
01:09:43,420 --> 01:09:47,420
 you said, it sounded like you suggested feedback, force feedback would be necessary.

1475
01:09:47,420 --> 01:09:48,420
 Do you think you could do less?

1476
01:09:48,420 --> 01:09:51,420
 Do you think you could just VR or?

1477
01:09:51,420 --> 01:10:04,420
 [INAUDIBLE]

1478
01:10:04,420 --> 01:10:06,420
 Ah.

1479
01:10:06,420 --> 01:10:07,420
 All right.

1480
01:10:07,420 --> 01:10:12,420
 [INAUDIBLE]

1481
01:10:12,420 --> 01:10:16,420
 Do you think the shadow hand would be better?

1482
01:10:16,420 --> 01:10:18,420
 The more dexterous hand?

1483
01:10:18,420 --> 01:10:19,420
 All right.

1484
01:10:19,420 --> 01:10:20,420
 Future work.

1485
01:10:20,420 --> 01:10:21,420
 Oh, good.

1486
01:10:21,420 --> 01:10:45,420
 [INAUDIBLE]

1487
01:10:45,420 --> 01:10:48,420
 And you guys have a happy glove to play with later, yeah?

1488
01:10:48,420 --> 01:10:54,420
 [INAUDIBLE]

1489
01:10:54,420 --> 01:10:56,420
 All right.

1490
01:10:56,420 --> 01:10:59,420
 Systems problems.

1491
01:10:59,420 --> 01:11:03,420
 OK, is Nikita here?

1492
01:11:03,420 --> 01:11:04,420
 Oh, yeah, OK, good.

1493
01:11:04,420 --> 01:11:10,420
 Hi, everyone.

1494
01:11:10,420 --> 01:11:11,420
 My name is Nikita.

1495
01:11:11,420 --> 01:11:13,420
 And in this talk, I would like to introduce my work on spades,

1496
01:11:13,420 --> 01:11:15,420
 reclaring robotic system.

1497
01:11:15,420 --> 01:11:16,420
 The problem is the following.

1498
01:11:16,420 --> 01:11:19,420
 Given a floor plan that is specified as a set of dashed lines,

1499
01:11:19,420 --> 01:11:22,420
 the system should be able to construct the building of certain height

1500
01:11:22,420 --> 01:11:25,420
 by layering bricks on top of each other according to the floor plan.

1501
01:11:25,420 --> 01:11:28,420
 There might be single or multiple robotic arms working together,

1502
01:11:28,420 --> 01:11:31,420
 and the number of bricks can be as high as hundreds.

1503
01:11:31,420 --> 01:11:34,420
 After parsing the floor plan and constructing poses for each brick,

1504
01:11:34,420 --> 01:11:37,420
 the first step is computing trajectories under a set of constraints

1505
01:11:37,420 --> 01:11:41,420
 that will guarantee gentle placement of the bricks, collision avoidance,

1506
01:11:41,420 --> 01:11:44,420
 and also avoidance of high velocities and unrealistic accelerations

1507
01:11:44,420 --> 01:11:46,420
 in the robot joints.

1508
01:11:46,420 --> 01:11:50,420
 As the first attempt, I tried using global kinematic trajectory optimization

1509
01:11:50,420 --> 01:11:53,420
 on joint positions to optimize end-to-end trajectories from the source

1510
01:11:53,420 --> 01:11:57,420
 to the destination with a large set of constraints shown below.

1511
01:11:57,420 --> 01:12:00,420
 Unfortunately, the resulting optimization problem turned out being very complex,

1512
01:12:00,420 --> 01:12:04,420
 and it was not able to cover most of the bricks.

1513
01:12:04,420 --> 01:12:06,420
 Then I decided to simplify the optimization problem

1514
01:12:06,420 --> 01:12:10,420
 by decoupling trajectory construction from solving inverse kinematics.

1515
01:12:10,420 --> 01:12:13,420
 I also split end-to-end trajectories into three regions,

1516
01:12:13,420 --> 01:12:15,420
 each with different constraints.

1517
01:12:15,420 --> 01:12:18,420
 For example, the grip and the move-return regions have relaxed constraints

1518
01:12:18,420 --> 01:12:22,420
 on the grip rotation, while the approach region strictly constrains

1519
01:12:22,420 --> 01:12:25,420
 both the poses and velocities to allow gentle placement of the bricks.

1520
01:12:25,420 --> 01:12:29,420
 The most challenging part here is planning the move-return trajectory,

1521
01:12:29,420 --> 01:12:32,420
 as this part of the trajectory should avoid collisions

1522
01:12:32,420 --> 01:12:34,420
 with the previously built walls and the robot itself.

1523
01:12:34,420 --> 01:12:39,420
 For this project, I simplified the problem by constraining the move-return zone

1524
01:12:39,420 --> 01:12:43,420
 to be always higher than the height of the destination brick,

1525
01:12:43,420 --> 01:12:48,420
 and I'm also using simple tangential trajectories in order to bypass the robot body.

1526
01:12:48,420 --> 01:12:52,420
 The constructed trajectories here are expressed in the grip poses,

1527
01:12:52,420 --> 01:12:55,420
 and they don't correspond to actual joint positions.

1528
01:12:55,420 --> 01:12:57,420
 So the next step is to solve inverse kinematics

1529
01:12:57,420 --> 01:12:59,420
 to define actual trajectories for the robot.

1530
01:12:59,420 --> 01:13:02,420
 The problem is formulated as joint centering optimization

1531
01:13:02,420 --> 01:13:05,420
 with a set of constraints based on the part of the trajectory from the previous slide.

1532
01:13:05,420 --> 01:13:09,420
 The algorithm for solving inverse kinematics for all bricks is shown on the left.

1533
01:13:09,420 --> 01:13:12,420
 In short, we construct several possible collision-free trajectories for each brick

1534
01:13:12,420 --> 01:13:14,420
 and interpolate them.

1535
01:13:14,420 --> 01:13:18,420
 Then we try to solve constraint inverse kinematics for each point of the trajectory,

1536
01:13:18,420 --> 01:13:21,420
 and if there is a trajectory for which inverse kinematics is solvable for every point,

1537
01:13:21,420 --> 01:13:22,420
 commit it.

1538
01:13:22,420 --> 01:13:25,420
 If there are multiple such trajectories, commit the shortest.

1539
01:13:25,420 --> 01:13:30,420
 This slide shows visualization of the coverage made with point clouds.

1540
01:13:30,420 --> 01:13:33,420
 Green clouds correspond to the reachable bricks, red to unreachable.

1541
01:13:33,420 --> 01:13:37,420
 The whole trajectory planning and inverse kinematic optimization can be solved offline,

1542
01:13:37,420 --> 01:13:40,420
 and the output can easily be the matrix,

1543
01:13:40,420 --> 01:13:43,420
 such as the number of covered bricks,

1544
01:13:43,420 --> 01:13:45,420
 and also the number of uncovered clusters, and so on.

1545
01:13:45,420 --> 01:13:49,420
 This matrix can be used to build higher-level optimizations on top of it,

1546
01:13:49,420 --> 01:13:53,420
 for example, to search for the best position of the robot or multiple robots

1547
01:13:53,420 --> 01:13:55,420
 that result in the best coverage.

1548
01:13:55,420 --> 01:13:57,420
 And in this work, I tried this.

1549
01:13:57,420 --> 01:14:02,420
 So in general, there might be many, many different possible approaches

1550
01:14:02,420 --> 01:14:04,420
 to optimize coverage with multiple robots,

1551
01:14:04,420 --> 01:14:06,420
 and in this work, I tried a simple greedy search.

1552
01:14:06,420 --> 01:14:08,420
 So the idea is to--

1553
01:14:08,420 --> 01:14:14,420
 --that was going to happen, but I was prepared to show a couple of these last little bits--

1554
01:14:14,420 --> 01:14:15,420
 --by optimization.

1555
01:14:15,420 --> 01:14:18,420
 However, the placement of the bricks is highly constrained,

1556
01:14:18,420 --> 01:14:22,420
 and here the manipulator always puts bricks--

1557
01:14:22,420 --> 01:14:28,420
 So here the second robot starts working--

1558
01:14:28,420 --> 01:14:30,420
 --to work at the same time.

1559
01:14:30,420 --> 01:14:33,420
 So this comes with better coverage, but--

1560
01:14:33,420 --> 01:14:38,420
 [applause]

1561
01:14:38,420 --> 01:14:41,420
 You said global trajectory optimization at the beginning.

1562
01:14:41,420 --> 01:14:44,420
 What did you mean by global trajectory optimization?

1563
01:14:44,420 --> 01:14:56,420
 So like end-to-end, it's in time.

1564
01:14:56,420 --> 01:14:59,420
 You're doing the whole-- like maybe the multimodal or something like that.

1565
01:14:59,420 --> 01:15:02,420
 But it's still with the SNOP or whatever.

1566
01:15:02,420 --> 01:15:04,420
 It's still the local method.

1567
01:15:04,420 --> 01:15:07,420
 The optimization is local and subject to local minima.

1568
01:15:07,420 --> 01:15:09,420
 Yeah, yeah.

1569
01:15:09,420 --> 01:15:22,420
 Because of the optimizer being too weak or because the kinematics are too limited?

1570
01:15:22,420 --> 01:15:25,420
 Yeah.

1571
01:15:25,420 --> 01:15:28,420
 There's so many constraints in the error service.

1572
01:15:28,420 --> 01:15:30,420
 Nice.

1573
01:15:30,420 --> 01:15:32,420
 Okay, any other questions?

1574
01:15:32,420 --> 01:15:40,420
 I was very impressed by the number of things you got in there.

1575
01:15:40,420 --> 01:15:51,420
 The peg and hole problem is a classic problem in robotics--

1576
01:15:51,420 --> 01:15:56,420
 --where an object is inserted into a hole that fits the object tightly.

1577
01:15:56,420 --> 01:15:59,420
 For this project, I chose to work on a variation of the problem--

1578
01:15:59,420 --> 01:16:02,420
 --with a square peg and a square hole.

1579
01:16:02,420 --> 01:16:04,420
 The goal was to first be able to perform the task--

1580
01:16:04,420 --> 01:16:07,420
 --knowing the precise locations of the block and the hole--

1581
01:16:07,420 --> 01:16:10,420
 --and then find out how much harder the problem becomes--

1582
01:16:10,420 --> 01:16:13,420
 --when some inaccuracy is introduced.

1583
01:16:13,420 --> 01:16:15,420
 Here we have the setup used for the project--

1584
01:16:15,420 --> 01:16:19,420
 --with the IWA arm, the gripper, a red block that will be our peg--

1585
01:16:19,420 --> 01:16:23,420
 --and a hole formed from blue blocks that are welded in place.

1586
01:16:23,420 --> 01:16:25,420
 Also, there's a camera floating high above the block--

1587
01:16:25,420 --> 01:16:29,420
 --that we can use for determining the x and y coordinates of the block.

1588
01:16:29,420 --> 01:16:32,420
 Here, a pseudo-inverse controller is used to control--

1589
01:16:32,420 --> 01:16:35,420
 --the spatial orientation of the gripper.

1590
01:16:35,420 --> 01:16:38,420
 The exact positions of the block and hole are known--

1591
01:16:38,420 --> 01:16:41,420
 --so a valid trajectory can be commanded to the robot--

1592
01:16:41,420 --> 01:16:43,420
 --to put the block in the hole.

1593
01:16:43,420 --> 01:16:45,420
 The gripper rotates to avoid hitting the hole--

1594
01:16:45,420 --> 01:16:48,420
 --grips the block above its center of mass--

1595
01:16:48,420 --> 01:16:53,420
 --avoids obstacles, lowers the block into the hole, and lets go.

1596
01:16:53,420 --> 01:16:57,420
 Now, we introduce vision to the problem.

1597
01:16:57,420 --> 01:17:01,420
 These are the points seen from the camera that passed through an RGB filter--

1598
01:17:01,420 --> 01:17:04,420
 --and they represent the top of the block pretty well.

1599
01:17:04,420 --> 01:17:09,420
 However, notice a row of points to the left hanging off of the block.

1600
01:17:09,420 --> 01:17:12,420
 Due to that rebellious row, the average of the points--

1601
01:17:12,420 --> 01:17:15,420
 --will not be the exact center of the block.

1602
01:17:15,420 --> 01:17:19,420
 The insertion fails, but two corners of the block go in--

1603
01:17:19,420 --> 01:17:23,420
 --showing that at least the y coordinate of the block was correct.

1604
01:17:23,420 --> 01:17:27,420
 To solve this predicament, I devised a spiral search method.

1605
01:17:27,420 --> 01:17:32,420
 If an upper bound for the error in the position of the block can be determined--

1606
01:17:32,420 --> 01:17:36,420
 --that gives a square area spanning out from the attempted insertion point--

1607
01:17:36,420 --> 01:17:39,420
 --where the hole must be located.

1608
01:17:39,420 --> 01:17:43,420
 So the idea is to move to one of the corners of this square area--

1609
01:17:43,420 --> 01:17:46,420
 --and then move along the square spiral trajectory--

1610
01:17:46,420 --> 01:17:49,420
 --spiraling to the center of the area.

1611
01:17:49,420 --> 01:17:53,420
 All the while, the block is being pressed down into the surface of the hole.

1612
01:17:53,420 --> 01:17:58,420
 The termination condition would be when there's a drop in normal force on the arm.

1613
01:17:58,420 --> 01:18:02,420
 Unfortunately, since not all rotations of the block are constrained--

1614
01:18:02,420 --> 01:18:07,420
 --when the block is held by the gripper, the block rotates when pressed down into the hole--

1615
01:18:07,420 --> 01:18:13,420
 --and since it is partially inserted, the gripper cannot move to perform the spiral search.

1616
01:18:13,420 --> 01:18:20,420
 Also, the block has rotated away from the correct orientation, so all hope is lost.

1617
01:18:20,420 --> 01:18:25,420
 For future work, I would love to try this with gripper arms that have a stronger grip--

1618
01:18:25,420 --> 01:18:28,420
 --or have a higher friction coefficient.

1619
01:18:28,420 --> 01:18:32,420
 If that doesn't work, I would like to try it with a gripper with forearms.

1620
01:18:32,420 --> 01:18:37,420
 I have faith that this can be made to work.

1621
01:18:37,420 --> 01:18:41,420
 [applause]

1622
01:18:41,420 --> 01:18:45,420
 Awesome. You just need to squeeze the heck out of it. That's what we learned from the other--

1623
01:18:45,420 --> 01:18:50,420
 Why is it sinking in so much? Is that the collision geometry being recessed?

1624
01:18:50,420 --> 01:19:00,420
 [inaudible]

1625
01:19:00,420 --> 01:19:01,420
 No?

1626
01:19:01,420 --> 01:19:05,420
 [inaudible]

1627
01:19:05,420 --> 01:19:07,420
 So what surprised you the most in that?

1628
01:19:07,420 --> 01:19:13,420
 [inaudible]

1629
01:19:13,420 --> 01:19:17,420
 That's fair. How difficult everything is was the answer.

1630
01:19:17,420 --> 01:19:24,420
 Cool. All right. So I think-- Is Radha here? All right, nice. Yes.

1631
01:19:24,420 --> 01:19:29,420
 I can't see that.

1632
01:19:29,420 --> 01:19:39,420
 Hi, my name is Radhika Ghoshal, and today I will talk about my project on RRT planning for push manipulation.

1633
01:19:39,420 --> 01:19:47,420
 This project focuses on generating trajectories to push a box from a start configuration to a goal using sampling-based planning.

1634
01:19:47,420 --> 01:19:56,420
 We re-implemented prior work by Zito et al. titled "Two-Level RRT Planning for Robotics Push Manipulation" and learned a lot along the way.

1635
01:19:56,420 --> 01:19:59,420
 Here's a quick summary of the two-level planner.

1636
01:19:59,420 --> 01:20:07,420
 The outer loop of the two-level planner samples box configurations in its C space to get Cs of configurations like this.

1637
01:20:07,420 --> 01:20:12,420
 The start config is marked in red and the goal is marked in green.

1638
01:20:12,420 --> 01:20:21,420
 And the inner local push planner operates in the C space of the robot to find feasible push sequences between the sampled box configurations.

1639
01:20:21,420 --> 01:20:26,420
 The local planner checks for feasibility by running forward simulations inside it.

1640
01:20:26,420 --> 01:20:35,420
 Note that the original paper uses purely kinematic methods to generate these push sequences, and the quality of pushes here isn't that great.

1641
01:20:35,420 --> 01:20:50,420
 Let me show a few more.

1642
01:20:50,420 --> 01:21:03,420
 For the implementation, the EVA push planner outputs a trajectory of desired end-effector poses, which is then fed to the differential inverse kinematics controller to convert to joint position commands.

1643
01:21:03,420 --> 01:21:08,420
 Turns out, the purely kinematic local push planner doesn't work well for us.

1644
01:21:08,420 --> 01:21:15,420
 Due to its poor quality pushes, RRT ends up requiring a large number of samples to find push sequences to the goal.

1645
01:21:15,420 --> 01:21:19,420
 This path usually ends up being circuitous and unusable.

1646
01:21:19,420 --> 01:21:25,420
 In the RRT plot above, it wasn't possible to find a path with even a large number of samples.

1647
01:21:25,420 --> 01:21:29,420
 So we decided to use a Cartesian force controller.

1648
01:21:29,420 --> 01:21:39,420
 This consistently produces high-quality pushes against the box during testing, but we didn't have time to integrate it into the full RRT loop.

1649
01:21:39,420 --> 01:21:47,420
 For future work, we'd like to complete the integration of the force control model into the full RRT loop.

1650
01:21:47,420 --> 01:21:54,420
 Finally, I realized that force control is awesome and often makes things easier than position/velocity control.

1651
01:21:54,420 --> 01:21:56,420
 Thanks for watching.

1652
01:21:57,420 --> 01:21:59,420
 [APPLAUSE]

1653
01:21:59,420 --> 01:22:01,420
 Here, here, force control.

1654
01:22:01,420 --> 01:22:07,420
 So how are you going to put the force controller into the-- is this going to be an RRT extend that's going to be using the controller?

1655
01:22:07,420 --> 01:22:08,420
 Yeah.

1656
01:22:08,420 --> 01:22:13,420
 So like, we run the forward simulation inside the RRT extend operation.

1657
01:22:13,420 --> 01:22:19,420
 So like, I don't need to put in the force controller inside that.

1658
01:22:19,420 --> 01:22:20,420
 Yeah, yeah.

1659
01:22:20,420 --> 01:22:21,420
 I have a--

1660
01:22:21,420 --> 01:22:22,420
 Yeah, nice.

1661
01:22:22,420 --> 01:22:28,420
 [INAUDIBLE]

1662
01:22:28,420 --> 01:22:30,420
 [LAUGHTER]

1663
01:22:30,420 --> 01:22:31,420
 It happens.

1664
01:22:31,420 --> 01:22:32,420
 Yes?

1665
01:22:32,420 --> 01:22:37,420
 What kind of push is necessary to be performing an RRT extension of that?

1666
01:22:37,420 --> 01:22:56,420
 [INAUDIBLE]

1667
01:22:56,420 --> 01:22:59,420
 So it's just differences in the pose of the brick.

1668
01:22:59,420 --> 01:23:00,420
 Yes.

1669
01:23:00,420 --> 01:23:01,420
 Yes.

1670
01:23:01,420 --> 01:23:03,420
 Cool.

1671
01:23:03,420 --> 01:23:06,420
 OK, I think Jared said he was going to come later.

1672
01:23:06,420 --> 01:23:07,420
 I'm trying to--

1673
01:23:07,420 --> 01:23:08,420
 [INAUDIBLE]

1674
01:23:08,420 --> 01:23:09,420
 Yeah, OK.

1675
01:23:09,420 --> 01:23:14,420
 I'll come back to anybody who shows up later.

1676
01:23:14,420 --> 01:23:18,420
 I think Annie Rude and Ken's here.

1677
01:23:18,420 --> 01:23:19,420
 Kenneth, there you go.

1678
01:23:19,420 --> 01:23:20,420
 Good.

1679
01:23:20,420 --> 01:23:21,420
 All right.

1680
01:23:21,420 --> 01:23:32,420
 Our project is automated robust stacking of prisms, also known as PrismBot.

1681
01:23:32,420 --> 01:23:33,420
 I'm Ethan.

1682
01:23:33,420 --> 01:23:34,420
 I'm Kenneth.

1683
01:23:34,420 --> 01:23:35,420
 I'm Annie.

1684
01:23:35,420 --> 01:23:41,420
 We care about stacking because stacking is a cognitively challenging task.

1685
01:23:41,420 --> 01:23:49,420
 There's applications of stacking in construction and fabrication.

1686
01:23:49,420 --> 01:23:53,420
 We focus our problem on stacking prisms.

1687
01:23:53,420 --> 01:24:01,420
 Prisms are easier to grasp and stack compared to arbitrary objects due to their simple geometry and flat surfaces.

1688
01:24:01,420 --> 01:24:05,420
 All right, so our initial method was mostly adapted from the bin picking notebook.

1689
01:24:05,420 --> 01:24:07,420
 As you can see, it was not very robust.

1690
01:24:07,420 --> 01:24:15,420
 It could not robustly grasp objects or even place them down precisely enough to build a tall stack.

1691
01:24:15,420 --> 01:24:18,420
 So in order to mitigate this, we use a state machine.

1692
01:24:18,420 --> 01:24:28,420
 The state machine robustly clears the space where the stack is going to be built by detecting when the stack falls over so that we don't end up stacking over fallen debris, which would render an unstable stack.

1693
01:24:28,420 --> 01:24:38,420
 We also use self correction when motion tracking error detected so that the trajectories that we execute are the trajectories that we expect.

1694
01:24:38,420 --> 01:24:44,420
 We also do center of mass calculation so that we can precisely place the blocks such that their center of masses line up vertically.

1695
01:24:44,420 --> 01:24:49,420
 Recall from class that we can estimate center of mass by lump parameter estimation.

1696
01:24:49,420 --> 01:25:00,420
 If the object is held still in fact, it's possible to calculate the term separately and separately calculate the mass and the center of mass of a held object, just by looking at the external torques of the EWA.

1697
01:25:00,420 --> 01:25:09,420
 The only problem is, is that the only the x and y coordinates of the center of mass of the object are identifiable in the world frame, since the z coordinate is in the direction of gravity.

1698
01:25:09,420 --> 01:25:19,420
 The solution is to just measure from two poses and use a mathematical program to find the optimal center of mass that matches up with the measured external torques.

1699
01:25:19,420 --> 01:25:28,420
 So another problem we ran into in our initial methods was problems related to grasping. Our main problem was that we would often try to grasp points that were near edges.

1700
01:25:28,420 --> 01:25:34,420
 So our grasp would either fail or if they succeeded, they would often be wobbly and would result in poor placing.

1701
01:25:34,420 --> 01:25:40,420
 So the way we fix this is to develop an edge heuristic so we can avoid grasping your edges.

1702
01:25:40,420 --> 01:25:51,420
 So the way it worked is that after we picked a target grasp point, we would sample the 25 closest points spatially from our point cloud and collect their normals.

1703
01:25:51,420 --> 01:26:05,420
 And then across these 25 normals, we would find the minimum dot product between any pair, and we would have a cutoff because for a face, this heuristic would give us a large value since all the normals would point in the same direction.

1704
01:26:05,420 --> 01:26:12,420
 And we would get a lower value in your edges, because we'd have normals from two different faces, which would result in a lower dot product.

1705
01:26:12,420 --> 01:26:27,420
 We also improved our perception by adding segmentation. We use the dbScan algorithm to segment objects spatially, and we also segmented them via hue to account for the case where two objects were touching each other.

1706
01:26:27,420 --> 01:26:37,420
 And this let us detect which objects were in the stacking zone and move them away. And it also helped us pick the order of blocks to stack.

1707
01:26:37,420 --> 01:26:46,420
 For our final results, we evaluated our system by testing it on 10 trials with varying number of objects and object type.

1708
01:26:46,420 --> 01:26:53,420
 So you can see here that we tested 2, 3, 4, and 5 rectangular prisms, and the same for pentagonal prisms.

1709
01:26:53,420 --> 01:27:02,420
 We have a high success rate for less than or equal to 3 objects, which is a greater than 90% success rate.

1710
01:27:02,420 --> 01:27:10,420
 However, as we increase the number of objects, we can see that the success rate drops, and for pentagonal prisms, the success rate also drops.

1711
01:27:10,420 --> 01:27:19,420
 One common error that we saw, especially for a large number of objects, was a simulator crashing due to the high number of collisions that are involved when the stack gets taller.

1712
01:27:19,420 --> 01:27:28,420
 Here you can see our system working in full. First, our robot clears the object from the stacking cylinder so that it could replace it with more precision.

1713
01:27:28,420 --> 01:27:39,420
 Now you can see for each object, our robot rotates the object a bit in order to do the center of mass calculation to again place it precisely on top of the preceding one.

1714
01:27:39,420 --> 01:27:49,420
 In this simulation, our robot is able to successfully stack 7 objects on top of each other, and we think that this is actually the maximum that is possible for our current simulation,

1715
01:27:49,420 --> 01:27:58,420
 as for the 6th block, the top of the block is not visible to the cameras.

1716
01:27:58,420 --> 01:28:05,420
 Thanks for listening. Our code is publicly available on GitHub for others to build on top of.

1717
01:28:05,420 --> 01:28:08,420
 [applause]

1718
01:28:08,420 --> 01:28:13,420
 So did you hide the center of mass in random places inside the block? Is that what you did?

1719
01:28:13,420 --> 01:28:19,420
 We were thinking to do that, but we figured it would work, but we didn't have enough time to do it.

1720
01:28:19,420 --> 01:28:26,420
 So what was the variability that gave you different-- I mean, was it only the bricks, same initial conditions, or where did you--

1721
01:28:26,420 --> 01:28:33,420
 It was, I guess, random collisions, [inaudible]

1722
01:28:33,420 --> 01:28:43,420
 Yeah? Very nice. I'm glad you got the center of mass estimation working. That's perfect. Yeah. Any other questions?

1723
01:28:43,420 --> 01:28:50,420
 All right. Jared, I knew you were coming at 3.30. We just passed you, but I'm going back.

1724
01:28:50,420 --> 01:28:53,420
 We're talking to you about my final project, which is called GreenBot.

1725
01:28:53,420 --> 01:28:58,420
 It's a manipulation system capable of identifying, picking, and placing recyclable waste.

1726
01:28:58,420 --> 01:29:04,420
 So how exactly does it work? It uses a custom-trained mask-RCNN model for object segmentation.

1727
01:29:04,420 --> 01:29:10,420
 Given a labeled and segmented point cloud, it can find a near-optimal antipodal grasp for this object.

1728
01:29:10,420 --> 01:29:18,420
 It uses a slightly modified version of a pseudo-inverse controller to move around, and of course, it uses some fun 3D models that I found on the Internet.

1729
01:29:18,420 --> 01:29:26,420
 Here's the mask-RCNN model. It's trained the same way that we approached it in class, which is that we start from a pre-trained model using the COCO dataset,

1730
01:29:26,420 --> 01:29:37,420
 and then we strip the last layer of nodes and retrain it such that the output is the labeled and masked images corresponding to the set of objects that I'd given the simulation.

1731
01:29:37,420 --> 01:29:47,420
 From here, any time the robot actually wants to pick something up and make a decision, it can query two different RGB images from cameras on either side of the picnic table,

1732
01:29:47,420 --> 01:29:56,420
 and then it can pass off these RGB images to mask-RCNN. And from here, what it gets is labeled and segmented depth images from mask-RCNN,

1733
01:29:56,420 --> 01:30:02,420
 which it can project to XYZ space and get a comprehensive point cloud for a given object.

1734
01:30:02,420 --> 01:30:06,420
 Once it has this point cloud, we generate around 25 grasp candidates.

1735
01:30:06,420 --> 01:30:11,420
 These candidates are found by choosing a random point on the point cloud, calculating its normal,

1736
01:30:11,420 --> 01:30:18,420
 projecting this normal such that it's planar with the XY plane, and then aligning the gripper X-axis with this projected normal.

1737
01:30:18,420 --> 01:30:27,420
 From here, it wants to select the most antipodal grasp, which is it tries to maximize the alignment with the normals of the point cloud with the gripper X-axis.

1738
01:30:27,420 --> 01:30:31,420
 From here, the trajectory planning is a pretty simple scheme.

1739
01:30:31,420 --> 01:30:38,420
 We just simply interpolate between a few different key poses, such that we have a home pose for the EWOB,

1740
01:30:38,420 --> 01:30:42,420
 we have a pre-picked pose, which is just slightly above the object of interest,

1741
01:30:42,420 --> 01:30:48,420
 we have a grasp pose, which is generated from the grasping scheme, and we have a pose just above the desired drop-off fit.

1742
01:30:48,420 --> 01:30:55,420
 From here, we can differentiate this linearly interpolated trajectory and use a pseudo-inverse controller to move around.

1743
01:30:55,420 --> 01:31:00,420
 In order to actually test GreenBot, we asked, "How many objects can GreenBot sort in 60 seconds?"

1744
01:31:00,420 --> 01:31:07,420
 That is, we gave GreenBot four potential objects on the picnic table and gave it 60 seconds to sort as many as possible.

1745
01:31:07,420 --> 01:31:15,420
 Of the 56 valid tests that we ran, 67% of them performed perfectly, sorting all four objects in the allotted time,

1746
01:31:15,420 --> 01:31:20,420
 and around 88% of them performed near-perfectly, sorting at least three of the four objects correctly.

1747
01:31:20,420 --> 01:31:23,420
 In terms of future work, I think mobile manipulators are really cool.

1748
01:31:23,420 --> 01:31:28,420
 One potential application of this is, instead of using a stationary EWO arm,

1749
01:31:28,420 --> 01:31:34,420
 use some sort of cheap mobile manipulator in simulation and try to get it to actually move around the environment,

1750
01:31:34,420 --> 01:31:38,420
 traverse, pick up items, and navigate over to the waste bin to drop it off.

1751
01:31:38,420 --> 01:31:42,420
 Thanks so much for listening to my presentation, and I'm happy to answer some questions.

1752
01:31:42,420 --> 01:31:48,420
 [applause]

1753
01:31:48,420 --> 01:31:50,420
 I have a totally nerdy question.

1754
01:31:50,420 --> 01:31:55,420
 What was the OBJ file that made the transparent bottle work in the RGBD renderer?

1755
01:31:55,420 --> 01:31:57,420
 I didn't know it could do that.

1756
01:31:57,420 --> 01:32:14,420
 [inaudible]

1757
01:32:14,420 --> 01:32:16,420
 I see, it shows that. Yeah.

1758
01:32:16,420 --> 01:32:18,420
 [inaudible]

1759
01:32:18,420 --> 01:32:20,420
 That's right. That's what I, okay.

1760
01:32:20,420 --> 01:32:23,420
 My world is consistent at least. Yeah.

1761
01:32:23,420 --> 01:32:26,420
 Nice. I mean, I would love for it to render in transparent objects.

1762
01:32:26,420 --> 01:32:31,420
 Yeah. All right. Questions?

1763
01:32:31,420 --> 01:32:33,420
 What was the hardest part of that whole pipeline?

1764
01:32:33,420 --> 01:32:52,420
 [inaudible]

1765
01:32:52,420 --> 01:32:57,420
 This guy is crazy. Nice. Okay. Thank you.

1766
01:32:57,420 --> 01:33:08,420
 [inaudible]

1767
01:33:08,420 --> 01:33:12,420
 Speedcubing is the task of solving a Rubik's Cube as fast as possible.

1768
01:33:12,420 --> 01:33:17,420
 Cubing is of great interest to robotics, as it requires precise manipulation.

1769
01:33:17,420 --> 01:33:22,420
 For example, if the face is not properly aligned after a turn, it can prevent the next face turn.

1770
01:33:22,420 --> 01:33:25,420
 Making turns on a Rubik's Cube quickly is an even greater challenge.

1771
01:33:25,420 --> 01:33:29,420
 Currently, there are two notable cube-solving robots.

1772
01:33:29,420 --> 01:33:31,420
 The first robot was made here at MIT.

1773
01:33:31,420 --> 01:33:33,420
 Wow, that's really fast.

1774
01:33:33,420 --> 01:33:36,420
 But isn't that kind of unfair to humans?

1775
01:33:36,420 --> 01:33:41,420
 We don't have six hands, let alone six limbs, and our wrists can't rotate like motors.

1776
01:33:41,420 --> 01:33:44,420
 The second is OpenAI's robot hand.

1777
01:33:44,420 --> 01:33:51,420
 Its hardware is more fair to humans, but it is really slow.

1778
01:33:51,420 --> 01:33:53,420
 Introducing Speedcuberbot.

1779
01:33:53,420 --> 01:33:56,420
 It aims to be a balance between the two robots.

1780
01:33:56,420 --> 01:34:01,420
 With only an arm and a gripper, it has a hardware limitation like the OpenAI robot,

1781
01:34:01,420 --> 01:34:04,420
 but it aims to make fast turns like the MIT robot.

1782
01:34:04,420 --> 01:34:08,420
 Before we get started, we need to understand move notation.

1783
01:34:08,420 --> 01:34:11,420
 U means to turn the upper face 90 degrees clockwise.

1784
01:34:11,420 --> 01:34:15,420
 U' means to turn the upper face 90 degrees counterclockwise.

1785
01:34:15,420 --> 01:34:19,420
 And U2 means to turn the upper face 180 degrees.

1786
01:34:19,420 --> 01:34:23,420
 Before we can make a turn, we need to constrain the layers that will not turn.

1787
01:34:23,420 --> 01:34:27,420
 Initially, I was planning to have a second eagle arm hold the cube in place.

1788
01:34:27,420 --> 01:34:30,420
 However, after receiving the model of the Rubik's Cube

1789
01:34:30,420 --> 01:34:33,420
 and finding out the grippers cannot really grip onto the cube,

1790
01:34:33,420 --> 01:34:38,420
 I decided to make and use a box that held the bottom layer in place.

1791
01:34:38,420 --> 01:34:42,420
 Because of this, Speedcuberbot will only be able to turn the top face.

1792
01:34:42,420 --> 01:34:47,420
 To make a U move, we move the gripper to the grabbed position for a U move,

1793
01:34:47,420 --> 01:34:51,420
 grabs the cube, make a clockwise turn, and un-grabs the cube.

1794
01:34:51,420 --> 01:34:56,420
 To make a U' move, it is the same except we move the gripper to the grabbed position

1795
01:34:56,420 --> 01:34:59,420
 for a U' move and make a counterclockwise turn.

1796
01:34:59,420 --> 01:35:04,420
 To make a U2 move, we can do two U moves or two U' moves.

1797
01:35:04,420 --> 01:35:10,420
 Our goal is to make turns quickly, so we should turn in the opposite direction of the previous move.

1798
01:35:10,420 --> 01:35:13,420
 This is because after completing a turn for a U move,

1799
01:35:13,420 --> 01:35:19,420
 the grabbed position for the U' move is closer than the grabbed position for the U move and vice versa.

1800
01:35:19,420 --> 01:35:25,420
 With this plan, we can model the motion of Speedcuberbot as a state machine.

1801
01:35:25,420 --> 01:35:28,420
 Additionally, to actually get the gripper to those positions,

1802
01:35:28,420 --> 01:35:34,420
 we solve inverse kinematics as an optimization problem to get the joint position for the desired pose.

1803
01:35:34,420 --> 01:35:40,420
 Putting it all together, we can give Speedcuberbot a sequence of up-face moves and it will execute them.

1804
01:35:40,420 --> 01:35:45,420
 The worst case turn time is 0.9 seconds, but thanks to the optimizations we made for U2 moves

1805
01:35:45,420 --> 01:35:51,420
 and the fact that going from U to U' and vice versa is faster, the average turn time is 0.77 seconds.

1806
01:35:51,420 --> 01:35:54,420
 While a 0.13 second time save on each turn seems very little,

1807
01:35:54,420 --> 01:36:00,420
 it adds up when doing a sequence of turns, and Speedcubers are looking to save every little second.

1808
01:36:00,420 --> 01:36:03,420
 [END]

1809
01:36:03,420 --> 01:36:07,420
 [APPLAUSE]

1810
01:36:07,420 --> 01:36:09,420
 We actually heard a story just the other day.

1811
01:36:09,420 --> 01:36:13,420
 Sava, who just left, was telling us that the Ben Katz robot, which is awesome,

1812
01:36:13,420 --> 01:36:16,420
 apparently it could go so fast that the Rubik's Cubes would explode.

1813
01:36:16,420 --> 01:36:18,420
 Did you know that? Okay.

1814
01:36:18,420 --> 01:36:21,420
 The cheap Rubik's Cubes would just go explode.

1815
01:36:21,420 --> 01:36:25,420
 All right. That was an awesome video. Thank you for doing it.

1816
01:36:25,420 --> 01:36:27,420
 Yeah, he's got a question.

1817
01:36:27,420 --> 01:36:37,420
 [INAUDIBLE]

1818
01:36:37,420 --> 01:36:44,420
 Yeah, so I think a method for turning it like this, you look on the cube.

1819
01:36:44,420 --> 01:36:52,420
 So when you do it, just like for Ben, just use the cube, just straight, not at an angle.

1820
01:36:52,420 --> 01:36:58,420
 And then try to turn it a little bit before getting to the point, and at that point,

1821
01:36:58,420 --> 01:37:03,420
 then it will start doing the spin. So it's better if you start at the point.

1822
01:37:03,420 --> 01:37:06,420
 Nice. Oh, yeah, another question.

1823
01:37:06,420 --> 01:37:16,420
 [INAUDIBLE]

1824
01:37:16,420 --> 01:37:25,420
 I think I didn't get it originally. I just see what it's doing.

1825
01:37:25,420 --> 01:37:30,420
 I also saw you when you were programming it in Office Hours one time or something.

1826
01:37:30,420 --> 01:37:34,420
 It looked like you had made a cool teleop, like some way to transition between teleop.

1827
01:37:34,420 --> 01:37:39,420
 You teleop for a minute, and then you would turn on the autonomy for a minute,

1828
01:37:39,420 --> 01:37:42,420
 and then you'd switch back to teleop. How did you do that?

1829
01:37:42,420 --> 01:37:46,420
 Did I see that right?

1830
01:37:46,420 --> 01:37:49,420
 It was all teleop. You fooled me.

1831
01:37:49,420 --> 01:37:56,420
 [LAUGHTER]

1832
01:37:56,420 --> 01:38:02,420
 OK. Nice. OK.

1833
01:38:02,420 --> 01:38:07,420
 I think Jenny, you guys are next. Jenny and Daniel.

1834
01:38:07,420 --> 01:38:12,420
 Ping pong is a fast-paced sport that requires fast processing and precise paddle control.

1835
01:38:12,420 --> 01:38:18,420
 In this project, we created Ping Pong Bot, a pair of KUKA IWA arms that can perform a controlled rally for multiple hits.

1836
01:38:18,420 --> 01:38:26,420
 This project is an iteration upon a previous project by Dylan Zhao and Chaitanya Ravuri for robotic manipulation in 2021.

1837
01:38:26,420 --> 01:38:31,420
 We used their framework and simulation as a starting point, but chose to re-implement the control logic state machine

1838
01:38:31,420 --> 01:38:35,420
 and adjusted several simulation parameters to improve performance and simulation realism.

1839
01:38:35,420 --> 01:38:41,420
 As you can see, in the previous iteration of this project, the simulated ball is not nearly as bouncy as a real-life ping pong ball.

1840
01:38:41,420 --> 01:38:46,420
 To fix this, Professor Tedrick helped us learn to use some of the Drake-specific contact parameters in our SDF

1841
01:38:46,420 --> 01:38:50,420
 to control the bounciness and make it look a lot more realistic.

1842
01:38:50,420 --> 01:38:54,420
 Here, we have a simple diagram showing the high-level state machine flow.

1843
01:38:54,420 --> 01:39:00,420
 Each IWA arm is running a separate instance of this controller, and both begin in the away state.

1844
01:39:00,420 --> 01:39:06,420
 Using the velocity vector of the ball, we know if it's traveling toward or away from a particular side of the table

1845
01:39:06,420 --> 01:39:09,420
 and set the away or toward state as appropriate.

1846
01:39:09,420 --> 01:39:14,420
 After the ball bounces on the same side of the table as the arm, we transition to the prep state.

1847
01:39:14,420 --> 01:39:19,420
 And finally, when the ball is close to the projected contact position, we go to the hit state.

1848
01:39:19,420 --> 01:39:26,420
 If at any point, the ball velocity is pointing away from the paddle, the system resets to the away state.

1849
01:39:26,420 --> 01:39:35,420
 In the away state, the desired linear and angular velocity of the end effector is calculated by targeting the home pose of the paddle,

1850
01:39:35,420 --> 01:39:40,420
 centered and slightly behind the edge of the table, with the paddle tilted up.

1851
01:39:40,420 --> 01:39:45,420
 To check whether the ball is actually traveling toward the arm and trigger a state transition to the toward state,

1852
01:39:45,420 --> 01:39:51,420
 it calculates the dot product of the velocity vector of the ball and the vector from the end effector to the ball position.

1853
01:39:51,420 --> 01:39:58,420
 If the dot product is negative, the ball is traveling towards the paddle, and the controller will transition to the toward state.

1854
01:39:58,420 --> 01:40:04,420
 When the controller transitions to the toward state, it uses the x and y velocity components of the ball

1855
01:40:04,420 --> 01:40:10,420
 and projects the y-coordinate of when it reaches a specific x value in the world frame.

1856
01:40:10,420 --> 01:40:15,420
 This calculation is only performed once, and the target pose is set to the same as the home pose,

1857
01:40:15,420 --> 01:40:19,420
 except with the updated y-coordinate for the projection.

1858
01:40:19,420 --> 01:40:29,420
 Using the time before the ball bounces to move to this intermediate pose is essential for getting the robot roughly close enough to the actual pose needed to hit the ball correctly.

1859
01:40:29,420 --> 01:40:38,420
 Without this step, there's generally not enough time after the bounce for the arm to move to the prep pose before the ball has already reached the contact point.

1860
01:40:38,420 --> 01:40:48,420
 When the ball has a positive z-component velocity and is slightly above the height of the table, we know that the ball has just bounced.

1861
01:40:48,420 --> 01:40:52,420
 After the ball bounces on the table, the arm enters the prep state.

1862
01:40:52,420 --> 01:40:58,420
 In the prep state, our controller uses the ball's current position and velocity to predict the ball's trajectory.

1863
01:40:58,420 --> 01:41:03,420
 It then chooses a contact point along the ball's trajectory where the paddle will hit the ball.

1864
01:41:03,420 --> 01:41:08,420
 It chooses a contact point a little past the edge of the table to minimize the chances that the paddle hits the table,

1865
01:41:08,420 --> 01:41:12,420
 which we found causes the arm to start flailing wildly.

1866
01:41:12,420 --> 01:41:17,420
 The controller then attempts to find a paddle pose that will hit the ball at a target point on the other side of the table.

1867
01:41:17,420 --> 01:41:25,420
 We use the equations of projectile motion to calculate the ideal velocity of the ball after the collision so that it lands on the target point.

1868
01:41:25,420 --> 01:41:34,420
 We use the mirror law along with the incoming and outgoing ball velocities at the contact point to calculate a paddle pose that will correctly reflect the ball towards the opponent.

1869
01:41:34,420 --> 01:41:43,420
 Finally, the paddle uses this calculated pose to move to a pre-hit pose slightly behind the contact point, giving it space to accelerate before the hit.

1870
01:41:43,420 --> 01:41:49,420
 When the ball is close enough to the contact point, we transition to the hit state to start accelerating the paddle towards the ball.

1871
01:41:49,420 --> 01:41:57,420
 Intuitively, the faster the ball is traveling, the slower we want the paddle to move, otherwise we're adding too much energy to the system.

1872
01:41:57,420 --> 01:42:02,420
 If the ball is bouncing too low, we also want to tilt the paddle upwards to add more vertical impulse.

1873
01:42:02,420 --> 01:42:07,420
 After successfully hitting the ball, the velocity vector should now be pointing away from the paddle.

1874
01:42:09,420 --> 01:42:13,420
 I thought that was really good.

1875
01:42:13,420 --> 01:42:15,420
 Yeah, that's quite good.

1876
01:42:15,420 --> 01:42:21,420
 Questions for these guys?

1877
01:42:21,420 --> 01:42:44,420
 [inaudible]

1878
01:42:44,420 --> 01:42:46,420
 Yeah.

1879
01:42:46,420 --> 01:42:55,420
 [inaudible]

1880
01:42:55,420 --> 01:43:00,420
 Nice. Which part was the biggest surprise for you of that pipeline?

1881
01:43:00,420 --> 01:43:04,420
 Was the state machine logic pretty good or was it?

1882
01:43:04,420 --> 01:43:33,420
 [inaudible]

1883
01:43:33,420 --> 01:43:42,420
 For sure. Great. Okay.

1884
01:43:42,420 --> 01:44:11,420
 [inaudible]

1885
01:44:11,420 --> 01:44:37,420
 [inaudible]

1886
01:44:37,420 --> 01:45:03,420
 [inaudible]

1887
01:45:03,420 --> 01:45:08,420
 To make everything work, we need to plan backward carefully.

1888
01:45:08,420 --> 01:45:15,420
 First, we need to constrain the landing angle theta because a large landing angle will make the ball bounce out of the hole.

1889
01:45:15,420 --> 01:45:23,420
 When flying in the air, the ball follows the aerodynamics, forced by gravity and the air resistance and nothing else.

1890
01:45:23,420 --> 01:45:29,420
 This makes it possible for us to do direct shooting to calculate the initial velocity of the golf ball.

1891
01:45:29,420 --> 01:45:38,420
 We aim at minimizing the flying time of the ball, subject to a set of constraints including landing constraint, landing angle constraint, and dynamics constraints.

1892
01:45:38,420 --> 01:45:45,420
 We solve the problem using a SADI solver and the solution velocity can 100% make the ball land in the hole.

1893
01:45:45,420 --> 01:45:53,420
 After solving the initial velocity, we can calculate the desired heating configuration of the robot.

1894
01:45:53,420 --> 01:46:05,420
 We manually assign the heating point on the club and build the heating frame such that the y-axis is parallel to the ground and the z-axis aligns with the initial velocity of the ball and the contact normal.

1895
01:46:05,420 --> 01:46:11,420
 Apply a series of frame transformation, we can get the desired pose of the club.

1896
01:46:11,420 --> 01:46:21,420
 Then we do optimization to solve the desired joint angles and use kinematic trajectory optimization to control the robot to reach the desired configuration.

1897
01:46:21,420 --> 01:46:25,420
 Next step, we want to control the robot to hit the ball.

1898
01:46:25,420 --> 01:46:33,420
 Theoretically, when the contact normal is aligned with target velocity, we just need one degree of freedom to reach the target velocity.

1899
01:46:33,420 --> 01:46:39,420
 To make the system more robust, we allow two joints to be movable during the heating process.

1900
01:46:39,420 --> 01:46:45,420
 In this way, we just need a mapping from the ball's target velocity back to the joint velocity.

1901
01:46:45,420 --> 01:46:51,420
 However, the heating process isn't differentiable or continuous, so it's hard to do the mapping.

1902
01:46:51,420 --> 01:46:55,420
 We propose to use data-driven methods to tackle the problem.

1903
01:46:55,420 --> 01:47:01,420
 We collect data by randomly sampling joint velocities and record the corresponding ball velocity.

1904
01:47:01,420 --> 01:47:07,420
 We observe that the data has strong linear correlation with some outliers.

1905
01:47:07,420 --> 01:47:14,420
 We apply RANSAC to filter out the outliers and try both linear regression and neural networks to fit the mapping model.

1906
01:47:14,420 --> 01:47:23,420
 Then at test time, we input the calculated ball initial velocity to get the desired joint velocity and do velocity control to hit the ball.

1907
01:47:23,420 --> 01:47:28,420
 Our framework can achieve a success rate of 19 out of 100 rollouts.

1908
01:47:28,420 --> 01:47:32,420
 The failure cases can be categorized into two sets.

1909
01:47:32,420 --> 01:47:37,420
 The strike is not successful or the landing point of the ball is not accurate enough.

1910
01:47:37,420 --> 01:47:40,420
 This inspires us with two future directions.

1911
01:47:40,420 --> 01:47:45,420
 The first is to improve our formulation for the preheating and heating process,

1912
01:47:45,420 --> 01:47:50,420
 controlling the robots in a continuous way to avoid sudden accelerations.

1913
01:47:50,420 --> 01:47:56,420
 The second is to simulate the heating process more reliably and learn from the previous work.

1914
01:47:56,420 --> 01:48:03,420
 That was impressive cinematography, right?

1915
01:48:03,420 --> 01:48:04,420
 Pretty good.

1916
01:48:04,420 --> 01:48:10,420
 So, sorry, you used shooting, you used Casati for shooting for the ball.

1917
01:48:10,420 --> 01:48:17,420
 Tell me how those two-- you said you used kinematic trajectory optimization for the arm and then direct shooting for the ball.

1918
01:48:17,420 --> 01:48:19,420
 Is that right?

1919
01:48:19,420 --> 01:48:21,420
 How did those two go together?

1920
01:48:21,420 --> 01:48:39,420
 [Unintelligible]

1921
01:48:39,420 --> 01:48:40,420
 Yes.

1922
01:48:40,420 --> 01:48:53,420
 [Unintelligible]

1923
01:48:53,420 --> 01:48:54,420
 Yes.

1924
01:48:54,420 --> 01:48:55,420
 Perfect.

1925
01:48:55,420 --> 01:48:57,420
 But why did you need shooting for the ball?

1926
01:48:57,420 --> 01:48:59,420
 Is it because of the aerodynamics?

1927
01:48:59,420 --> 01:49:01,420
 You said there was drag.

1928
01:49:01,420 --> 01:49:02,420
 What's that?

1929
01:49:02,420 --> 01:49:03,420
 [Unintelligible]

1930
01:49:03,420 --> 01:49:04,420
 That's why it was hard.

1931
01:49:04,420 --> 01:49:10,420
 [Unintelligible]

1932
01:49:10,420 --> 01:49:11,420
 I see.

1933
01:49:11,420 --> 01:49:14,420
 [Unintelligible]

1934
01:49:14,420 --> 01:49:15,420
 Okay.

1935
01:49:15,420 --> 01:49:16,420
 Very nice.

1936
01:49:16,420 --> 01:49:33,420
 [Unintelligible]

1937
01:49:33,420 --> 01:49:35,420
 You did that yourself, you said?

1938
01:49:35,420 --> 01:49:36,420
 The aerodynamics?

1939
01:49:36,420 --> 01:49:37,420
 Or that's just in bullet--

1940
01:49:37,420 --> 01:49:39,420
 [Unintelligible]

1941
01:49:39,420 --> 01:49:40,420
 Yeah, yeah.

1942
01:49:40,420 --> 01:49:41,420
 [Unintelligible]

1943
01:49:41,420 --> 01:49:42,420
 Yeah, yeah.

1944
01:49:42,420 --> 01:49:44,420
 [Unintelligible]

1945
01:49:44,420 --> 01:49:45,420
 Cool. Okay.

1946
01:49:45,420 --> 01:49:48,420
 Pascal and Ravi are here.

1947
01:49:48,420 --> 01:49:51,420
 [Unintelligible]

1948
01:49:51,420 --> 01:49:52,420
 Hello.

1949
01:49:52,420 --> 01:49:58,420
 We are Pascal Spino and Ravi Tejwani, and this is our final project for robot manipulation.

1950
01:49:58,420 --> 01:50:04,420
 We built a pipeline for an EWO arm to execute a trajectory over a curved surface

1951
01:50:04,420 --> 01:50:10,420
 while maintaining contact with that surface and maintaining force normal to the surface.

1952
01:50:10,420 --> 01:50:16,420
 There is much prior work on robots drawing on two-dimensional surfaces,

1953
01:50:16,420 --> 01:50:21,420
 but for our project, we wanted to explore drawing on a three-dimensional curved surface.

1954
01:50:21,420 --> 01:50:26,420
 For two-dimensional surfaces, there's many simple robots that are well-suited for the task,

1955
01:50:26,420 --> 01:50:30,420
 but drawing on a three-dimensional surface requires many more degrees of freedom

1956
01:50:30,420 --> 01:50:32,420
 for which the EWO arm is well-suited.

1957
01:50:32,420 --> 01:50:35,420
 The pipeline to our system is as follows.

1958
01:50:35,420 --> 01:50:38,420
 We take as inputs a two-dimensional trajectory plan

1959
01:50:38,420 --> 01:50:44,420
 and a three-dimensional instance of curved geometry with accurate collisions.

1960
01:50:44,420 --> 01:50:50,420
 We first estimate the point cloud corresponding to the surface of this geometry

1961
01:50:50,420 --> 01:50:52,420
 through an array of four cameras.

1962
01:50:52,420 --> 01:50:55,420
 Then we estimate normals over this point cloud.

1963
01:50:55,420 --> 01:51:00,420
 We then take our inputted trajectory and plan a three-dimensional trajectory

1964
01:51:00,420 --> 01:51:03,420
 over the surface while respecting the normals,

1965
01:51:03,420 --> 01:51:08,420
 and then we extract from this a series of poses, which we turn into a piecewise pose,

1966
01:51:08,420 --> 01:51:14,420
 and then pass to a differential inverse kinematics controller to execute on our EWO.

1967
01:51:14,420 --> 01:51:19,420
 Now for collision geometry, we wanted to simulate a variety of shapes,

1968
01:51:19,420 --> 01:51:22,420
 which included non-convex shapes.

1969
01:51:22,420 --> 01:51:25,420
 In order to simulate these shapes in Drake,

1970
01:51:25,420 --> 01:51:29,420
 the method we chose was to perform convex decomposition,

1971
01:51:29,420 --> 01:51:32,420
 which essentially takes a non-convex shape

1972
01:51:32,420 --> 01:51:36,420
 and turns it into a series of convex shapes that approximate that surface.

1973
01:51:36,420 --> 01:51:42,420
 Here is a video of our EWO arm executing the pipeline that we described in the earlier slides.

1974
01:51:42,420 --> 01:51:46,420
 We are taking a predefined trajectory shown in the top right of the screen

1975
01:51:46,420 --> 01:51:52,420
 and executing it over this non-convex curved geometry.

1976
01:51:52,420 --> 01:51:57,420
 As you can see, portions of this predefined motion are in contact with the surface,

1977
01:51:57,420 --> 01:52:04,420
 while portions are intentionally not in contact with the surface.

1978
01:52:04,420 --> 01:52:06,420
 In the bottom right of the screen,

1979
01:52:06,420 --> 01:52:10,420
 you can see a visualization of the trajectory that we are executing.

1980
01:52:10,420 --> 01:52:13,420
 On this slide, we show three experiments,

1981
01:52:13,420 --> 01:52:18,420
 each performing the same trajectory from the previous slide and shown on the top of this slide,

1982
01:52:18,420 --> 01:52:22,420
 but over an array of different surfaces with different curved geometry,

1983
01:52:22,420 --> 01:52:25,420
 some convex, some non-convex.

1984
01:52:25,420 --> 01:52:40,420
 As you can see, the trajectory is not in contact with the surface,

1985
01:52:40,420 --> 01:52:54,420
 but is in contact with the surface.

1986
01:52:54,420 --> 01:52:59,420
 As you can see in this slide, there exists a couple failure modes with our approach.

1987
01:52:59,420 --> 01:53:03,420
 These include when aspects of the EWO arm collide with the geometry,

1988
01:53:03,420 --> 01:53:06,420
 when the curvature of the geometry is too severe,

1989
01:53:06,420 --> 01:53:22,420
 or when the planned trajectory extends beyond the bounds of the curved geometry.

1990
01:53:22,420 --> 01:53:29,420
 We envision multiple applications to this control pipeline beyond the aspects of drawing tasks.

1991
01:53:29,420 --> 01:53:33,420
 For example, window washing on curved windows,

1992
01:53:33,420 --> 01:53:38,420
 or washing curved dishes, or even applying massages to humans.

1993
01:53:38,420 --> 01:53:43,420
 Essentially, any task where you must apply force normal to a surface,

1994
01:53:43,420 --> 01:53:48,420
 and that surface is not necessarily flat.

1995
01:53:48,420 --> 01:53:54,420
 One more thing we would like to consider in the future is the application of the EWO arm.

1996
01:53:54,420 --> 01:53:57,420
 [Applause]

1997
01:53:57,420 --> 01:54:07,420
 So you were gripping the chalk, yeah?

1998
01:54:07,420 --> 01:54:11,420
 Oh, so that I see you rolled it to one finger, because I saw one point where it came open,

1999
01:54:11,420 --> 01:54:14,420
 but that was it was welded to the finger that it moved with.

2000
01:54:14,420 --> 01:54:36,420
 Ah, you tricked me. Good. Nice.

2001
01:54:36,420 --> 01:54:40,420
 Any other questions? Yeah?

2002
01:54:40,420 --> 01:55:09,420
 [Inaudible]

2003
01:55:09,420 --> 01:55:15,420
 Okay, we're actually not far off time, but public service announcement.

2004
01:55:15,420 --> 01:55:19,420
 I don't technically have the room after four.

2005
01:55:19,420 --> 01:55:23,420
 They haven't, oftentimes people, I couldn't, I asked for it, they said no, it's booked,

2006
01:55:23,420 --> 01:55:28,420
 but a lot of, it's been booked all semester, and like three times people have come in at 4.15.

2007
01:55:28,420 --> 01:55:32,420
 So let's see. I do have a room across the hall.

2008
01:55:32,420 --> 01:55:35,420
 As a worst case, if someone comes in and looks really mad,

2009
01:55:35,420 --> 01:55:38,420
 we'll just politely go to the room across and keep going, yeah?

2010
01:55:38,420 --> 01:55:40,420
 Because there's a lot of good presentations coming up.

2011
01:55:40,420 --> 01:55:43,420
 The people who said they were arriving at 3, uh-oh.

2012
01:55:43,420 --> 01:55:46,420
 [Laughter]

2013
01:55:46,420 --> 01:55:50,420
 Oh, no, that's good.

2014
01:55:50,420 --> 01:55:58,420
 Okay, quick stretch session. That sounds good.

2015
01:55:58,420 --> 01:56:03,420
 I think there's a couple people that are already queued up here, but I can open Slack.

2016
01:56:03,420 --> 01:56:09,420
 [Inaudible]

2017
01:56:09,420 --> 01:56:13,420
 I think the students here are those who are not, who doesn't have a Slack.

2018
01:56:13,420 --> 01:56:15,420
 I actually checked the Slack.

2019
01:56:15,420 --> 01:56:17,420
 I don't think they have their project in there.

2020
01:56:17,420 --> 01:56:18,420
 Okay.

2021
01:56:18,420 --> 01:56:19,420
 [Inaudible]

2022
01:56:19,420 --> 01:56:20,420
 Okay.

2023
01:56:21,420 --> 01:56:24,420
 [Silence]

2024
01:56:24,420 --> 01:56:27,420
 [Silence]

2025
01:56:27,420 --> 01:56:30,420
 [Silence]

2026
01:56:30,420 --> 01:56:33,420
 [Silence]

2027
01:56:33,420 --> 01:56:36,420
 [Silence]

2028
01:56:36,420 --> 01:56:39,420
 [Silence]

2029
01:56:39,420 --> 01:56:42,420
 [Silence]

2030
01:56:43,420 --> 01:56:46,420
 [Silence]

2031
01:57:11,420 --> 01:57:13,420
 Is there someone here?

2032
01:57:13,420 --> 01:57:14,420
 He's peeing.

2033
01:57:14,420 --> 01:57:17,420
 [Silence]

2034
01:57:17,420 --> 01:57:20,420
 [Silence]

2035
01:57:20,420 --> 01:57:23,420
 [Silence]

2036
01:57:23,420 --> 01:57:26,420
 [Silence]

2037
01:57:26,420 --> 01:57:29,420
 [Silence]

2038
01:57:29,420 --> 01:57:31,420
 From ancient history to modern scandals,

2039
01:57:31,420 --> 01:57:35,420
 robotic chess has captured the awe and curiosity of humanity.

2040
01:57:35,420 --> 01:57:40,420
 In order to play, all systems must understand the current piece locations,

2041
01:57:40,420 --> 01:57:45,420
 generate a feasible move, and execute the move in the physical world.

2042
01:57:45,420 --> 01:57:51,420
 In addition, any simulated chess playing system must have a complete simulated environment.

2043
01:57:51,420 --> 01:57:56,420
 Chess solvers and planar pick-and-place machines have a variety of well-documented solutions.

2044
01:57:56,420 --> 01:58:03,420
 However, the issue of board perception remains open.

2045
01:58:03,420 --> 01:58:07,420
 Current perception methods include using top-mounted cameras, as shown left,

2046
01:58:07,420 --> 01:58:11,420
 or using custom-built chess boards, as shown right.

2047
01:58:11,420 --> 01:58:15,420
 Unfortunately, these methods are physically complex, awkward to install,

2048
01:58:15,420 --> 01:58:18,420
 and gather limited information about the game state.

2049
01:58:18,420 --> 01:58:21,420
 Ideally, a chess system would be physically simple,

2050
01:58:21,420 --> 01:58:27,420
 while understanding both the integer coordinates of all pieces, as well as their spatial location.

2051
01:58:27,420 --> 01:58:30,420
 We now present ChessBot.

2052
01:58:30,420 --> 01:58:35,420
 ChessBot is a deep perception and manipulation system for all your robotic chess playing needs.

2053
01:58:35,420 --> 01:58:38,420
 Using a single, side-mounted RGBD sensor,

2054
01:58:38,420 --> 01:58:41,420
 ChessBot can read the state of an arbitrary simulated chess board,

2055
01:58:41,420 --> 01:58:46,420
 producing both exact piece locations and reconstructed point clouds of all pieces,

2056
01:58:46,420 --> 01:58:48,420
 including occluded geometry.

2057
01:58:48,420 --> 01:58:58,420
 Using this, ChessBot determines the optimal next move and executes it using its robotic arm.

2058
01:58:58,420 --> 01:59:01,420
 ChessBot is designed as a continuous loop.

2059
01:59:01,420 --> 01:59:03,420
 First, the user enters their move.

2060
01:59:03,420 --> 01:59:08,420
 Then, this move is sent to the simulator, which teleports the user's piece to the appropriate location.

2061
01:59:08,420 --> 01:59:14,420
 Then, ChessBot utilizes its deep perception system to segment and reconstruct the modified chess board.

2062
01:59:14,420 --> 01:59:18,420
 This allows ChessBot to determine what move the user made.

2063
01:59:18,420 --> 01:59:25,420
 Finally, ChessBot queries its chess solver to generate a feasible move which it promptly executes.

2064
01:59:25,420 --> 01:59:29,420
 Let's take a closer look at ChessBot's perception system.

2065
01:59:29,420 --> 01:59:33,420
 We first separate an RGBD image into a depth mask and color image.

2066
01:59:33,420 --> 01:59:39,420
 Using a fine-tuned mask-rcnn, we segment and label the RGBD image.

2067
01:59:39,420 --> 01:59:42,420
 This allows us to create a set of partial point clouds.

2068
01:59:42,420 --> 01:59:48,420
 Next, we use the labels from mask-rcnn to determine what piece type each point cloud belongs to.

2069
01:59:48,420 --> 01:59:54,420
 We use ICP to fit a reference point cloud corresponding to this type of piece to the original partial point clouds.

2070
01:59:54,420 --> 01:59:59,420
 Then, we use the point clouds for board inference and manipulation.

2071
01:59:59,420 --> 02:00:04,420
 However, no system is perfect, including ChessBot.

2072
02:00:04,420 --> 02:00:10,420
 In testing, we found two primary failure modes, kinematic failures and perception failures.

2073
02:00:10,420 --> 02:00:14,420
 On the left, we see that a kinematic failure resulted in a piece being knocked over.

2074
02:00:14,420 --> 02:00:18,420
 On the right, we see that a piece was perceived in the incorrect location.

2075
02:00:18,420 --> 02:00:25,420
 While these are annoying for gameplay, we think that more sophisticated move detection algorithms may circumvent these issues.

2076
02:00:25,420 --> 02:00:30,420
 We hope you enjoyed learning about ChessBot. Thank you for watching.

2077
02:00:31,420 --> 02:00:34,420
 [Applause]

2078
02:00:34,420 --> 02:00:37,420
 That was a tour de force. Very impressive.

2079
02:00:37,420 --> 02:00:39,420
 Which was the hardest part?

2080
02:00:39,420 --> 02:01:07,420
 [Inaudible]

2081
02:01:07,420 --> 02:01:10,420
 And why did you use WSG instead of the Panda Gripper?

2082
02:01:10,420 --> 02:01:31,420
 [Inaudible]

2083
02:01:31,420 --> 02:01:33,420
 Anybody else?

2084
02:01:33,420 --> 02:02:00,420
 [Inaudible]

2085
02:02:00,420 --> 02:02:06,420
 Awesome. Okay. Reuben's back. Let's do Reuben.

2086
02:02:06,420 --> 02:02:13,420
 Hi. My name is Reuben Castro, and I'm here to present my final project for robotic manipulation, which is a volleyball-setting robot.

2087
02:02:13,420 --> 02:02:18,420
 Now, this is interesting because so far, we've been focusing mostly on quasi-static tasks.

2088
02:02:18,420 --> 02:02:23,420
 But I suspect that as we get into more complex tool usage, we will need to be agile and more powerful.

2089
02:02:23,420 --> 02:02:27,420
 And the question asked when it comes to this is, what happens if the pipette slips?

2090
02:02:27,420 --> 02:02:31,420
 If that's the case, in 0.1 seconds, it could fall around 5 centimeters.

2091
02:02:31,420 --> 02:02:36,420
 So if we can't re-grasp it in time, gravity is in control, and we could reach failure.

2092
02:02:36,420 --> 02:02:40,420
 Now, volleyball itself is pretty interesting because it's fast, it's forceful, and it's fun.

2093
02:02:40,420 --> 02:02:46,420
 We only have around 80 milliseconds to control the ball, and we're reaching the upper bound of our torque limits.

2094
02:02:46,420 --> 02:02:54,420
 There's three main components to this. We need to sense where the ball is coming from, we need to absorb it, and then we need to relaunch it to the desired position.

2095
02:02:54,420 --> 02:03:03,420
 My research focuses on making actuators for robotic hands, so I took the parameters from my fingers and input them as parameters for the simulation itself.

2096
02:03:03,420 --> 02:03:08,420
 There are three main legos to our project. That's the manipulator, the robot arm, and the ball tracker.

2097
02:03:08,420 --> 02:03:12,420
 The manipulator itself consists of two fingers with two degrees of freedom each.

2098
02:03:12,420 --> 02:03:19,420
 There's a Y-shape at the end for stability and centering while catching the ball, and running operation space impedance control on them.

2099
02:03:19,420 --> 02:03:28,420
 This allows the ball to act as a mass spring damper system where we can dynamically change the stiffness depending on how much energy we want to input into the ball.

2100
02:03:28,420 --> 02:03:32,420
 Originally, we were hoping to just use the fingers to launch the ball.

2101
02:03:32,420 --> 02:03:38,420
 However, we quickly ran into torque limits, which means that we needed to use the arm itself to also launch the ball.

2102
02:03:38,420 --> 02:03:46,420
 The robot arm is an e-worm, which is capable of reaching a desired pose by using a Cartesian space PID controller.

2103
02:03:47,420 --> 02:03:53,420
 Now, the fingers by themselves cannot input enough energy into the ball to get it to the desired height,

2104
02:03:53,420 --> 02:04:00,420
 so we have to figure out what speed we need to launch the arm at, and we can do that using the simple energy equations.

2105
02:04:00,420 --> 02:04:08,420
 From there, we simply follow a linear trajectory using constant acceleration in the desired direction of the ball until we reach the velocity launch.

2106
02:04:08,420 --> 02:04:15,420
 Now, the ball tracker is pretty simple. It fits a parabolic trajectory to a few simple points in the air.

2107
02:04:15,420 --> 02:04:19,420
 We assume perfect state estimation, and we have a tolerance for around 3 cm.

2108
02:04:19,420 --> 02:04:25,420
 Now, we have all the pieces together, and we have a robot that can set the value of all.

2109
02:04:25,420 --> 02:04:38,420
 Out of 10 trials that we did with different initial parameters, we averaged around 7 sets with a high of 12 sets and a low of 2 sets.

2110
02:04:38,420 --> 02:04:45,420
 The output height was not as accurate as we were hoping it to be. We failed to reach the desired height by around 0.5 m.

2111
02:04:45,420 --> 02:04:52,420
 In conclusion, we were able to show a robotic system that uses constraints based on real-world manipulators,

2112
02:04:52,420 --> 02:04:55,420
 and it's capable of playing the agile task of volleyball.

2113
02:04:55,420 --> 02:05:02,420
 Second of all, we have seen that impedance control plus compliant hardware has allowed the high level to be simpler.

2114
02:05:02,420 --> 02:05:06,420
 We are simply using constant acceleration, linear trajectory.

2115
02:05:07,420 --> 02:05:08,420
 Good.

2116
02:05:08,420 --> 02:05:13,420
 Any questions for Ruben?

2117
02:05:13,420 --> 02:05:20,420
 The collision geometry of the end effector.

2118
02:05:20,420 --> 02:05:27,420
 Do you have it with you?

2119
02:05:27,420 --> 02:05:32,420
 We have like 30 seconds, but if you could hold it up and show.

2120
02:05:35,420 --> 02:05:36,420
 I see.

2121
02:05:36,420 --> 02:05:41,420
 Nice.

2122
02:06:01,420 --> 02:06:05,420
 And do you think the impedance control was essential? You couldn't have done the same thing with position control, you think?

2123
02:06:05,420 --> 02:06:07,420
 I mean, sorry, at the arm.

2124
02:06:07,420 --> 02:06:14,420
 I think that the impedance control gets a lot of confidence, mainly because we know the fact that we've got the end effector.

2125
02:06:14,420 --> 02:06:19,420
 We can't say that the control got throughout the whole time the model.

2126
02:06:19,420 --> 02:06:26,420
 So we do that, and we do the projection simulation, but it's too easy to say.

2127
02:06:26,420 --> 02:06:28,420
 We can't do it in real time.

2128
02:06:28,420 --> 02:06:34,420
 Awesome. Thank you.

2129
02:06:35,420 --> 02:06:38,420
 Okay, so Jacob next.

2130
02:06:38,420 --> 02:06:43,420
 Start.

2131
02:06:43,420 --> 02:06:48,420
 Hi, everyone. This is me and Thomason. I'm Biwei Yan.

2132
02:06:48,420 --> 02:06:53,420
 Today we're talking about large language models for abstract pick and place task planning in Drake.

2133
02:06:56,420 --> 02:07:09,420
 So the introduction and motivation is that abstract task planning in robotics is really hard, and that humans take a lot of our abstract and semantic understanding of tasks and the way we communicate tasks for granted.

2134
02:07:09,420 --> 02:07:13,420
 These are in no way obvious to the robot receiving the texts.

2135
02:07:13,420 --> 02:07:23,420
 And so the idea is to use a large language model, such as GPT-3, which is trained on vast amounts of text data that encode human reasoning into the text data.

2136
02:07:23,420 --> 02:07:34,420
 And these large language models demonstrate impressive high-level abstract reasoning capability because of their training set to act as a robot's brain.

2137
02:07:34,420 --> 02:07:40,420
 Our approach is use abstract human-defined task planning in Drake.

2138
02:07:40,420 --> 02:07:48,420
 We prompt the large language model with environment context and the goal, and we don't do any other prompt engineering, which is kind of surprising.

2139
02:07:48,420 --> 02:07:53,420
 The large language model generates a series of pick and place actions.

2140
02:07:53,420 --> 02:07:59,420
 Note that thanks to large language models, output is virus and is kind of neural language.

2141
02:07:59,420 --> 02:08:05,420
 So we need some functions to standardize the output and extract pick and place specific actions.

2142
02:08:07,420 --> 02:08:12,420
 So here's an example of a prompt that we provide the manipulator.

2143
02:08:12,420 --> 02:08:19,420
 I think it's important to note that our Drake environment is just a modification of the Chapter 5 bin picking example.

2144
02:08:19,420 --> 02:08:25,420
 And so we just have a simple tabletop environment with our manipulator in the center and a few objects that are generated around it.

2145
02:08:25,420 --> 02:08:30,420
 So the context we provide the LLM are the objects in the scene.

2146
02:08:30,420 --> 02:08:34,420
 So in this case, we have the four blocks and the four plates or disks.

2147
02:08:34,420 --> 02:08:40,420
 And then the goal that we provide to the LLM is that we want to sort the blocks onto the disks.

2148
02:08:40,420 --> 02:08:49,420
 And while not explicitly stated, the idea here is that the LLM will notice the relation between the blocks and the disks and place the blocks on the corresponding color.

2149
02:08:49,420 --> 02:08:53,420
 And not surprisingly, they did what we guessed.

2150
02:08:53,420 --> 02:09:04,420
 They generate step-by-step instructions about where to put each block exactly by their color, which is great for translating into instructions.

2151
02:09:04,420 --> 02:09:08,420
 So let's look at the example of our videos.

2152
02:09:08,420 --> 02:09:10,420
 There is actually three tasks.

2153
02:09:10,420 --> 02:09:20,420
 And the first task is to place the blocks into a square formation and see it.

2154
02:09:20,420 --> 02:09:23,420
 Understand the square has four corners.

2155
02:09:23,420 --> 02:09:30,420
 The second task is to sort the blocks onto the disks, which we have already illustrated before.

2156
02:09:30,420 --> 02:09:32,420
 It is sorted by color.

2157
02:09:32,420 --> 02:09:51,420
 The final task is to stack all the blocks on the red disk, which demonstrates and understands what does it mean by stack and what does it mean by the geometric relationships.

2158
02:09:51,420 --> 02:10:02,420
 So the most important takeaways from this project are that with a simple helper function, LLMs are capable of kind of directly producing this series of pick and place locations for your planner.

2159
02:10:02,420 --> 02:10:08,420
 And so this is surprisingly without prompt engineering, so you can kind of just use a large language model out of the box for this.

2160
02:10:08,420 --> 02:10:20,420
 And so you can kind of treat this as a trajectory providing black box because you can compute the trajectories on each of the provided pick and place locations.

2161
02:10:20,420 --> 02:10:28,420
 And so each time your planner then completes one of those trajectories, we can then re-prompt the LLM for a new task.

2162
02:10:28,420 --> 02:10:39,420
 And because we're only operating with trajectories here, N-Drake and the rest is handled by the LLM, this enables more complex tasks because they're often parameterized just in the trajectory.

2163
02:10:39,420 --> 02:10:48,420
 And so what's really cool about this is this enables real-time --

2164
02:10:48,420 --> 02:10:49,420
 That's crazy.

2165
02:10:49,420 --> 02:10:54,420
 So maybe I missed the simple helper function part, but how do you not have to do SACAN kind of things?

2166
02:10:54,420 --> 02:10:59,420
 How did it come up with the actions you know how to take?

2167
02:10:59,420 --> 02:11:20,420
 [ Inaudible ]

2168
02:11:20,420 --> 02:11:25,420
 So the helper function is you translating a slightly more diverse text into your pick and place for primitives.

2169
02:11:25,420 --> 02:11:26,420
 Yeah.

2170
02:11:26,420 --> 02:11:36,420
 [ Inaudible ]

2171
02:11:36,420 --> 02:11:37,420
 Awesome.

2172
02:11:37,420 --> 02:11:39,420
 Any other questions?

2173
02:11:39,420 --> 02:11:54,420
 [ Inaudible ]

2174
02:11:54,420 --> 02:11:55,420
 Cool.

2175
02:11:55,420 --> 02:11:56,420
 All right.

2176
02:11:56,420 --> 02:11:59,420
 I'm feeling good about nobody -- Shen, is there like a line of people about to come into the room?

2177
02:11:59,420 --> 02:12:01,420
 [ Inaudible ]

2178
02:12:01,420 --> 02:12:02,420
 Yeah, yeah, yeah.

2179
02:12:02,420 --> 02:12:03,420
 You're good.

2180
02:12:03,420 --> 02:12:04,420
 You're good.

2181
02:12:04,420 --> 02:12:05,420
 Okay.

2182
02:12:05,420 --> 02:12:07,420
 I think Catherine is --

2183
02:12:07,420 --> 02:12:16,420
 Hi, everyone, and welcome to Caribot, which is our final project in making an EWA draw caricatures in simulation using Drake and MeshCat.

2184
02:12:16,420 --> 02:12:22,420
 So we decided to choose caricature drawing as our task because it presents a really interesting manipulation problem.

2185
02:12:22,420 --> 02:12:34,420
 So as you can see in this video demonstrated here, caricatures inherently have a technical and creative component in that you really want to emphasize certain features of the image or caricature you're trying to present.

2186
02:12:34,420 --> 02:12:42,420
 At the same time, you can't lose recognizability without sacrificing, you know, the comical nature of the caricature itself.

2187
02:12:42,420 --> 02:12:50,420
 So for our project, we really focused on the technical precision required to generate and then draw a caricature of a given input image.

2188
02:12:50,420 --> 02:13:01,420
 And so in the future, this could have use cases including facial imaging drawing where drawing features are important, like police suspect drawings or courtroom sketches or even drawing AI-generated faces.

2189
02:13:01,420 --> 02:13:07,420
 And the precision itself could be useful in non-facial drawings like architecture or manufacturing.

2190
02:13:07,420 --> 02:13:09,420
 Now to move on to our approach.

2191
02:13:09,420 --> 02:13:12,420
 So Caribot consists of two subsystems.

2192
02:13:12,420 --> 02:13:16,420
 So given an input image, we feed it into our image processing system.

2193
02:13:16,420 --> 02:13:25,420
 And then that generates a set of trajectory points that the manipulation system then uses to draw the actual output image.

2194
02:13:25,420 --> 02:13:31,420
 Now, the first step in our image processing subsystem is to create a caricature from a given input image.

2195
02:13:31,420 --> 02:13:41,420
 And to do this, we refer to the CariB paper by Gu et al and recreated their machine learning model in Google Colab.

2196
02:13:41,420 --> 02:13:45,420
 Using this model, we generated caricatures of different input images.

2197
02:13:45,420 --> 02:13:55,420
 And as you can see, there's a variety of different warping techniques that are applied pretty randomly to generate a diverse set of possible caricatures.

2198
02:13:55,420 --> 02:13:58,420
 Here are some other results that we have.

2199
02:13:58,420 --> 02:14:09,420
 And basically, our model -- our pipeline displays five randomly generated caricatures and allows the user to select one that they want to draw, such as this one.

2200
02:14:09,420 --> 02:14:15,420
 The second step was to process the caricaturized image using the Canny edge detection algorithm,

2201
02:14:15,420 --> 02:14:23,420
 which through testing we determined was the best way to pick out a set of definitive contours that our robot should draw from an image.

2202
02:14:23,420 --> 02:14:28,420
 So the steps of this algorithm, just very quickly, were first to convert a picture to grayscale.

2203
02:14:28,420 --> 02:14:33,420
 Secondly, to apply Gaussian blurring to reduce noise in the image.

2204
02:14:33,420 --> 02:14:39,420
 Third, using intensity gradient, definitive high contrast edges were identified.

2205
02:14:39,420 --> 02:14:48,420
 And then finally, using some kind of thresholding process, more edges were more clearly defined.

2206
02:14:48,420 --> 02:14:52,420
 And basically, here are some results of the Canny edge detection algorithm.

2207
02:14:52,420 --> 02:14:59,420
 As you can see, the new image has a bunch of lines that are somewhat unnecessary to fully recreating the image.

2208
02:14:59,420 --> 02:15:08,420
 So to deal with this, we also filtered some of these contours by arc length to remove small ones that were not necessary and end up with a better picture of the subject.

2209
02:15:08,420 --> 02:15:14,420
 And this was the original input photo that we ran the algorithm on.

2210
02:15:14,420 --> 02:15:24,420
 Now, moving on to the manipulation system, we'll also add in a video of Caribot drawing a caricature on the side here, since the process takes quite a bit of time.

2211
02:15:24,420 --> 02:15:32,420
 For Caribot's manipulation system, we took Russ's drawing example, combined it with the RobotPainter class from a previous pset, and changed the controller to Diff IK.

2212
02:15:32,420 --> 02:15:39,420
 We welded the chalk to the robot's right finger, and when the chalk comes into contact with the chalkboard, a line is drawn following the robot's trajectory.

2213
02:15:39,420 --> 02:15:49,420
 We chose to base it off of the RobotPainter code, since the pset featured the EWAP following given points, which encapsulates one of the main features of Caribot.

2214
02:15:49,420 --> 02:15:57,420
 To generate the actual trajectory, we take the points from edge detection and then apply scaling, offsets, and rotations in order to translate the image into the world frame.

2215
02:15:57,420 --> 02:16:05,420
 We also had to insert additional lift points at the start and end of the line in order for the EWAP to transition between distinct lines without drawing on the chalkboard.

2216
02:16:05,420 --> 02:16:11,420
 Here, we can see the output image of the EWAP drawing a caricature of the rock.

2217
02:16:11,420 --> 02:16:15,420
 We also faced many challenges in both subsystems during the project.

2218
02:16:15,420 --> 02:16:26,420
 For the image processing subsystem, we had trouble filtering out the lines that we did not want, since it varied quite a bit between images and often produced very questionable results.

2219
02:16:26,420 --> 02:16:36,420
 Then, during the manipulation, we had a lot of trouble controlling how the robot arm lifted up from the chalkboard, which caused a lot of extraneous lines.

2220
02:16:36,420 --> 02:16:40,420
 And now that Caribot is done drawing, let's take a look at the results.

2221
02:16:40,420 --> 02:16:47,420
 Here we have Russ in his many stages, and here's the final result.

2222
02:16:47,420 --> 02:16:53,420
 In the future, there are many areas of improvement for Caribot to work on.

2223
02:16:53,420 --> 02:17:04,420
 Here are some of them, such as noise reduction and more cartoon-like caricatures, as well as different applications for Caribot, such as using different art utensils and doing non-facial drawing tasks.

2224
02:17:04,420 --> 02:17:06,420
 Thank you for your time.

2225
02:17:06,420 --> 02:17:11,420
 [Applause]

2226
02:17:11,420 --> 02:17:13,420
 I'm sorry, I don't look like the rock.

2227
02:17:13,420 --> 02:17:15,420
 [Laughter]

2228
02:17:15,420 --> 02:17:19,420
 So how did you go from – so the edge detector gives you an image, right?

2229
02:17:19,420 --> 02:17:21,420
 So how do you get the strokes from the image?

2230
02:17:21,420 --> 02:17:23,420
 I missed that part.

2231
02:17:23,420 --> 02:17:27,420
 [Unintelligible]

2232
02:17:27,420 --> 02:17:28,420
 Oh, it does?

2233
02:17:28,420 --> 02:17:29,420
 Oh, okay.

2234
02:17:29,420 --> 02:17:31,420
 Right from canning?

2235
02:17:31,420 --> 02:18:00,420
 [Unintelligible]

2236
02:18:00,420 --> 02:18:14,420
 Super nice.

2237
02:18:14,420 --> 02:18:15,420
 Any other questions?

2238
02:18:15,420 --> 02:18:44,420
 [Unintelligible]

2239
02:18:44,420 --> 02:19:08,420
 Okay, let's keep moving.

2240
02:19:08,420 --> 02:19:09,420
 That's awesome.

2241
02:19:09,420 --> 02:19:13,420
 Thank you.

2242
02:19:13,420 --> 02:19:24,420
 My name is Lucian Kouroubias, and –

2243
02:19:24,420 --> 02:19:34,420
 To find when the projectile first gets near enough to the robot, I use a linear optimization where I minimize the decision variable t while constraining the projectile to stay within reach of the robot.

2244
02:19:34,420 --> 02:19:38,420
 This effectively finds the earliest time where we could possibly grab the object.

2245
02:19:38,420 --> 02:19:47,420
 By instead maximizing t with the same constraints, we can instead find the exit time of the projectile, which is the last possible moment where we could catch it.

2246
02:19:47,420 --> 02:19:57,420
 Now that we have the time of entry for the projectile, and since we know that the projectile has a constant orientation and geometry, we can calculate a comfortable pre-grasp pose for the object.

2247
02:19:57,420 --> 02:20:06,420
 A comfortable pre-grasp pose is one where the gripper is always facing away from the robot, so as to maximize range and movement capabilities while catching.

2248
02:20:06,420 --> 02:20:13,420
 The gripper trajectory module calculates a full end-effector trajectory from initial pose to final grasp pose.

2249
02:20:13,420 --> 02:20:20,420
 Phase 1 is the transition from initial configuration to pre-grasp pose, which was calculated by the previous module.

2250
02:20:20,420 --> 02:20:25,420
 For the position component, I linearly –

2251
02:20:25,420 --> 02:20:28,420
 As there is no obstacles to avoid.

2252
02:20:28,420 --> 02:20:34,420
 For the orientation, I used a quaternion slurp to avoid gimbal lock and find a smooth trajectory.

2253
02:20:34,420 --> 02:20:41,420
 Phase 2 is the transition from pre-grasp to grasp pose, where we must track the projectile's motion while moving closer in order to grab the object.

2254
02:20:41,420 --> 02:20:49,420
 The position component can be determined by the physics equation from the first module, with an additional offset as the gripper moves closer to the projectile.

2255
02:20:49,420 --> 02:20:54,420
 The orientation at this point is constant, because there is no rotation in the object.

2256
02:20:54,420 --> 02:21:02,420
 The final module is responsible for translating the end-effector trajectory into an executable joint trajectory to send to the robot.

2257
02:21:02,420 --> 02:21:07,420
 The approach used in this system is to solve an inverse kinematic optimization problem,

2258
02:21:07,420 --> 02:21:13,420
 which attempts to minimize the distance between the current joint configuration and a pre-set nominal configuration,

2259
02:21:13,420 --> 02:21:16,420
 while constraining motion to follow the end-effector trajectory.

2260
02:21:16,420 --> 02:21:23,420
 When forming the bounds for this optimization, I was more lenient with the upper bound of the z-component, as projectiles normally came from above.

2261
02:21:23,420 --> 02:21:27,420
 Now we can observe the full system at work.

2262
02:21:27,420 --> 02:21:30,420
 [Music]

2263
02:21:30,420 --> 02:21:33,420
 [Music]

2264
02:21:33,420 --> 02:21:36,420
 [Music]

2265
02:21:36,420 --> 02:21:41,420
 [Music]

2266
02:21:41,420 --> 02:21:46,420
 [Music]

2267
02:21:46,420 --> 02:21:53,420
 [Music]

2268
02:21:53,420 --> 02:22:01,420
 [Music]

2269
02:22:02,420 --> 02:22:05,420
 [Music]

2270
02:22:05,420 --> 02:22:16,420
 Thanks for watching, and I hope you found it interesting.

2271
02:22:16,420 --> 02:22:19,420
 [Applause]

2272
02:22:19,420 --> 02:22:24,420
 Awesome. I think your beginning was the one with the chopsticks, wasn't it?

2273
02:22:24,420 --> 02:22:27,420
 That was hilarious, I'm sorry.

2274
02:22:27,420 --> 02:22:30,420
 Any questions?

2275
02:22:31,420 --> 02:22:36,420
 You didn't deal with the, I mean, it looked like you were just barely staying in the fingers.

2276
02:22:36,420 --> 02:22:39,420
 [Inaudible]

2277
02:22:39,420 --> 02:22:42,420
 [Inaudible]

2278
02:22:42,420 --> 02:22:45,420
 [Inaudible]

2279
02:22:45,420 --> 02:22:48,420
 [Inaudible]

2280
02:22:48,420 --> 02:22:51,420
 [Inaudible]

2281
02:22:51,420 --> 02:22:54,420
 [Inaudible]

2282
02:22:54,420 --> 02:22:57,420
 [Inaudible]

2283
02:22:57,420 --> 02:23:00,420
 [Inaudible]

2284
02:23:00,420 --> 02:23:03,420
 [Inaudible]

2285
02:23:03,420 --> 02:23:06,420
 I see.

2286
02:23:06,420 --> 02:23:09,420
 [Inaudible]

2287
02:23:09,420 --> 02:23:12,420
 [Inaudible]

2288
02:23:12,420 --> 02:23:15,420
 [Inaudible]

2289
02:23:15,420 --> 02:23:18,420
 [Inaudible]

2290
02:23:18,420 --> 02:23:21,420
 Awesome. Very nice.

2291
02:23:21,420 --> 02:23:24,420
 [Inaudible]

2292
02:23:24,420 --> 02:23:27,420
 [Inaudible]

2293
02:23:27,420 --> 02:23:30,420
 [Inaudible]

2294
02:23:30,420 --> 02:23:33,420
 [Inaudible]

2295
02:23:33,420 --> 02:23:36,420
 [Inaudible]

2296
02:23:36,420 --> 02:23:39,420
 [Inaudible]

2297
02:23:39,420 --> 02:23:42,420
 [Inaudible]

2298
02:23:42,420 --> 02:23:45,420
 [Inaudible]

2299
02:23:45,420 --> 02:23:48,420
 [Inaudible]

2300
02:23:48,420 --> 02:23:51,420
 [Inaudible]

2301
02:23:51,420 --> 02:23:54,420
 [Inaudible]

2302
02:23:54,420 --> 02:23:57,420
 [Inaudible]

2303
02:23:57,420 --> 02:24:00,420
 [Inaudible]

2304
02:24:00,420 --> 02:24:03,420
 Awesome.

2305
02:24:03,420 --> 02:24:06,420
 I'm sorry to keep moving, but I don't see Tom.

2306
02:24:06,420 --> 02:24:10,420
 You Tom, I know. The other Tom, I don't see here.

2307
02:24:10,420 --> 02:24:13,420
 Is Kevra here?

2308
02:24:13,420 --> 02:24:16,420
 Okay, I'm going to go to

2309
02:24:16,420 --> 02:24:21,420
 Shruti's here.

2310
02:24:21,420 --> 02:24:24,420
 All right.

2311
02:24:24,420 --> 02:24:27,420
 [Inaudible]

2312
02:24:27,420 --> 02:24:30,420
 [Inaudible]

2313
02:24:30,420 --> 02:24:33,420
 [Inaudible]

2314
02:24:33,420 --> 02:24:36,420
 [Inaudible]

2315
02:24:36,420 --> 02:24:39,420
 [Inaudible]

2316
02:24:39,420 --> 02:24:42,420
 [Inaudible]

2317
02:24:42,420 --> 02:24:45,420
 [Inaudible]

2318
02:24:45,420 --> 02:24:48,420
 [Inaudible]

2319
02:24:48,420 --> 02:24:51,420
 [Inaudible]

2320
02:24:51,420 --> 02:24:54,420
 [Inaudible]

2321
02:24:54,420 --> 02:24:57,420
 [Inaudible]

2322
02:24:57,420 --> 02:25:00,420
 [Inaudible]

2323
02:25:00,420 --> 02:25:03,420
 [Inaudible]

2324
02:25:03,420 --> 02:25:06,420
 [Inaudible]

2325
02:25:06,420 --> 02:25:09,420
 [Inaudible]

2326
02:25:09,420 --> 02:25:12,420
 [Inaudible]

2327
02:25:12,420 --> 02:25:15,420
 [Inaudible]

2328
02:25:15,420 --> 02:25:18,420
 [Inaudible]

2329
02:25:18,420 --> 02:25:21,420
 [Inaudible]

2330
02:25:21,420 --> 02:25:24,420
 [Inaudible]

2331
02:25:24,420 --> 02:25:27,420
 [Inaudible]

2332
02:25:27,420 --> 02:25:30,420
 [Inaudible]

2333
02:25:30,420 --> 02:25:33,420
 [Inaudible]

2334
02:25:33,420 --> 02:25:36,420
 [Inaudible]

2335
02:25:36,420 --> 02:25:39,420
 [Inaudible]

2336
02:25:39,420 --> 02:25:42,420
 [Inaudible]

2337
02:25:42,420 --> 02:25:45,420
 [Inaudible]

2338
02:25:45,420 --> 02:25:48,420
 [Inaudible]

2339
02:25:48,420 --> 02:25:51,420
 [Inaudible]

2340
02:25:51,420 --> 02:25:54,420
 [Inaudible]

2341
02:25:54,420 --> 02:25:57,420
 [Inaudible]

2342
02:25:57,420 --> 02:26:00,420
 [Inaudible]

2343
02:26:00,420 --> 02:26:03,420
 [Inaudible]

2344
02:26:03,420 --> 02:26:06,420
 [Inaudible]

2345
02:26:06,420 --> 02:26:09,420
 [Inaudible]

2346
02:26:09,420 --> 02:26:12,420
 [Inaudible]

2347
02:26:12,420 --> 02:26:15,420
 [Inaudible]

2348
02:26:15,420 --> 02:26:18,420
 [Inaudible]

2349
02:26:18,420 --> 02:26:21,420
 [Inaudible]

2350
02:26:21,420 --> 02:26:24,420
 [Inaudible]

2351
02:26:24,420 --> 02:26:27,420
 [Inaudible]

2352
02:26:27,420 --> 02:26:30,420
 [Inaudible]

2353
02:26:30,420 --> 02:26:33,420
 [Inaudible]

2354
02:26:33,420 --> 02:26:36,420
 [Inaudible]

2355
02:26:36,420 --> 02:26:39,420
 [Inaudible]

2356
02:26:39,420 --> 02:26:42,420
 [Inaudible]

2357
02:26:42,420 --> 02:26:45,420
 [Inaudible]

2358
02:26:45,420 --> 02:26:48,420
 [Inaudible]

2359
02:26:48,420 --> 02:26:51,420
 [Inaudible]

2360
02:26:51,420 --> 02:26:54,420
 [Inaudible]

2361
02:26:54,420 --> 02:26:57,420
 [Inaudible]

2362
02:26:57,420 --> 02:27:00,420
 [Inaudible]

2363
02:27:00,420 --> 02:27:03,420
 [Inaudible]

2364
02:27:03,420 --> 02:27:06,420
 [Inaudible]

2365
02:27:06,420 --> 02:27:09,420
 [Inaudible]

2366
02:27:09,420 --> 02:27:12,420
 [Inaudible]

2367
02:27:12,420 --> 02:27:15,420
 [Inaudible]

2368
02:27:15,420 --> 02:27:18,420
 [Inaudible]

2369
02:27:18,420 --> 02:27:21,420
 [Inaudible]

2370
02:27:21,420 --> 02:27:24,420
 [Inaudible]

2371
02:27:24,420 --> 02:27:27,420
 [Inaudible]

2372
02:27:27,420 --> 02:27:30,420
 [Inaudible]

2373
02:27:30,420 --> 02:27:33,420
 [Inaudible]

2374
02:27:33,420 --> 02:27:36,420
 [Inaudible]

2375
02:27:36,420 --> 02:27:39,420
 [Inaudible]

2376
02:27:39,420 --> 02:27:42,420
 [Inaudible]

2377
02:27:42,420 --> 02:27:45,420
 [Inaudible]

2378
02:27:45,420 --> 02:27:48,420
 [Inaudible]

2379
02:27:48,420 --> 02:27:51,420
 [Inaudible]

2380
02:27:51,420 --> 02:27:54,420
 [Inaudible]

2381
02:27:54,420 --> 02:27:57,420
 [Inaudible]

2382
02:27:57,420 --> 02:28:00,420
 [Inaudible]

2383
02:28:00,420 --> 02:28:03,420
 [Inaudible]

2384
02:28:03,420 --> 02:28:06,420
 [Inaudible]

2385
02:28:06,420 --> 02:28:09,420
 [Inaudible]

2386
02:28:09,420 --> 02:28:12,420
 [Inaudible]

2387
02:28:12,420 --> 02:28:15,420
 [Inaudible]

2388
02:28:15,420 --> 02:28:18,420
 [Inaudible]

2389
02:28:18,420 --> 02:28:21,420
 [Inaudible]

2390
02:28:21,420 --> 02:28:24,420
 [Inaudible]

2391
02:28:24,420 --> 02:28:27,420
 [Inaudible]

2392
02:28:27,420 --> 02:28:30,420
 [Inaudible]

2393
02:28:30,420 --> 02:28:33,420
 [Inaudible]

2394
02:28:33,420 --> 02:28:36,420
 [Inaudible]

2395
02:28:36,420 --> 02:28:39,420
 [Inaudible]

2396
02:28:39,420 --> 02:28:42,420
 [Inaudible]

2397
02:28:42,420 --> 02:28:48,420
 Not that easy. Even the Rubik's Cube, if you think about the mechanism for the Rubik's Cube, it's actually a pretty slick mechanism.

2398
02:28:48,420 --> 02:28:51,420
 Any other questions?

2399
02:28:51,420 --> 02:28:54,420
 Okay, I think

2400
02:28:54,420 --> 02:28:57,420
 [Inaudible]

2401
02:28:57,420 --> 02:29:00,420
 [Inaudible]

2402
02:29:00,420 --> 02:29:03,420
 Hi, I'm Karolina. My partner was Marissa.

2403
02:29:03,420 --> 02:29:08,420
 We're doing a project on egg breaking, cracking eggs with inverse kinematics.

2404
02:29:08,420 --> 02:29:15,420
 So, there have been other projects that dealt with eggs, such as cooking and baking.

2405
02:29:15,420 --> 02:29:26,420
 One of the most noteworthy is Yoshida et al. that used one arm to crack the egg against the side of a pan and dragging the half of the shell that it was holding backwards and using the pan as a leverage point to separate the shells.

2406
02:29:26,420 --> 02:29:39,420
 In most cases, though, they disregard the process of cracking eggs by using a separate machine to either break the eggs entirely or using premade mixes, which include powdered eggs.

2407
02:29:39,420 --> 02:29:45,420
 The egg breaker seeks to emulate the more intuitive way in which humans may crack eggs.

2408
02:29:45,420 --> 02:29:54,420
 A trajectory of poses through the entire motion is created with varying speeds throughout to move controllably yet create enough force to crack the egg when hit but not to crush it.

2409
02:29:54,420 --> 02:29:59,420
 The trajectory is followed by generating IWA arm joint positions with inverse kinematics.

2410
02:29:59,420 --> 02:30:08,420
 On the left, we have our simple trajectory, which goes directly to the egg, but it enters at a non-ideal angle and doesn't allow for a nice grabbing position.

2411
02:30:08,420 --> 02:30:16,420
 On the right, we have another trajectory, but instead of following the trajectory nicely, our arms, as you can see, bounce back and forth.

2412
02:30:16,420 --> 02:30:25,420
 The egg breaker calculates the trajectory from intermediate poses and timestamps and then uses inverse kinematics to follow this trajectory.

2413
02:30:25,420 --> 02:30:43,420
 We control the velocity of motion as well by timing out our timestamps, which allows us to pick up the egg without going too fast and pushing it away and hit the egg quickly enough that it breaks the egg, but not too fast that it crushes it entirely.

2414
02:30:43,420 --> 02:30:51,420
 Additionally, we are changing the angles of the grippers to pick up and empty the egg at different points of the trajectory.

2415
02:30:51,420 --> 02:30:56,420
 This trajectory is shown as follows here.

2416
02:30:56,420 --> 02:30:59,420
 We start out by initializing it.

2417
02:30:59,420 --> 02:31:07,420
 We move directly above the egg, but not going directly to it, so that way we don't crash into the egg or in the bowl at a weird angle.

2418
02:31:07,420 --> 02:31:10,420
 Then we are lowering it to the egg level.

2419
02:31:10,420 --> 02:31:21,420
 Here we wait for the grippers to close around the egg by putting two subsequent positions at the same time step, or different time steps, same position, right in a row.

2420
02:31:21,420 --> 02:31:27,420
 Then we are raising the egg up above our initial, where the egg is located.

2421
02:31:27,420 --> 02:31:39,420
 Then bring it back down and hit the egg against the table, raise the egg up again, and then over top of the bowl so we don't crash into the bowl, and then rotate the grippers to free the yolk and the egg whites.

2422
02:31:39,420 --> 02:31:44,420
 We have successfully commanded the trajectories and picked up the egg with both grippers simultaneously.

2423
02:31:44,420 --> 02:31:54,420
 In order to hit the egg with sufficient force against the table, we approximated the required velocity by both intuition and video analysis at 0.07 m/s.

2424
02:31:54,420 --> 02:32:06,420
 Our model hits at 0.1 m/s, which we calculated using the base of the EWO at roughly 200 mm and giving the gripper 2 seconds to get from the top of the trajectory to the bottom.

2425
02:32:06,420 --> 02:32:15,420
 Here is our final successful attempt. We followed the same trajectory as mentioned before, rotating the grippers to simulate separating the egg shell from each other.

2426
02:32:15,420 --> 02:32:20,420
 Our design successfully emulates a human cracking an egg with inverse kinematics.

2427
02:32:20,420 --> 02:32:29,420
 This may come in handy when creating robots for human-robot interaction, as well as learning from other human processing or manipulating objects.

2428
02:32:29,420 --> 02:32:38,420
 Some areas for improvement include finding the contact forces between the egg and the table directly rather than approximating the velocities,

2429
02:32:38,420 --> 02:32:46,420
 as well as finding the force between the grippers and the egg to make sure we were not crushing the eggs between the gripper,

2430
02:32:46,420 --> 02:32:58,420
 as well as a better egg model. Rather than our solid egg we have right now, we're representing it as two halves and also modeling the inside, the egg white and yolk.

2431
02:32:58,420 --> 02:33:01,420
 So in the future...

2432
02:33:01,420 --> 02:33:05,420
 [Applause]

2433
02:33:05,420 --> 02:33:09,420
 You got a lot working, that's awesome. Any questions?

2434
02:33:09,420 --> 02:33:15,420
 I want to be a little shorter with the questions just because we're dialing in to 5 o'clock pretty fast.

2435
02:33:15,420 --> 02:33:19,420
 Okay, great work.

2436
02:33:19,420 --> 02:33:26,420
 Hello, I'm Isabella and I'm going to be talking about my project on simultaneous localization and motion planning using belief-based approaches.

2437
02:33:26,420 --> 02:33:34,420
 Robots get information about the world using sensors, but real-world sensors are almost often noisy as shown with the Intel RealSense depth images here.

2438
02:33:34,420 --> 02:33:38,420
 Thus, an important problem in robotics is motion planning under this uncertainty.

2439
02:33:38,420 --> 02:33:43,420
 One way of addressing this is to perform actions that maximize the information content you get from your sensors.

2440
02:33:43,420 --> 02:33:48,420
 For example, if you're trying to get somewhere in a dark room, you might talk to the walls to get information about your environment.

2441
02:33:48,420 --> 02:33:51,420
 So the question is, how can we make robots do something similar?

2442
02:33:51,420 --> 02:33:57,420
 One approach is to make trajectories accounting for the uncertainty of a robot's state, or belief space planning.

2443
02:33:57,420 --> 02:34:01,420
 To make the concept of belief spaces concrete, here's an example.

2444
02:34:01,420 --> 02:34:09,420
 This is the distribution of a robot's position in a 2D world updated by a particle filter that takes in haptic information and a robot's velocity.

2445
02:34:09,420 --> 02:34:13,420
 Notice that the distribution is quite non-Gaussian, which is important later.

2446
02:34:13,420 --> 02:34:21,420
 In terms of prior work, a common method for robot localization is the extended Kalman filter, but it's not optimal in systems with non-linear dynamics.

2447
02:34:21,420 --> 02:34:28,420
 Past work in belief space planning also assumes a Gaussian belief state and maximum likelihood observations.

2448
02:34:28,420 --> 02:34:35,420
 Thus, some questions are, how do we generalize to high-dimensional non-Gaussian states, and how can we make the algorithm filter-agnostic?

2449
02:34:35,420 --> 02:34:43,420
 An idea suggested by Platt et al. in their paper "Efficient Planning in Non-Gaussian Belief Spaces" is to sample from the belief space to approximate it.

2450
02:34:43,420 --> 02:34:52,420
 Here, while the robot is not at the goal, it gets a state with maximum probability, x', and samples some other states from the belief space.

2451
02:34:52,420 --> 02:35:00,420
 It then creates a plan maximizing the information difference from x' and other states, which is an optimization problem solved by quadratic programming.

2452
02:35:00,420 --> 02:35:05,420
 It then executes the plan while tracking the belief state using a histogram filter.

2453
02:35:05,420 --> 02:35:11,420
 If the belief state deviates too far from the trajectory, then we replan.

2454
02:35:11,420 --> 02:35:26,420
 To evaluate this algorithm, I set up an EWA arm facing two boxes of known positions, and we have a laser pointer sensing the distances to the boxes.

2455
02:35:26,420 --> 02:35:33,420
 The only unknown of this system is the position along the y-axis of the robot.

2456
02:35:33,420 --> 02:35:39,420
 The overall goal is for the robot to localize and reach the goal position, which in this case is the middle of the gap.

2457
02:35:39,420 --> 02:35:45,420
 Applying Platt's algorithm, this is a possible trajectory we get.

2458
02:35:45,420 --> 02:35:55,420
 The robot starts off with its laser pointer at the lower box, so it generates a trajectory that detects a key feature in the environment, which is the gap, and then it goes to the gap.

2459
02:35:55,420 --> 02:36:12,420
 Overall, the algorithm is quite successful with a 93% average task completion rate, and I observe that if it starts closer to the goal, it has a higher chance of success.

2460
02:36:12,420 --> 02:36:17,420
 However, there are still some oddities in the results.

2461
02:36:17,420 --> 02:36:27,420
 If you look at the plot of the trajectory-

2462
02:36:27,420 --> 02:36:32,420
 So when I was talking about belief space planning, someone, I think Leroy asked,

2463
02:36:32,420 --> 02:36:41,420
 do we think that trajectory optimization without all the stochastic belief space everything is easier or harder than the belief space version?

2464
02:36:41,420 --> 02:36:47,420
 Do you feel like you had any- was the numerical optimization fairly successful or was it pretty brittle?

2465
02:36:47,420 --> 02:37:11,420
 [inaudible]

2466
02:37:11,420 --> 02:37:18,420
 All right.

2467
02:37:18,420 --> 02:37:20,420
 That seems like something we should figure out.

2468
02:37:20,420 --> 02:37:24,420
 Awesome. Okay. I think we have an insertion here.

2469
02:37:24,420 --> 02:37:34,420
 Gameplay.

2470
02:37:34,420 --> 02:37:42,420
 Hi, everyone. My name is Frank Gonzalez. And I'm Robbie Cato. And our project is on manipulating and grasping industrial tools.

2471
02:37:42,420 --> 02:37:50,420
 Nowadays, manufacturing is turning more and more towards automation, using robotic systems to efficiently create high quality products.

2472
02:37:50,420 --> 02:37:58,420
 In order to support this effort, we investigated creating a system that could autonomously grasp a tool in such a way that it was ready to use.

2473
02:37:58,420 --> 02:38:10,420
 For this project, we simulated a Kukui IWA7 arm with a general WSG gripper and two tools, a hammer and a screwdriver, to demonstrate the capabilities of our project.

2474
02:38:10,420 --> 02:38:18,420
 This plot diagram shows the overall robotic system we implemented with the two red boxes indicating modules that were not fully integrated.

2475
02:38:18,420 --> 02:38:27,420
 For this project, we developed a stack using MASQ, RCNN, and ICP, proposed estimation, and an inverse kinematics solver for state determination.

2476
02:38:27,420 --> 02:38:32,420
 Ideally, we would use a decision maker module to choose which tool to pick up.

2477
02:38:32,420 --> 02:38:39,420
 However, with there only being two tools in this situation, the decision maker was deprioritized and thus not implemented.

2478
02:38:39,420 --> 02:38:44,420
 Regardless, the key poses were passed to SLURP for generating those trajectories.

2479
02:38:44,420 --> 02:38:48,420
 We end by passing the computed state trajectories into the simulator.

2480
02:38:48,420 --> 02:38:55,420
 And Frank will show the results from just this block with everything else being precomputed.

2481
02:38:55,420 --> 02:39:07,420
 In these two videos, we can see the end result of the full stack implementation, with the robotic arm navigating, grasping, and lifting the hammer on the left, and doing the same for the screwdriver on the right.

2482
02:39:07,420 --> 02:39:13,420
 Here, we can see a slightly different perspective on the grasps after the IWA arm has come to rest.

2483
02:39:13,420 --> 02:39:27,420
 From these images, we can know that these are indeed usable grasps and not a random grasp on the tool, putting the system in a prime position to continue forward with whatever task it might be provided.

2484
02:39:27,420 --> 02:39:31,420
 One key pitfall we had was the MASQ RCNN model.

2485
02:39:31,420 --> 02:39:40,420
 The image on the left shows the top five bounding box results from our model, all of which are labeled as scissors, many of which don't capture the full extent of the tools.

2486
02:39:40,420 --> 02:39:43,420
 This is likely due to the images we used to train the model.

2487
02:39:43,420 --> 02:39:47,420
 An example screwdriver from one of the datasets is shown on the right.

2488
02:39:47,420 --> 02:39:54,420
 The images in that dataset were close-cropped and rarely showed the full tools, just like the hammer handle here.

2489
02:39:54,420 --> 02:40:00,420
 It's also important to note that scissors were the most represented object in that dataset as well.

2490
02:40:00,420 --> 02:40:05,420
 This highlights the importance of finding training data representative of the expected environment.

2491
02:40:05,420 --> 02:40:10,420
 In our case, a cluttered table with most of each tool pictured.

2492
02:40:10,420 --> 02:40:15,420
 A critical next step for this project would be to generate our own training dataset in the simulation.

2493
02:40:15,420 --> 02:40:24,420
 A new dataset would likely give better labeling and segmentation results that could be fully used to integrate MASQ RCNN.

2494
02:40:24,420 --> 02:40:34,420
 Briefly touching on some of the lessons we learned throughout this project, for starters, over time, we both definitely became more comfortable working with the DRAC environment.

2495
02:40:34,420 --> 02:40:39,420
 At the beginning, there were definitely some bumps along the road that made getting started pretty challenging.

2496
02:40:39,420 --> 02:40:42,420
 But once these were overcome, progress became much smoother.

2497
02:40:42,420 --> 02:40:46,420
 As for me, I learned quickly that sometimes simpler is better.

2498
02:40:46,420 --> 02:40:55,420
 I ran into several issues trying to create proper definitions for the tools, resulting in awkward physics simulations due to incorrectly defined inertials.

2499
02:40:55,420 --> 02:41:02,420
 This difficulty came from using awkwardly defined links and trying to determine the inertials myself, which didn't go well at all.

2500
02:41:02,420 --> 02:41:07,420
 The moment I switched to simpler geometries, everything worked much, much better.

2501
02:41:07,420 --> 02:41:12,420
 And that summarizes our project. Thank you for listening to our presentation.

2502
02:41:13,420 --> 02:41:16,420
 [APPLAUSE]

2503
02:41:16,420 --> 02:41:34,420
 Apologies. All right. Any questions for these guys? Yeah.

2504
02:41:34,420 --> 02:41:37,420
 [INAUDIBLE]

2505
02:41:37,420 --> 02:41:40,420
 [INAUDIBLE]

2506
02:41:40,420 --> 02:41:43,420
 [INAUDIBLE]

2507
02:41:43,420 --> 02:41:46,420
 [INAUDIBLE]

2508
02:41:46,420 --> 02:41:49,420
 [INAUDIBLE]

2509
02:42:13,420 --> 02:42:16,420
 Awesome.

2510
02:42:16,420 --> 02:42:19,420
 OK.

2511
02:42:19,420 --> 02:42:22,420
 Hello.

2512
02:42:22,420 --> 02:42:26,420
 My name is Oswin, and I'm working with Ray Xiao.

2513
02:42:26,420 --> 02:42:31,420
 Today, I'll be working to present our project on collaborative planning for multi-armed depilation of particle systems.

2514
02:42:31,420 --> 02:42:36,420
 Imagine you're a teppanyaki chef pushing round fried eggs on the iron griddle to prevent it from burning.

2515
02:42:36,420 --> 02:42:40,420
 Or maybe you've chopped up carrots and you're collecting it into a pile on a cutting board.

2516
02:42:40,420 --> 02:42:45,420
 For particle systems like these, it is often difficult to find a good state representation to describe the dynamics.

2517
02:42:45,420 --> 02:42:52,420
 Instead of working with the state of each particle, one can choose to represent the state using a density image and perform planning directly on this density image space.

2518
02:42:52,420 --> 02:42:58,420
 This is an approach that has been done by a paper by Terry and Ress before for the case of single-armed pushing.

2519
02:42:58,420 --> 02:43:02,420
 In this project, we choose to extend the single-armed case to the multi-armed case.

2520
02:43:02,420 --> 02:43:04,420
 Why is this interesting?

2521
02:43:04,420 --> 02:43:12,420
 First, you get squeezing behaviors. You also get sharp corners. And finally, you get inter-arm collisions, leading to both arms moving in a different direction.

2522
02:43:12,420 --> 02:43:18,420
 Moreover, it's difficult to generalize single-armed algorithms to the multi-armed case.

2523
02:43:18,420 --> 02:43:25,420
 Higher dimensionally action space means that there's bad scaling for action space discretization and algorithms that use sampling-based optimization.

2524
02:43:25,420 --> 02:43:30,420
 And inter-arm collisions are something that only happen in the multi-armed case and not in the single-armed case.

2525
02:43:30,420 --> 02:43:33,420
 So how do we solve this problem?

2526
02:43:33,420 --> 02:43:36,420
 First, we use density image state representation.

2527
02:43:36,420 --> 02:43:40,420
 Next, we design and learn dynamics directly on this density image.

2528
02:43:40,420 --> 02:43:49,420
 And finally, we use a model-based RL approach, where we use sampling-based optimization to minimize some control-the-alpha function to guarantee the controller's stability.

2529
02:43:49,420 --> 02:43:54,420
 We have tried three different methods for learning the dynamics of our particle systems.

2530
02:43:54,420 --> 02:44:02,420
 Currently, nonlinear methods, including heuristic model and the least square method, works, but neural network does not work well.

2531
02:44:02,420 --> 02:44:05,420
 The design for heuristic model contains four parts.

2532
02:44:05,420 --> 02:44:09,420
 First, we are given the initial state and the action commands.

2533
02:44:09,420 --> 02:44:19,420
 Then, based on these states and commands, we generate a figure mask representing the pushed area by our heuristic function.

2534
02:44:19,420 --> 02:44:27,420
 Later, we generate a mask for weight spread allocation on the area mask.

2535
02:44:27,420 --> 02:44:34,420
 And finally, we clear the pixels according to the area mask and reallocate these cleared weights according to the weight mask.

2536
02:44:34,420 --> 02:44:38,420
 You can see that our prediction is quite close to the ground truth.

2537
02:44:38,420 --> 02:44:42,420
 However, predicting the collision is still very difficult.

2538
02:44:42,420 --> 02:44:45,420
 All our three methods suffer here.

2539
02:44:45,420 --> 02:44:50,420
 Compared with single-arm steady, collision will only happen in multi-armed case.

2540
02:44:50,420 --> 02:44:56,420
 It adds nonlinearity to the dynamic and more dependency to the initial distribution.

2541
02:44:56,420 --> 02:44:58,420
 Take heuristic model as an example.

2542
02:44:58,420 --> 02:45:02,420
 If there is no particles, the pushers should not collide with each other.

2543
02:45:02,420 --> 02:45:10,420
 With particles, as the horizontal pusher pushes too much particles and those particles collide with the other pusher,

2544
02:45:10,420 --> 02:45:16,420
 it gives a large force back to the initial pusher and stops it.

2545
02:45:16,420 --> 02:45:19,420
 And this also makes the dynamic more sensitive.

2546
02:45:19,420 --> 02:45:25,420
 A slight change in distribution might result in a huge change from collision-free to collision.

2547
02:45:25,420 --> 02:45:28,420
 The deep dynamic model does not work well.

2548
02:45:28,420 --> 02:45:33,420
 It does learn something that it should decrease the weight in the pushed area,

2549
02:45:33,420 --> 02:45:40,420
 but it does not quite understand how much it should decrease and where these decreased weights should be added back.

2550
02:45:40,420 --> 02:45:44,420
 On the other hand, both the non-deep methods can predict the image dynamic as well.

2551
02:45:44,420 --> 02:45:48,420
 The heuristic method has a more accurate structural bias, meaning that the error is more sparse.

2552
02:45:48,420 --> 02:45:52,420
 While the least squares method here is more accurate overall with smaller errors of magnitude.

2553
02:45:52,420 --> 02:45:59,420
 Finally, we combine both working dynamics models with soundly based optimization from the Terry and Ress's paper

2554
02:45:59,420 --> 02:46:03,420
 and successfully synthesize a controller that can push particles into the target surface.

2555
02:46:03,420 --> 02:46:07,420
 Surprisingly, despite the much larger action space compared to the single-arm case,

2556
02:46:07,420 --> 02:46:11,420
 we still obtain a stabilizing controller using both non-deep dynamics models.

2557
02:46:11,420 --> 02:46:18,420
 In conclusion, the basic problem of learning general dynamics is too hard to solve

2558
02:46:18,420 --> 02:46:21,420
 and it is not necessary to fully solve it for planning.

2559
02:46:21,420 --> 02:46:22,420
 To the OSA.

2560
02:46:23,420 --> 02:46:26,420
 Was that? Oh, I see. That was not your end.

2561
02:46:26,420 --> 02:46:28,420
 Okay. Nice.

2562
02:46:28,420 --> 02:46:32,420
 So what's the conclusion?

2563
02:46:32,420 --> 02:46:36,420
 Would you stay away from deep going forward or do you think the deep--

2564
02:46:36,420 --> 02:47:01,420
 [inaudible]

2565
02:47:01,420 --> 02:47:02,420
 Yeah.

2566
02:47:02,420 --> 02:47:13,420
 [inaudible]

2567
02:47:13,420 --> 02:47:15,420
 Nicely said.

2568
02:47:15,420 --> 02:47:17,420
 Okay. We have four left.

2569
02:47:17,420 --> 02:47:20,420
 If you guys are willing to stick with us, we're going to get everybody in.

2570
02:47:20,420 --> 02:47:24,420
 Yeah. Four left, assuming everybody's here.

2571
02:47:24,420 --> 02:47:26,420
 Thank you guys for your patience.

2572
02:47:26,420 --> 02:47:31,420
 I'm loving it.

2573
02:47:31,420 --> 02:47:34,420
 If I can avoid blasting you through.

2574
02:47:34,420 --> 02:47:35,420
 All right.

2575
02:47:35,420 --> 02:47:39,420
 In class, we learned about some powerful techniques in motion planning,

2576
02:47:39,420 --> 02:47:42,420
 including kinematic trajectory optimization.

2577
02:47:42,420 --> 02:47:47,420
 As we discussed, however, they often tend to avoid collisions too conservatively.

2578
02:47:47,420 --> 02:47:52,420
 In addition, real-world tasks frequently have multiple potential solutions.

2579
02:47:52,420 --> 02:47:56,420
 In this project, we take inspiration from cognitive science models of how humans

2580
02:47:56,420 --> 02:48:01,420
 plan and make decisions and explore how we can obtain robot trajectories that are

2581
02:48:01,420 --> 02:48:05,420
 good or more precisely near optimal and that also take varying approaches to

2582
02:48:05,420 --> 02:48:09,420
 traversing the environment.

2583
02:48:09,420 --> 02:48:13,420
 My name is Jovana and my friend Stewie and I are going to present our project on

2584
02:48:13,420 --> 02:48:18,420
 Marco Chain Monte Carlo, in short MCMC, motion planning for Boltzmann rational

2585
02:48:18,420 --> 02:48:22,420
 trajectory optimization.

2586
02:48:22,420 --> 02:48:27,420
 In this project, we implement five MCMC algorithms of zeroth, first, and second

2587
02:48:27,420 --> 02:48:32,420
 order and combine random sampling and optimization to generate good and diverse

2588
02:48:32,420 --> 02:48:33,420
 trajectories.

2589
02:48:33,420 --> 02:48:38,420
 We evaluate the trajectories and perform ablation studies on 2D navigation and

2590
02:48:38,420 --> 02:48:40,420
 3D manipulation problems.

2591
02:48:40,420 --> 02:48:45,420
 We show that zeroth and first order methods prove is sufficient for 2D problems

2592
02:48:45,420 --> 02:48:50,420
 and that solving 3D manipulation tasks benefits from second order derivatives.

2593
02:48:50,420 --> 02:48:55,420
 Finally, we suggest that our MCMC motion planners may be a helpful way for robots

2594
02:48:55,420 --> 02:48:57,420
 to model humans.

2595
02:48:57,420 --> 02:49:02,420
 Next, we discuss what we learned and our process of getting there.

2596
02:49:02,420 --> 02:49:07,420
 We learned that Marco Chain Monte Carlo is quite a powerful method for generating

2597
02:49:07,420 --> 02:49:09,420
 samples from desired distributions.

2598
02:49:09,420 --> 02:49:13,420
 Indeed, even for the simplest of the algorithms that uses no derivative

2599
02:49:13,420 --> 02:49:19,420
 information, we were able to obtain a surprising amount of viable trajectories.

2600
02:49:19,420 --> 02:49:23,420
 We experienced how defining the cost function for the problem can be difficult.

2601
02:49:23,420 --> 02:49:27,420
 And for just a small change in how we penalize obstacle collision compared to the

2602
02:49:27,420 --> 02:49:32,420
 length of trajectory, we observed trajectories going straight through the obstacles

2603
02:49:32,420 --> 02:49:34,420
 and away from the goal.

2604
02:49:34,420 --> 02:49:38,420
 We compared the algorithms with respect to their time complexity and the kind of

2605
02:49:38,420 --> 02:49:40,420
 trajectories they generate.

2606
02:49:40,420 --> 02:49:44,420
 And we found that for 2D environments, unadjusted Landgerman algorithm requires

2607
02:49:44,420 --> 02:49:49,420
 least tuning to generate diverse obstacle avoiding trajectories.

2608
02:49:49,420 --> 02:49:53,420
 We learned that sampling from a Boltzmann distribution, which is commonly used to

2609
02:49:53,420 --> 02:49:57,420
 model human decision making, allows the trajectories to explore multiple potential

2610
02:49:57,420 --> 02:49:58,420
 solutions.

2611
02:49:58,420 --> 02:50:02,420
 And as would make sense intuitively, we found that the variance of the path scales

2612
02:50:02,420 --> 02:50:05,420
 nicely with the number of obstacles.

2613
02:50:05,420 --> 02:50:09,420
 The beta parameter of the Boltzmann distribution, commonly referred to as the

2614
02:50:09,420 --> 02:50:13,420
 rationality coefficient, intuitively corresponds to how strong is the preference

2615
02:50:13,420 --> 02:50:17,420
 for low-cost trajectories.

2616
02:50:17,420 --> 02:50:23,420
 Next, we performed experiments on a 3D manipulation environment involving motion

2617
02:50:23,420 --> 02:50:28,420
 planning to a desired pose of a seven degree of freedom robotic arm.

2618
02:50:28,420 --> 02:50:34,420
 Here we compare two trajectories sampled from Newtonian Monte Carlo, a second

2619
02:50:34,420 --> 02:50:41,420
 order method, with beta equals 0.1 and beta equals 10.

2620
02:50:41,420 --> 02:50:46,420
 We can see that for beta equals 0.1, there's quite a bit of shaking and

2621
02:50:46,420 --> 02:50:52,420
 suboptimality, while for beta equals 10, we see a smooth interpolation to the

2622
02:50:52,420 --> 02:50:59,420
 desired final pose.

2623
02:50:59,420 --> 02:51:04,420
 We also compare it to Hamiltonian Monte Carlo, a first order algorithm that's

2624
02:51:04,420 --> 02:51:10,420
 widely considered as the gold standard for MCMC, with beta equals 10.

2625
02:51:10,420 --> 02:51:16,420
 And we observed that there's quite a bit of wild oscillation.

2626
02:51:16,420 --> 02:51:21,420
 This is consistent with our quantitative results, which show that for this 3D

2627
02:51:21,420 --> 02:51:25,420
 motion planning problem, only the second order algorithms are able to optimize

2628
02:51:25,420 --> 02:51:29,420
 well enough to discover low-cost trajectories.

2629
02:51:29,420 --> 02:51:34,420
 At the same time, Newtonian Monte Carlo also produces the most diverse

2630
02:51:34,420 --> 02:51:39,420
 trajectories, and therefore does the best at generating diverse and near-optimal

2631
02:51:39,420 --> 02:51:42,420
 trajectories.

2632
02:51:42,420 --> 02:51:48,420
 In this project, we successfully used MCMC algorithms to produce diverse and

2633
02:51:48,420 --> 02:51:52,420
 approximately optimal trajectories for motion planning problems.

2634
02:51:52,420 --> 02:51:56,420
 We also consider several directions for future work.

2635
02:51:56,420 --> 02:52:00,420
 Most importantly is a more complete evaluation with more diverse and complex

2636
02:52:00,420 --> 02:52:03,420
 experimental environments.

2637
02:52:03,420 --> 02:52:08,420
 Second, in handling constraints, we perform a projection step, which causes us

2638
02:52:08,420 --> 02:52:13,420
 to lose the technical condition of reversibility and theoretical convergence.

2639
02:52:13,420 --> 02:52:17,420
 It would be interesting to see how we could restore these with more advanced

2640
02:52:17,420 --> 02:52:19,420
 methods.

2641
02:52:19,420 --> 02:52:24,420
 Lastly, the direction that we are most excited about is in using our motion

2642
02:52:24,420 --> 02:52:28,420
 planning algorithms for Bayesian trajectory prediction and human-robot

2643
02:52:28,420 --> 02:52:31,420
 collaboration problems.

2644
02:52:31,420 --> 02:52:36,420
 Thank you for listening to our presentation, and we hope you enjoy it.

2645
02:52:36,420 --> 02:52:39,420
 [APPLAUSE]

2646
02:52:39,420 --> 02:52:42,420
 I chopped one second.

2647
02:52:42,420 --> 02:52:43,420
 That was super clear.

2648
02:52:43,420 --> 02:52:45,420
 Thank you.

2649
02:52:45,420 --> 02:52:49,420
 It was joint space in the 3D examples that you're planning in, right?

2650
02:52:49,420 --> 02:52:53,420
 So the jump from the 2D examples to the 3D examples was actually a jump from

2651
02:52:53,420 --> 02:52:56,420
 two degrees of freedom to like seven degrees of freedom.

2652
02:52:56,420 --> 02:52:57,420
 Yeah.

2653
02:52:57,420 --> 02:52:58,420
 Nice.

2654
02:52:58,420 --> 02:53:00,420
 No, that was really, really well articulated.

2655
02:53:00,420 --> 02:53:03,420
 Thank you.

2656
02:53:03,420 --> 02:53:04,420
 OK, Tom's been waiting here.

2657
02:53:04,420 --> 02:53:06,420
 Here we go.

2658
02:53:06,420 --> 02:53:10,420
 Hi, my name's Tom, and today I'm excited to present my final project for robotic

2659
02:53:10,420 --> 02:53:15,420
 manipulation on augmenting ICP using dense object nets with applications in

2660
02:53:15,420 --> 02:53:18,420
 surgical robot perception.

2661
02:53:18,420 --> 02:53:22,420
 State-of-the-art robots for vascular surgery offer minimal autonomy.

2662
02:53:22,420 --> 02:53:26,420
 Surgeons maintain direct control over wire-based surgical manipulators, and

2663
02:53:26,420 --> 02:53:30,420
 many surgeons are deterred by the associated learning curve.

2664
02:53:30,420 --> 02:53:34,420
 A major obstacle to higher autonomy surgical manipulators is developing highly

2665
02:53:34,420 --> 02:53:37,420
 robust simultaneous localization and mapping.

2666
02:53:37,420 --> 02:53:41,420
 Ultrasound is the primary imaging modality utilized, and we are interested in

2667
02:53:41,420 --> 02:53:46,420
 developing a SLAM algorithm capable of mapping an unknown vessel geometry in

2668
02:53:46,420 --> 02:53:48,420
 vivo.

2669
02:53:48,420 --> 02:53:53,420
 ICP is a common front-end algorithm used in acoustically based SLAM.

2670
02:53:53,420 --> 02:53:57,420
 However, as we learned in class, ICP is susceptible to convergence to local

2671
02:53:57,420 --> 02:54:02,420
 minima in the case of poor initialization or minimal point cloud saliency.

2672
02:54:02,420 --> 02:54:05,420
 Interoperative ultrasound suffers from both these issues.

2673
02:54:05,420 --> 02:54:10,420
 Though exploitation of 3D priors from preoperative imaging has the potential to

2674
02:54:10,420 --> 02:54:13,420
 improve ICP performance.

2675
02:54:13,420 --> 02:54:17,420
 Therefore, I propose the use of contrastive correspondence learning between

2676
02:54:17,420 --> 02:54:22,420
 adjacent depth images simulated from a prior CT mesh.

2677
02:54:22,420 --> 02:54:26,420
 These correspondences will be used to inform registration tasks on real

2678
02:54:26,420 --> 02:54:30,420
 ultrasound point clouds representing successive poses of the robot.

2679
02:54:30,420 --> 02:54:34,420
 Prior to intervention, the surgeon would run the following data generation

2680
02:54:34,420 --> 02:54:36,420
 pipeline.

2681
02:54:36,420 --> 02:54:40,420
 First, we select a random pose inside the preoperative CT scan to represent the

2682
02:54:40,420 --> 02:54:44,420
 surgical end effector and generate an offset pose to simulate motion of the

2683
02:54:44,420 --> 02:54:46,420
 optical frame.

2684
02:54:46,420 --> 02:54:50,420
 We utilize a combination of ray casting and an inverse pinhole model to generate

2685
02:54:50,420 --> 02:54:55,420
 simulated depth images and ground truth pixel-wise correspondences.

2686
02:54:55,420 --> 02:54:59,420
 This data set is then fed to a dense object network, which is a similar framework

2687
02:54:59,420 --> 02:55:02,420
 to what we learned about in class.

2688
02:55:02,420 --> 02:55:06,420
 Finally, we calculate pixel-wise loss in the descriptor space, given knowledge of

2689
02:55:06,420 --> 02:55:09,420
 ground truth correspondences.

2690
02:55:09,420 --> 02:55:13,420
 To illustrate quantitative results, first I show the pixel-wise correspondence

2691
02:55:13,420 --> 02:55:14,420
 precision.

2692
02:55:14,420 --> 02:55:18,420
 The network did not perform as well as the original architecture proposed by

2693
02:55:18,420 --> 02:55:22,420
 Florence, likely because we were working with sparser depth images in comparison

2694
02:55:22,420 --> 02:55:25,420
 to rich RGB information.

2695
02:55:25,420 --> 02:55:29,420
 It was noticed that when the probe views surfaces from far away, interesting

2696
02:55:29,420 --> 02:55:33,420
 structures like vessel sub-branches and significant variations in curvature are

2697
02:55:33,420 --> 02:55:36,420
 evident from depth images.

2698
02:55:36,420 --> 02:55:39,420
 Therefore, in future work, it may be possible to apply a larger weighting to

2699
02:55:39,420 --> 02:55:44,420
 high-intensity depth values in the pixel-wise loss function.

2700
02:55:44,420 --> 02:55:49,420
 Here I demonstrate the registration performance for vanilla ICP and network

2701
02:55:49,420 --> 02:55:53,420
 augmented ICP across 40 point cloud pairs.

2702
02:55:53,420 --> 02:55:56,420
 Don ICP had a lower inlier error average.

2703
02:55:56,420 --> 02:55:59,420
 So variations were not found to be statistically significant.

2704
02:55:59,420 --> 02:56:03,420
 To explore underlying factors for this, I'll now discuss some qualitative

2705
02:56:03,420 --> 02:56:05,420
 registration results.

2706
02:56:05,420 --> 02:56:08,420
 [PAUSE]

2707
02:56:08,420 --> 02:56:11,420
 [APPLAUSE]

2708
02:56:11,420 --> 02:56:14,420
 All right, so the big money question is, does it work for the -- does it help

2709
02:56:14,420 --> 02:56:17,420
 solve the global correspondence problem?

2710
02:56:17,420 --> 02:56:38,420
 [INAUDIBLE]

2711
02:56:38,420 --> 02:56:39,420
 Awesome.

2712
02:56:39,420 --> 02:56:41,420
 I'm glad it worked.

2713
02:56:41,420 --> 02:56:43,420
 Okay, last two taken at home.

2714
02:56:43,420 --> 02:56:52,420
 Here we go.

2715
02:56:52,420 --> 02:56:54,420
 Hi, we're Martin, Fiona, and Hannah, and we're presenting our final

2716
02:56:54,420 --> 02:56:58,420
 presentation on scooping for 6.4.2.10, fall 2022 robotic manipulation.

2717
02:56:58,420 --> 02:57:00,420
 With DexAI Robotics, Alfred, which we talked about in class, they use a

2718
02:57:00,420 --> 02:57:02,420
 trajectory optimization scooping technique.

2719
02:57:02,420 --> 02:57:06,420
 However, besides some niche applications of scooping in industry, there's little

2720
02:57:06,420 --> 02:57:08,420
 academic work in scooping.

2721
02:57:08,420 --> 02:57:12,420
 Best we could find was scooping with a flat spatula off of a flat surface,

2722
02:57:12,420 --> 02:57:16,420
 whereas we're going to be exploring scooping with a convex tool.

2723
02:57:16,420 --> 02:57:20,420
 And based off of all of the previous research, as well as some previous

2724
02:57:20,420 --> 02:57:23,420
 conversations with Russ, we decided to try and pursue a precompute-then-choose

2725
02:57:23,420 --> 02:57:27,420
 approach, where we precompute appropriate trajectories for different bin

2726
02:57:27,420 --> 02:57:30,420
 states, such as empty, half-full, and full, then use perception to try and

2727
02:57:30,420 --> 02:57:32,420
 determine which trajectory to select.

2728
02:57:32,420 --> 02:57:35,420
 And so this is the direction that we eventually hope to take this project,

2729
02:57:35,420 --> 02:57:40,420
 although we did not quite finish pursuing the perception selection process.

2730
02:57:40,420 --> 02:57:43,420
 The first part of our project was setting up the scooping environment.

2731
02:57:43,420 --> 02:57:45,420
 It's pretty similar to the bin picking setup.

2732
02:57:45,420 --> 02:57:48,420
 The main difference with our EWAP setup is that there's a measuring cup that

2733
02:57:48,420 --> 02:57:50,420
 we have welded onto the end of the EWAP.

2734
02:57:50,420 --> 02:57:53,420
 In addition, we are using spheres instead of bricks for collision geometry.

2735
02:57:53,420 --> 02:57:56,420
 And this is because they're way easier to simulate, and we were running into

2736
02:57:56,420 --> 02:57:59,420
 significant issues with our simulation speed, even after we updated to use the

2737
02:57:59,420 --> 02:58:03,420
 SAP contact solver, which is faster than the default MeshCat contact solver.

2738
02:58:03,420 --> 02:58:07,420
 And actually, the simulation speed constraint becomes a pretty strong

2739
02:58:07,420 --> 02:58:10,420
 constraint for how we thought about the granularity of our poses in the future.

2740
02:58:10,420 --> 02:58:13,420
 We pursued two approaches in parallel, and the first one was our geometric

2741
02:58:13,420 --> 02:58:16,420
 scooping. This is adapted from the robot painter notebook that we saw in class.

2742
02:58:16,420 --> 02:58:19,420
 And the key idea here is we're using simple geometries to plan the motion of

2743
02:58:19,420 --> 02:58:21,420
 scooping from one bin and pouring into the other.

2744
02:58:21,420 --> 02:58:24,420
 So for the scoop trajectory, we started with a circle, but it was really hard to

2745
02:58:24,420 --> 02:58:27,420
 balance getting deep into the bin and also being able to turn the scoop upright

2746
02:58:27,420 --> 02:58:29,420
 at the end. So we ended up using an ellipse to have

2747
02:58:29,420 --> 02:58:32,420
 more control over both the width and depth of the scoop.

2748
02:58:32,420 --> 02:58:35,420
 For the pouring, this motion was less constrained, since we're starting out of

2749
02:58:35,420 --> 02:58:38,420
 the bin and then moving the objects in, so we could still use a circular path.

2750
02:58:38,420 --> 02:58:41,420
 For our other approach, we went for a teleop record.

2751
02:58:41,420 --> 02:58:45,420
 And so the idea was, if we could scoop successfully using teleop, we can save

2752
02:58:45,420 --> 02:58:47,420
 those trajectories and use them back later.

2753
02:58:47,420 --> 02:58:51,420
 So this is just two steps, one recording and a two playback.

2754
02:58:51,420 --> 02:58:54,420
 For recording, it was pretty straightforward because we tried to set up a

2755
02:58:54,420 --> 02:58:57,420
 good interface for us to record with.

2756
02:58:57,420 --> 02:59:00,420
 So it was pretty much teleop the robot as you would usually.

2757
02:59:00,420 --> 02:59:04,420
 And then when you're done, you would press the save poses button, and it would

2758
02:59:04,420 --> 02:59:08,420
 save it all to file. From a user standpoint, pretty easy.

2759
02:59:08,420 --> 02:59:12,420
 But behind the scenes, we had to put in quite a lot to make it happen.

2760
02:59:12,420 --> 02:59:16,420
 And the way that we had the poses saved was we used a file representation of

2761
02:59:16,420 --> 02:59:20,420
 a file with basically one pose for each line, where each pose is represented

2762
02:59:20,420 --> 02:59:23,420
 by a six tuple. You have rotation for three of the numbers, and then x, y, z

2763
02:59:23,420 --> 02:59:26,420
 for the other. They're not perfect descriptors, but they work when you're

2764
02:59:26,420 --> 02:59:30,420
 trying to just get from one pose to the other for the longest of poses.

2765
02:59:30,420 --> 02:59:37,420
 So teleop record playback.

2766
02:59:37,420 --> 02:59:40,420
 The main components to using the teleop record for a future automatic playback

2767
02:59:40,420 --> 02:59:43,420
 are in cleaning up the teleop path, reading the files that they're stored in,

2768
02:59:43,420 --> 02:59:46,420
 and actually moving the EWARM based on the information that we've read.

2769
02:59:46,420 --> 02:59:50,420
 So a single trajectory file will usually hold an ordered list of desired poses.

2770
02:59:50,420 --> 02:59:54,420
 And there's often more than a few unnecessary positions within this file,

2771
02:59:54,420 --> 02:59:58,420
 just because of the nature of teleop. And so some manual cleaning is sometimes

2772
02:59:58,420 --> 03:00:02,420
 desired. In addition, deleting and adjusting poses can sometimes help with

2773
03:00:02,420 --> 03:00:05,420
 resolving issues from the EWARM getting stuck or having joints that are fully

2774
03:00:05,420 --> 03:00:08,420
 extended. And we parse these cleaned files to create paths based on the

2775
03:00:08,420 --> 03:00:11,420
 movements between poses rather than the positions themselves. And then the EWARM

2776
03:00:11,420 --> 03:00:17,420
 can then be directed to execute these movements.

2777
03:00:17,420 --> 03:00:21,420
 Now we'll talk a little bit more about our results. So here we have a video of

2778
03:00:21,420 --> 03:00:26,420
 a full scooping and pouring path with our 50 simulated spheres. So we sped this

2779
03:00:26,420 --> 03:00:29,420
 video up by five times, because as you can see, there's a big slowdown when the

2780
03:00:29,420 --> 03:00:33,420
 scoop starts interacting with the spheres. And with this trajectory, we also tend

2781
03:00:33,420 --> 03:00:36,420
 to have to push some spheres out of the bin, but we can reliably pick up three

2782
03:00:36,420 --> 03:00:40,420
 spheres every time. After the scoop, we have an intermediate frame to make sure

2783
03:00:40,420 --> 03:00:44,420
 the scoop stays upright so we don't drop the spheres we just picked up. And then

2784
03:00:44,420 --> 03:00:47,420
 finally, we move on to pouring into the other bin. Both approaches yielded

2785
03:00:47,420 --> 03:00:50,420
 successful scooping and pouring results. While the geometric approach created

2786
03:00:50,420 --> 03:00:53,420
 relatively smooth trajectories, it proved difficult to navigate the bin

2787
03:00:53,420 --> 03:00:56,420
 environment and plan what geometric shapes would work best. The teleop

2788
03:00:56,420 --> 03:00:59,420
 approach creates much more flexibility and can quickly plan trajectories to new

2789
03:00:59,420 --> 03:01:01,420
 situations, but may be a little bit less smooth.

2790
03:01:01,420 --> 03:01:06,420
 So some of the challenges along the way and lessons we learned. We found that

2791
03:01:06,420 --> 03:01:09,420
 even though this is a robotics class, a lot of the stuff we did to help us out

2792
03:01:09,420 --> 03:01:13,420
 were actually just good software development sort of deals. And some future

2793
03:01:13,420 --> 03:01:19,420
 work is that we settled on our scooper early on, but it's actually not the best

2794
03:01:19,420 --> 03:01:23,420
 scooper for scooping. The walls are actually pretty high on the scooper, so

2795
03:01:23,420 --> 03:01:27,420
 it's tough to get it to go into a bin full of spheres. And also, we didn't get

2796
03:01:27,420 --> 03:01:31,420
 to automatically choose the trajectories through perception. There's also a

2797
03:01:31,420 --> 03:01:35,420
 room for trajectory optimization. The trajectories you get are ultimately as

2798
03:01:35,420 --> 03:01:38,420
 good as the trajectories that you can teleop yourself. Part of that is

2799
03:01:38,420 --> 03:01:41,420
 probably like making a cost function for scoops. I'm not sure if this is

2800
03:01:41,420 --> 03:01:45,420
 something for Drake or something for users of Drake, but speaking of that,

2801
03:01:45,420 --> 03:01:47,420
 simulation should make things better.

2802
03:01:47,420 --> 03:01:51,420
 But yeah, that's our scooping projects. It was tough. We learned a lot.

2803
03:01:51,420 --> 03:01:53,420
 Thanks for watching.

2804
03:01:53,420 --> 03:01:56,420
 [Applause]

2805
03:01:56,420 --> 03:02:00,420
 >> Awesome. I can definitely help you speed it up. If you want. If you care,

2806
03:02:00,420 --> 03:02:05,420
 we can speed it up.

2807
03:02:05,420 --> 03:02:12,420
 Okay. Ryan is ready. All right.

2808
03:02:12,420 --> 03:02:16,420
 >> Thanks for joining us today. So GardenBuddy is a robot arm that controls

2809
03:02:16,420 --> 03:02:20,420
 some unfamiliar hoes in an unfamiliar garden. The information here is unknown

2810
03:02:20,420 --> 03:02:23,420
 beforehand to the robot arm, and so the speed of the water is unknown and

2811
03:02:23,420 --> 03:02:28,420
 location of the plants is unknown. Here's a little demonstration that we'll

2812
03:02:28,420 --> 03:02:36,420
 dive deeper into later in this presentation.

2813
03:02:36,420 --> 03:02:39,420
 So the two main components of the project. There's a perception side, which

2814
03:02:39,420 --> 03:02:43,420
 is exploring this unknown environment, and there's a motion planning side,

2815
03:02:43,420 --> 03:02:46,420
 which is commanding the robot, planning the trajectory, and tuning the

2816
03:02:46,420 --> 03:02:51,420
 controllers. So here's the overall approach. So we start by getting the

2817
03:02:51,420 --> 03:02:56,420
 perception, and then the perception component, the module, passes on the

2818
03:02:56,420 --> 03:02:59,420
 information to pose optimization, which passes on the information to the

2819
03:02:59,420 --> 03:03:03,420
 interpolation module, which goes to the IK module, and then the controller

2820
03:03:03,420 --> 03:03:06,420
 module, and this is a closed loop.

2821
03:03:06,420 --> 03:03:09,420
 >> So we begin with the perception component, which takes in the scene and

2822
03:03:09,420 --> 03:03:13,420
 needs to find the target plant locations as well as the droplet speed. To find

2823
03:03:13,420 --> 03:03:17,420
 the plant positions, we use our RGBD sensor to get a depth image, and then

2824
03:03:17,420 --> 03:03:21,420
 filter for pixels with lower depth as those represent the plants, excluding

2825
03:03:21,420 --> 03:03:25,420
 the EWA controller. And once we get this filtered depth image, we perform a

2826
03:03:25,420 --> 03:03:28,420
 graph search to find the clusters, which represent the plants, as well as the

2827
03:03:28,420 --> 03:03:34,420
 center plants, which represents our target locations, shown here.

2828
03:03:34,420 --> 03:03:38,420
 To find the droplet launch speed, we begin by executing a sequence where the

2829
03:03:38,420 --> 03:03:43,420
 robot launches droplets horizontally, and then using kinematics, knowing the

2830
03:03:43,420 --> 03:03:46,420
 height of the robot, we just need to find the average location of where the

2831
03:03:46,420 --> 03:03:51,420
 droplets land. To do so, we take a sequence of five color images taken 0.1

2832
03:03:51,420 --> 03:03:54,420
 seconds apart, and compute the difference between them to find the motion of the

2833
03:03:54,420 --> 03:03:58,420
 droplets. And then we perform a convolution of filter to remove any other

2834
03:03:58,420 --> 03:04:01,420
 noise or motion, leaving just the droplets, and thus we can find the average

2835
03:04:01,420 --> 03:04:09,420
 location, and using inverse kinematics, find the speed, and feeding this into the

2836
03:04:09,420 --> 03:04:14,420
 motion plane as long as, as well as the plant locations.

2837
03:04:14,420 --> 03:04:17,420
 The positions of the potted plants, and the speed at which droplets leave the

2838
03:04:17,420 --> 03:04:22,420
 hose, we want to find the optimal poses for the dripper to be in, such that water

2839
03:04:22,420 --> 03:04:27,420
 coming out of the hose reaches the potted plants. We do this by solving a

2840
03:04:27,420 --> 03:04:32,420
 constrained optimization problem using mathematical program. We want to find a

2841
03:04:32,420 --> 03:04:36,420
 pose that's very close to the one the robot is currently in, but such that

2842
03:04:36,420 --> 03:04:40,420
 water droplets coming out of it intersect with the z-height of the potted plant, in

2843
03:04:40,420 --> 03:04:46,420
 the same x and y position as where the plant actually is. So we do this by using

2844
03:04:46,420 --> 03:04:51,420
 the equations of projectile motion that we've seen in 801. We find the time, t*,

2845
03:04:51,420 --> 03:04:55,420
 at which the droplet reaches the correct z-height, and then we find the positions

2846
03:04:55,420 --> 03:05:01,420
 x* and y* where that happens. So we constrain our optimization problem for the

2847
03:05:01,420 --> 03:05:07,420
 error cost to be the difference between the candidate pose and the current pose,

2848
03:05:07,420 --> 03:05:12,420
 and we set the constraint that the x and y position of the landing is the same as

2849
03:05:12,420 --> 03:05:15,420
 the position of the potted plant.

2850
03:05:15,420 --> 03:05:20,420
 Sequence of poses to a smooth sequence of keyframe poses for a robot to follow.

2851
03:05:20,420 --> 03:05:24,420
 Currently we move from pose to pose directly. This leads to jerky movement,

2852
03:05:24,420 --> 03:05:27,420
 and also means that our robot does not stay on one plant long enough for the

2853
03:05:27,420 --> 03:05:32,420
 plant to be watered fully. We propose a solution where we interpolate with two

2854
03:05:32,420 --> 03:05:37,420
 more segments in between. We first move to a comfortable z-height where the

2855
03:05:37,420 --> 03:05:40,420
 robot does not have to move through itself to get from one pose to another,

2856
03:05:40,420 --> 03:05:44,420
 and after the second segment we aim to be at the goal pose. We spend the third

2857
03:05:44,420 --> 03:05:48,420
 segment at the goal pose so that the plant is watered fully.

2858
03:05:48,420 --> 03:05:51,420
 This three-part solution leads to a smoother trajectory which leads to good

2859
03:05:51,420 --> 03:05:56,420
 results. We pass this as a piecewise pose trajectory into inverse kinematics so

2860
03:05:56,420 --> 03:06:01,420
 that we can convert these into joint angles.

2861
03:06:01,420 --> 03:06:04,420
 Here's an example of the pose optimization actually working, and you can see the

2862
03:06:04,420 --> 03:06:09,420
 frames and the robot snapping to the frames.

2863
03:06:09,420 --> 03:06:13,420
 So for the inverse kinematics we use the IK solver manually, and this allows us to

2864
03:06:13,420 --> 03:06:18,420
 go from desired poses to the joint angles, and we update these every 0.1 seconds.

2865
03:06:18,420 --> 03:06:22,420
 Here without any optimization on anything, you can see that the robot just jerks

2866
03:06:22,420 --> 03:06:25,420
 around and the trajectory is not optimized at all.

2867
03:06:25,420 --> 03:06:28,420
 For the controller we decided to use the inverse dynamics controller.

2868
03:06:28,420 --> 03:06:32,420
 This allows us to go from joint angles to the forces that we need.

2869
03:06:32,420 --> 03:06:35,420
 The inverse dynamics controller specifically allows us for bigger time

2870
03:06:35,420 --> 03:06:41,420
 steps, and it allows us to tune the PID gains manually.

2871
03:06:41,420 --> 03:06:44,420
 Here's the whole simulation being run, and this is the whole closed loop.

2872
03:06:44,420 --> 03:06:49,420
 So you see that the water shoots horizontally, which allows us to get the

2873
03:06:49,420 --> 03:06:52,420
 speed, and then we can shoot the plants knowing the speed.

2874
03:06:52,420 --> 03:06:57,420
 And this entire simulation is a closed loop, so the robot doesn't know anything.

2875
03:06:57,420 --> 03:07:03,420
 The camera gives it information, and then the robot can know where to go from there.

2876
03:07:03,420 --> 03:07:08,420
 Thanks for watching our presentation, and here's a link to the DeepNote.

2877
03:07:08,420 --> 03:07:14,420
 [Applause]

2878
03:07:14,420 --> 03:07:18,420
 Apart from Pi virtual display, what was the hardest part?

2879
03:07:18,420 --> 03:07:27,420
 [Silence]

2880
03:07:27,420 --> 03:07:29,420
 That was weird? Okay.

2881
03:07:29,420 --> 03:07:54,420
 [Silence]

2882
03:07:54,420 --> 03:07:56,420
 Right?

2883
03:07:56,420 --> 03:07:59,420
 [Silence]

2884
03:07:59,420 --> 03:08:02,420
 Very nice. Thank you everybody for an awesome semester.

2885
03:08:02,420 --> 03:08:04,420
 That was really, really, really fun.

2886
03:08:04,420 --> 03:08:08,420
 And for those of you that stuck in here the whole time, it's awesome.

2887
03:08:08,420 --> 03:08:11,420
 I'm happy to stick around for a little bit, but yeah.

