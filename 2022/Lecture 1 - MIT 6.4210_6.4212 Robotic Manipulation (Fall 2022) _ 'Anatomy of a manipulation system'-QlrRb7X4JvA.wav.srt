1
00:00:00,000 --> 00:00:10,000
 [BLANK_AUDIO]

2
00:00:10,000 --> 00:00:20,000
 [BLANK_AUDIO]

3
00:00:20,000 --> 00:00:30,000
 [BLANK_AUDIO]

4
00:00:30,000 --> 00:00:40,000
 [BLANK_AUDIO]

5
00:00:40,000 --> 00:00:50,000
 [BLANK_AUDIO]

6
00:00:50,000 --> 00:01:00,000
 [BLANK_AUDIO]

7
00:01:00,000 --> 00:01:10,000
 [BLANK_AUDIO]

8
00:01:10,000 --> 00:01:20,000
 [BLANK_AUDIO]

9
00:01:20,000 --> 00:01:30,000
 [BLANK_AUDIO]

10
00:01:30,000 --> 00:01:40,000
 [BLANK_AUDIO]

11
00:01:40,000 --> 00:01:50,000
 [BLANK_AUDIO]

12
00:01:50,000 --> 00:02:00,000
 [BLANK_AUDIO]

13
00:02:00,000 --> 00:02:10,000
 [BLANK_AUDIO]

14
00:02:10,000 --> 00:02:20,000
 [BLANK_AUDIO]

15
00:02:20,000 --> 00:02:30,000
 [BLANK_AUDIO]

16
00:02:30,000 --> 00:02:40,000
 [BLANK_AUDIO]

17
00:02:40,000 --> 00:02:50,000
 [BLANK_AUDIO]

18
00:02:50,000 --> 00:03:00,000
 [BLANK_AUDIO]

19
00:03:00,000 --> 00:03:10,000
 [BLANK_AUDIO]

20
00:03:10,000 --> 00:03:20,000
 [BLANK_AUDIO]

21
00:03:20,000 --> 00:03:30,000
 [BLANK_AUDIO]

22
00:03:30,000 --> 00:03:40,000
 [BLANK_AUDIO]

23
00:03:40,000 --> 00:03:46,000
 [BLANK_AUDIO]

24
00:03:46,000 --> 00:03:51,000
 >> Yeah, I'll start talking consistently now and it sounds like it's going

25
00:03:51,000 --> 00:03:54,000
 through the room at least, so that's a good sign.

26
00:03:54,000 --> 00:04:01,000
 [BLANK_AUDIO]

27
00:04:01,000 --> 00:04:03,000
 Is that coming through on the live stream?

28
00:04:03,000 --> 00:04:05,000
 [BLANK_AUDIO]

29
00:04:05,000 --> 00:04:06,000
 Are you able to hear me?

30
00:04:06,000 --> 00:04:08,000
 [BLANK_AUDIO]

31
00:04:08,000 --> 00:04:09,000
 It's a little bit delayed, I guess.

32
00:04:09,000 --> 00:04:16,000
 [BLANK_AUDIO]

33
00:04:16,000 --> 00:04:18,000
 I guess I've already spoken, so.

34
00:04:18,000 --> 00:04:28,000
 [BLANK_AUDIO]

35
00:04:28,000 --> 00:04:36,000
 [BLANK_AUDIO]

36
00:04:36,000 --> 00:04:37,000
 There's some seats here.

37
00:04:37,000 --> 00:04:40,000
 [BLANK_AUDIO]

38
00:04:40,000 --> 00:04:42,000
 I think sitting on the stairs isn't terrible.

39
00:04:42,000 --> 00:04:53,000
 [BLANK_AUDIO]

40
00:04:53,000 --> 00:04:54,000
 Yeah, there's some seats here.

41
00:04:54,000 --> 00:05:09,000
 [BLANK_AUDIO]

42
00:05:09,000 --> 00:05:10,000
 You think it's making noise?

43
00:05:10,000 --> 00:05:11,000
 Oh, nice.

44
00:05:11,000 --> 00:05:19,000
 [BLANK_AUDIO]

45
00:05:19,000 --> 00:05:21,000
 Okay, hi everybody.

46
00:05:21,000 --> 00:05:22,000
 [BLANK_AUDIO]

47
00:05:22,000 --> 00:05:23,000
 Thank you for coming.

48
00:05:23,000 --> 00:05:26,000
 Thank you for approximately the right number of you coming.

49
00:05:26,000 --> 00:05:30,000
 I didn't actually give you any information that would have been helpful to decide

50
00:05:30,000 --> 00:05:33,000
 whether you should come or not, but exactly the right number of people came,

51
00:05:33,000 --> 00:05:35,000
 well, plus or minus a few, so thank you for that.

52
00:05:35,000 --> 00:05:38,000
 I apologize for the room scheduling.

53
00:05:38,000 --> 00:05:43,000
 I'm glad that the class is popular, but I also don't envy the role of the scheduling

54
00:05:43,000 --> 00:05:49,000
 office right now, who are-- it's not just that there's classes that have big numbers

55
00:05:49,000 --> 00:05:53,000
 or something, but it's also that if you look at the registration numbers across the classes

56
00:05:53,000 --> 00:05:58,000
 and a histogram of these, or like a time plot of these numbers, they go like this.

57
00:05:58,000 --> 00:06:04,000
 And so the scheduling offices has no-- it's a very hard prediction problem for them.

58
00:06:04,000 --> 00:06:09,000
 And they're going to do their very best, I hope, to get us a bigger room, but we're

59
00:06:09,000 --> 00:06:12,000
 going to-- we'll roll with it as it happens.

60
00:06:12,000 --> 00:06:16,000
 So welcome to robotic manipulation.

61
00:06:16,000 --> 00:06:21,000
 It was important, I felt, to have in the class-- I would have loved to call the class just

62
00:06:21,000 --> 00:06:25,000
 manipulation, but I thought if somebody who doesn't know that we're talking about robots

63
00:06:25,000 --> 00:06:28,000
 saw that, you know, maybe from political science or something like that, they would think of

64
00:06:28,000 --> 00:06:29,000
 something very different.

65
00:06:29,000 --> 00:06:31,000
 So I thought, let's qualify that a little bit.

66
00:06:31,000 --> 00:06:34,000
 There was an early version that we called it intelligent robotic manipulation, but I

67
00:06:34,000 --> 00:06:38,000
 didn't want some other manipulation class to come around, and then it looked like a

68
00:06:38,000 --> 00:06:40,000
 put-down or something like that.

69
00:06:40,000 --> 00:06:45,000
 So robotic manipulation it is, and I think it's going to be a fun class.

70
00:06:45,000 --> 00:06:46,000
 There's a lot of things.

71
00:06:46,000 --> 00:06:49,000
 The field is alive with progress.

72
00:06:49,000 --> 00:06:55,000
 There's more robots out there doing more cool things now than ever before, and it's just

73
00:06:55,000 --> 00:06:57,000
 an incredibly exciting place to be.

74
00:06:57,000 --> 00:07:02,000
 So I hope to capture some of that enthusiasm for you, and even today, but certainly throughout

75
00:07:02,000 --> 00:07:04,000
 the course of the term.

76
00:07:04,000 --> 00:07:06,000
 Let me start by introducing us.

77
00:07:06,000 --> 00:07:07,000
 So I'm Russ.

78
00:07:07,000 --> 00:07:09,000
 I've been here for a while teaching robotics.

79
00:07:09,000 --> 00:07:11,000
 We've got an excellent teaching staff.

80
00:07:11,000 --> 00:07:13,000
 They happen to all be sitting together right here.

81
00:07:13,000 --> 00:07:18,000
 So Bo-Yan's here, and Anthony is here, and Rhea is here.

82
00:07:18,000 --> 00:07:20,000
 They're our TAs for the class.

83
00:07:20,000 --> 00:07:26,000
 If there are a lot of people in the class, we might get that last seat filled.

84
00:07:26,000 --> 00:07:28,000
 We'll see.

85
00:07:28,000 --> 00:07:32,000
 The other, I would say the one most important bit of information, which is not a hard thing

86
00:07:32,000 --> 00:07:40,000
 to remember, is that the website is at manipulation.mit.edu, and I'm not giving out any handouts, but

87
00:07:40,000 --> 00:07:45,000
 all of the course information, grading rubrics, collaboration policies, all of the things

88
00:07:45,000 --> 00:07:50,000
 that I am officially giving you, I am officially giving you with that link right there.

89
00:07:50,000 --> 00:07:57,000
 And if you have any questions or thoughts about that, I'm happy to take those.

90
00:07:57,000 --> 00:07:59,000
 There's an extra piece of the course.

91
00:07:59,000 --> 00:08:02,000
 It started last year, and it's continuing this year.

92
00:08:02,000 --> 00:08:08,000
 If you're in the undergraduate version of the course, it counts as a CIM now.

93
00:08:08,000 --> 00:08:15,000
 So as the department has grown, and now we have AI and decision-making as a core part

94
00:08:15,000 --> 00:08:20,000
 of the department, we wanted more of the AI and decision-making courses to be able to

95
00:08:20,000 --> 00:08:22,000
 count as a CIM requirement.

96
00:08:22,000 --> 00:08:29,000
 So natural language processing and this course have now taken on the ability to be a communications

97
00:08:29,000 --> 00:08:31,000
 intensive in the major class.

98
00:08:31,000 --> 00:08:36,000
 We have excellent teaching staff from CMS that are helping us with that.

99
00:08:36,000 --> 00:08:38,000
 David's here.

100
00:08:38,000 --> 00:08:45,000
 I think maybe Nora and Liz decided to save seats for the rest of you today.

101
00:08:45,000 --> 00:08:51,000
 So the way you should think about that, so it looks like 15, the exact distinction between

102
00:08:51,000 --> 00:08:55,000
 the two different tracks, the 4.

103
00:08:55,000 --> 00:08:57,000
 It also makes me sad, by the way.

104
00:08:57,000 --> 00:09:01,000
 Last year we had the number 6.800, which was like, I felt like I won the lottery getting

105
00:09:01,000 --> 00:09:05,000
 .800, and now we've got 4, 2, 1, 0, which is not as cool.

106
00:09:05,000 --> 00:09:09,000
 But if you're in the 4, 2, 1, 0, that's the undergraduate version of the class.

107
00:09:09,000 --> 00:09:11,000
 It counts as 15 credits.

108
00:09:11,000 --> 00:09:14,000
 You get one extra recitation on Fridays.

109
00:09:14,000 --> 00:09:18,000
 And I'll tell you a little bit about what comes with that, all the great things that

110
00:09:18,000 --> 00:09:20,000
 come with that.

111
00:09:20,000 --> 00:09:25,000
 And then if you're in 4, 2, 1, 2, you're in the graduate version of the class.

112
00:09:25,000 --> 00:09:28,000
 So just a very, the detailed differences are on the website.

113
00:09:28,000 --> 00:09:30,000
 We can answer questions.

114
00:09:30,000 --> 00:09:34,000
 At a high level, both groups will be doing a final project.

115
00:09:34,000 --> 00:09:40,000
 The undergraduate version will be doing a really good final project, coached by some

116
00:09:40,000 --> 00:09:43,000
 of the best communication people we have.

117
00:09:43,000 --> 00:09:49,000
 And I would say at the end of last year, the people that were in that track had some of

118
00:09:49,000 --> 00:09:56,000
 the best project presentations and videos and reports that I've seen ever.

119
00:09:56,000 --> 00:10:00,000
 So if you're, I think a big part of this class actually is this product that you'll have

120
00:10:00,000 --> 00:10:04,000
 at the end, which is a pretty, we've seen some pretty amazing videos.

121
00:10:04,000 --> 00:10:07,000
 You should go watch some of the videos from the previous years.

122
00:10:07,000 --> 00:10:11,000
 We had a robot playing the piano, like a concerto on a simulated piano.

123
00:10:11,000 --> 00:10:13,000
 We've had some amazing things.

124
00:10:13,000 --> 00:10:16,000
 And it'll look great on your portfolio, your CV.

125
00:10:16,000 --> 00:10:20,000
 And I mean, industry, I have people from industry saying, "Oh my gosh, who did that project?"

126
00:10:20,000 --> 00:10:21,000
 Right?

127
00:10:21,000 --> 00:10:24,000
 So I think it really is an amazing opportunity.

128
00:10:24,000 --> 00:10:27,000
 The recitations don't start tomorrow.

129
00:10:27,000 --> 00:10:29,000
 They start a week from tomorrow.

130
00:10:29,000 --> 00:10:35,000
 Before we've taught significant topics in manipulation, we're going to be doing, in

131
00:10:35,000 --> 00:10:41,000
 those recitations, reading some research papers and just understanding a rhetorical analysis

132
00:10:41,000 --> 00:10:43,000
 of how to write a good paper in manipulation.

133
00:10:43,000 --> 00:10:49,000
 And then that will graduate into the project, the work towards the project.

134
00:10:49,000 --> 00:10:52,000
 The graduate version will do also a project.

135
00:10:52,000 --> 00:10:55,000
 The emphasis will be more on the technical and less on the communication.

136
00:10:55,000 --> 00:11:00,000
 And the graduate version also gets a few extra problems and other things that are expecting

137
00:11:00,000 --> 00:11:04,000
 a higher level of maturity on the problem sets and the like.

138
00:11:04,000 --> 00:11:08,000
 Okay.

139
00:11:08,000 --> 00:11:15,000
 So, like I said, the information about the grading policy, collaboration policies, everything

140
00:11:15,000 --> 00:11:19,000
 is on the website, including the differences.

141
00:11:19,000 --> 00:11:24,000
 If you scroll around, it talks about the exact differences, what it counts for, what it doesn't

142
00:11:24,000 --> 00:11:25,000
 count for.

143
00:11:25,000 --> 00:11:30,000
 It all changed last year, so I completely expect questions and there might be details

144
00:11:30,000 --> 00:11:34,000
 that we didn't get.

145
00:11:34,000 --> 00:11:39,000
 And the last thing I'll say about sort of logistics before we dive into some robots

146
00:11:39,000 --> 00:11:44,000
 is that right now, you know, I'm going to say this again at the end, but just go to

147
00:11:44,000 --> 00:11:48,000
 the website, click on the link to join the Piazza group.

148
00:11:48,000 --> 00:11:52,000
 Apart from the one email I sent through the registrar yesterday, we're going to do all

149
00:11:52,000 --> 00:11:55,000
 of our communication through Piazza.

150
00:11:55,000 --> 00:12:00,000
 You can please review the course guidelines now so you're not surprised on anything later.

151
00:12:00,000 --> 00:12:05,000
 The lecture notes are all online at the same website.

152
00:12:05,000 --> 00:12:08,000
 The plan, the schedule is online.

153
00:12:08,000 --> 00:12:12,000
 We're going to have weekly problem sets throughout the class, typically due on a Wednesday cadence.

154
00:12:12,000 --> 00:12:17,000
 We'll have office hours to support that, probably Friday, Monday, and then we'll see

155
00:12:17,000 --> 00:12:20,000
 maybe if we give one right before it's due, too.

156
00:12:20,000 --> 00:12:23,000
 The first one will be released late tonight or tomorrow.

157
00:12:23,000 --> 00:12:29,000
 It'll be a light one, just a warm up, and it'll be due next Wednesday.

158
00:12:29,000 --> 00:12:35,000
 And then there's a lot of emphasis on the final project, and they've been really, really good.

159
00:12:35,000 --> 00:12:39,000
 In fact, I should just, I'll queue up a few of the good ones to make sure that you've

160
00:12:39,000 --> 00:12:45,000
 all seen some of them from last year, but they can be really exceptional.

161
00:12:45,000 --> 00:12:52,000
 Okay, the course notes, which are there, when I say read and please comment on the course notes.

162
00:12:52,000 --> 00:12:55,000
 So this is what you get when you go to the manipulation website.

163
00:12:55,000 --> 00:12:57,000
 It's HTML notes.

164
00:12:57,000 --> 00:12:58,000
 Some people hate that.

165
00:12:58,000 --> 00:13:02,000
 They want PDF, and you can get the PDF if you want, but the HTML is, I'm trying to do more

166
00:13:02,000 --> 00:13:04,000
 than you can do in a PDF.

167
00:13:04,000 --> 00:13:06,000
 I'm trying to have really interactive content.

168
00:13:06,000 --> 00:13:08,000
 There's animations that you'll be able to play.

169
00:13:08,000 --> 00:13:12,000
 There's interactive simulations you'll be able to play.

170
00:13:12,000 --> 00:13:16,000
 It's interactive in the sense that you can go in and you can comment, kind of like a

171
00:13:16,000 --> 00:13:21,000
 Google Doc, and we have discussions on there about, like, I have no idea what you meant

172
00:13:21,000 --> 00:13:26,000
 by this sentence, Russ, and so I'll try to say, I tried to say this, and people really

173
00:13:26,000 --> 00:13:31,000
 do help the notes along and I think get to the bottom of some issues.

174
00:13:31,000 --> 00:13:35,000
 Okay, and the link to the course part of the website is right in this course being taught

175
00:13:35,000 --> 00:13:37,000
 at MIT.

176
00:13:37,000 --> 00:13:41,000
 Okay, so I hope you take a look at that.

177
00:13:41,000 --> 00:13:45,000
 I hope you use that, and I'm happy to take feedback on it.

178
00:13:45,000 --> 00:13:55,000
 There's also all these links to the online notebooks that go along with the course.

179
00:13:55,000 --> 00:14:00,000
 One of the cool things about it, I'm testing now my suspiciously bad connection to the

180
00:14:00,000 --> 00:14:06,000
 MIT network, which is interfering with the Stata network, but one of the great things

181
00:14:06,000 --> 00:14:14,000
 about all the class infrastructure is that now we can allow simulations to load over

182
00:14:14,000 --> 00:14:17,000
 the internet with no installation.

183
00:14:17,000 --> 00:14:22,000
 So, it used to be that we would try to help people through, limp through sort of making

184
00:14:22,000 --> 00:14:27,000
 all of our robot code run on your machine and people would come with a Win32 machine

185
00:14:27,000 --> 00:14:31,000
 or like, you know, Ubuntu 12 or something like this, right?

186
00:14:31,000 --> 00:14:38,000
 And, that's all gone now because there's online cloud resources that come provisioned.

187
00:14:38,000 --> 00:14:42,000
 You just log on to DeepNote, which if you've used Google Co-Laboratory, we used Google

188
00:14:42,000 --> 00:14:46,000
 Co-Laboratory for a while, we've switched to DeepNote because it's easier to provision

189
00:14:46,000 --> 00:14:51,000
 a stable, Colab can change the requirements out from under me and they always do it like

190
00:14:51,000 --> 00:14:56,000
 on the first day of class, but DeepNote, I get to provision it with a Docker image.

191
00:14:56,000 --> 00:15:00,000
 And so, you could basically be able to go to the website, instantly run the code, even

192
00:15:00,000 --> 00:15:05,000
 the visualization, which is not loading very well for me right here, I'll run a local version.

193
00:15:05,000 --> 00:15:09,000
 Should all just run with no installation on any machine you've got.

194
00:15:09,000 --> 00:15:11,000
 I'll just run a local one here.

195
00:15:11,000 --> 00:15:18,000
 So, it looks like this.

196
00:15:18,000 --> 00:15:21,000
 This is the intro notebook just running locally.

197
00:15:21,000 --> 00:15:24,000
 You'll get a little visualization just in the web.

198
00:15:24,000 --> 00:15:29,000
 And then, if you run the very first example, you'll get a little robot up here and you

199
00:15:29,000 --> 00:15:33,000
 can go into the controls and right through the web, you can sort of drive your robot

200
00:15:33,000 --> 00:15:38,000
 around and I'll see if I can pick up the little red brick here for us here.

201
00:15:38,000 --> 00:15:43,000
 Be sad if I can't.

202
00:15:43,000 --> 00:15:44,000
 It's close.

203
00:15:44,000 --> 00:15:45,000
 A little back up.

204
00:15:45,000 --> 00:15:50,000
 It's easier with a joystick, but.

205
00:15:50,000 --> 00:15:55,000
 It's a full physics engine if I grab the brick, pull it up, right?

206
00:15:55,000 --> 00:15:59,000
 I can probably even throw it if I was really good.

207
00:15:59,000 --> 00:16:00,000
 Okay.

208
00:16:00,000 --> 00:16:03,000
 And that's just going to hopefully just work seamlessly for you.

209
00:16:03,000 --> 00:16:07,000
 That's a long road to get to the point where it really mostly just works.

210
00:16:07,000 --> 00:16:12,000
 Every once in a while, the cloud services will have an issue or something, but we've been

211
00:16:12,000 --> 00:16:13,000
 pretty lucky with that.

212
00:16:13,000 --> 00:16:21,000
 It works pretty darn well.

213
00:16:21,000 --> 00:16:22,000
 Ah.

214
00:16:22,000 --> 00:16:23,000
 See?

215
00:16:23,000 --> 00:16:28,000
 It just loaded slowly over the MIT network, but it was there.

216
00:16:28,000 --> 00:16:29,000
 Okay.

217
00:16:29,000 --> 00:16:35,000
 So, my goals for today are to give you kind of a tour through what you're going to learn

218
00:16:35,000 --> 00:16:41,000
 in the course and also to give you a little bit of the sort of initial thinking about

219
00:16:41,000 --> 00:16:45,000
 not only the components of a manipulation system that we're going to talk about from

220
00:16:45,000 --> 00:16:50,000
 perception and planning and control, but also the way that we think about it in this class,

221
00:16:50,000 --> 00:16:58,000
 which is a little bit different, I think, than your average manipulation class.

222
00:16:58,000 --> 00:17:03,000
 So, I try to take a little bit of a systems theory perspective when it fits, and I want

223
00:17:03,000 --> 00:17:07,000
 to make sure I make some of those connections for you today.

224
00:17:07,000 --> 00:17:11,000
 But let me start about just making sure we know what I mean by manipulation, because

225
00:17:11,000 --> 00:17:16,000
 actually, I find a lot of people when they hear robot manipulation, they think of fairly

226
00:17:16,000 --> 00:17:21,000
 narrow examples of robots that are just doing pick and place, for instance.

227
00:17:21,000 --> 00:17:24,000
 And actually, manipulation is much more than pick and place.

228
00:17:24,000 --> 00:17:27,000
 If you take away one thing from today's lecture, I hope you'll say manipulation is more than

229
00:17:27,000 --> 00:17:29,000
 pick and place.

230
00:17:29,000 --> 00:17:30,000
 Okay?

231
00:17:30,000 --> 00:17:39,000
 Matt Mason, who's one of the leaders and big names in the field from Carnegie Mellon,

232
00:17:39,000 --> 00:17:43,000
 he wrote an excellent review on robotic manipulation a few years ago.

233
00:17:43,000 --> 00:17:47,000
 And one of the interesting things he did is he really tried to define, think deeply about

234
00:17:47,000 --> 00:17:49,000
 what does it mean to be manipulation.

235
00:17:49,000 --> 00:17:50,000
 Okay?

236
00:17:50,000 --> 00:17:52,000
 And he actually gave five definitions, because he couldn't decide.

237
00:17:52,000 --> 00:17:54,000
 He couldn't narrow it down, I guess.

238
00:17:54,000 --> 00:18:00,000
 You know, the first one was, you know, just manipulation means activities performed by

239
00:18:00,000 --> 00:18:01,000
 the hands.

240
00:18:01,000 --> 00:18:07,000
 I won't take you through all of them, but he kind of eventually got to, you know, manipulation

241
00:18:07,000 --> 00:18:11,000
 refers to an agent's control of its environment through contact.

242
00:18:11,000 --> 00:18:16,000
 And I like that very much, I think, through selective contact.

243
00:18:16,000 --> 00:18:22,000
 And I think it captures some of the, I mean, what robots are supposed to do, right?

244
00:18:22,000 --> 00:18:27,000
 What makes robotics special compared to, let's say, computer vision or natural language or

245
00:18:27,000 --> 00:18:29,000
 something like this is you get to move stuff.

246
00:18:29,000 --> 00:18:31,000
 You get to change the world.

247
00:18:31,000 --> 00:18:35,000
 And that, I mean, you could argue, maybe I will argue, that if we're really going to

248
00:18:35,000 --> 00:18:41,000
 solve intelligence, it seems hard to imagine solving intelligence as a passive observer

249
00:18:41,000 --> 00:18:43,000
 of the world through cameras.

250
00:18:43,000 --> 00:18:48,000
 I think being able to pick stuff up and move it around and interact with the world seems

251
00:18:48,000 --> 00:18:51,000
 pretty essential to our natural intelligence.

252
00:18:51,000 --> 00:18:59,000
 And that's what this class is about, is filling in that part of the artificial intelligence

253
00:18:59,000 --> 00:19:00,000
 spectrum.

254
00:19:00,000 --> 00:19:01,000
 Okay?

255
00:19:01,000 --> 00:19:06,000
 So, if I take that little example, like I just showed you, of a robot moving a brick

256
00:19:06,000 --> 00:19:11,000
 around, we can sort of, you know, think through what that's going to look like.

257
00:19:11,000 --> 00:19:13,000
 That's a pick and place example.

258
00:19:13,000 --> 00:19:17,000
 But I really want to say, you know, robotic manipulation is not just pick and place.

259
00:19:17,000 --> 00:19:23,000
 This is clearly robot manipulation by Matt Mason's definition.

260
00:19:23,000 --> 00:19:27,000
 And it's way harder than picking up a red brick and moving it to the side, right?

261
00:19:27,000 --> 00:19:32,000
 And if you look at the rich contact that's happening between his fingers and the shoelaces,

262
00:19:32,000 --> 00:19:38,000
 and even just the dynamics of the shoelaces, you know, robots aren't doing this yet.

263
00:19:38,000 --> 00:19:39,000
 Okay?

264
00:19:39,000 --> 00:19:42,000
 So we have grand challenges just sort of in the mechanics of manipulation.

265
00:19:42,000 --> 00:19:47,000
 But I'll show you some examples, too, that even if we're doing pick and place, if we're

266
00:19:47,000 --> 00:19:51,000
 trying to do it out in the wild, things get pretty rich and pretty complicated in other

267
00:19:51,000 --> 00:19:53,000
 axes, too.

268
00:19:53,000 --> 00:19:58,000
 So I would say that's where I feel like Matt's definition doesn't completely capture the

269
00:19:58,000 --> 00:20:00,000
 goals for the course.

270
00:20:00,000 --> 00:20:02,000
 Matt says manipulation is about contact.

271
00:20:02,000 --> 00:20:04,000
 And I think that's, of course, true.

272
00:20:04,000 --> 00:20:09,000
 But if you think about doing manipulation not just in a closed environment or in a factory

273
00:20:09,000 --> 00:20:14,000
 or something, but if you are out in the broad world and you want to send a robot out and

274
00:20:14,000 --> 00:20:21,000
 do manipulation, then there's more broad requirements that come into play.

275
00:20:21,000 --> 00:20:29,000
 I think it requires, in order to be in control of the environment, that's like an arbitrary

276
00:20:29,000 --> 00:20:33,000
 loophole in the definition where we can inject having to understand everything about the

277
00:20:33,000 --> 00:20:34,000
 world, right?

278
00:20:34,000 --> 00:20:38,000
 Having a very rich perceptual understanding of the environment.

279
00:20:38,000 --> 00:20:42,000
 I don't mean putting a bounding box around a person.

280
00:20:42,000 --> 00:20:43,000
 That's good.

281
00:20:43,000 --> 00:20:47,000
 But I need to know how much mass, the things I'm going to, you know, what's an object,

282
00:20:47,000 --> 00:20:49,000
 what's not an object, what happens when I push it.

283
00:20:49,000 --> 00:20:53,000
 These are demanding things that a computer vision system doesn't typically give you out

284
00:20:53,000 --> 00:20:55,000
 of the box.

285
00:20:55,000 --> 00:20:58,000
 This common sense of understanding, like what's going to happen if I push something?

286
00:20:58,000 --> 00:21:02,000
 Am I going to topple a pile if I push it in the bottom?

287
00:21:02,000 --> 00:21:07,000
 You know, these kind of things are grand challenges in AI.

288
00:21:07,000 --> 00:21:11,000
 And I feel like they're part, they're under the umbrella of manipulation.

289
00:21:11,000 --> 00:21:12,000
 Okay?

290
00:21:12,000 --> 00:21:17,000
 The ability to make very long-term plans at the task level, like what am I going to do

291
00:21:17,000 --> 00:21:19,000
 to get the milk out of the fridge?

292
00:21:19,000 --> 00:21:20,000
 Okay?

293
00:21:20,000 --> 00:21:23,000
 I've got to, first I've got to open the fridge, you know, then I've got to move the pickles

294
00:21:23,000 --> 00:21:24,000
 out of the way.

295
00:21:24,000 --> 00:21:27,000
 I'm not sure if you keep your pickles close to your milk, but you know what I mean, right?

296
00:21:27,000 --> 00:21:31,000
 And then you reach back to the, there's a lot of steps involved to do a manipulation

297
00:21:31,000 --> 00:21:37,000
 task which require a pretty high-level understanding of the world and reasoning long into the future.

298
00:21:37,000 --> 00:21:41,000
 So, that's in the course material, too.

299
00:21:41,000 --> 00:21:44,000
 And then you have to, once you've decided to do that, you've got to figure out how to

300
00:21:44,000 --> 00:21:47,000
 use your motors and your joints to make that happen.

301
00:21:47,000 --> 00:21:48,000
 Right?

302
00:21:48,000 --> 00:21:54,000
 So, combining those different levels of abstraction is a grand challenge that we try to face.

303
00:21:54,000 --> 00:21:58,000
 So let me show you a system that exemplifies some of that.

304
00:21:58,000 --> 00:22:03,000
 It was a project a few years ago at the Toyota Research Institute, TRI, which is just down

305
00:22:03,000 --> 00:22:04,000
 the street.

306
00:22:04,000 --> 00:22:09,000
 I've been working with them for a number of years now to try to make some of the larger-scale

307
00:22:09,000 --> 00:22:14,000
 examples of manipulation and take them to higher levels of maturity.

308
00:22:14,000 --> 00:22:18,000
 And this is an example that I learned a lot from, trying to just, let's see, if you could

309
00:22:18,000 --> 00:22:22,000
 take a big robot, this is not something that we are advertising you put in the home, but

310
00:22:22,000 --> 00:22:28,000
 it is a robot that we have today that works pretty well, and we asked, could that robot,

311
00:22:28,000 --> 00:22:32,000
 if someone put it in front of their sink and asked it to load the dishwasher, could it

312
00:22:32,000 --> 00:22:33,000
 do it?

313
00:22:33,000 --> 00:22:36,000
 What are all the problems involved in doing that?

314
00:22:36,000 --> 00:22:42,000
 So, the problem in the open-world manipulation sense is someone, that's the one, he comes

315
00:22:42,000 --> 00:22:46,000
 and he dumped whatever random things into the sink.

316
00:22:46,000 --> 00:22:49,000
 Amongst them, some of them are dishes.

317
00:22:49,000 --> 00:22:51,000
 Some mugs, some plates, some spoons.

318
00:22:51,000 --> 00:22:56,000
 There's a dishwasher right next to it, and the task is to open up the dishwasher and

319
00:22:56,000 --> 00:23:04,000
 to start putting the mugs in the top rack, the plates in the bottom rack, the trash off

320
00:23:04,000 --> 00:23:08,000
 to the side, and the silverware in the little silverware rack.

321
00:23:08,000 --> 00:23:09,000
 Okay?

322
00:23:09,000 --> 00:23:14,000
 And this is a complete manipulation stack that did all of those components of perception,

323
00:23:14,000 --> 00:23:17,000
 planning, control, high-level reasoning, okay?

324
00:23:17,000 --> 00:23:22,000
 And it took a lot of work to put it all together, and then it took a lot more work to try to

325
00:23:22,000 --> 00:23:25,000
 get it to operate at a very high level of repeatability.

326
00:23:25,000 --> 00:23:30,000
 Like, if the same challenges that autonomous driving companies are facing these days to

327
00:23:30,000 --> 00:23:34,000
 try to, you know, it's one thing to make a car drive down the road and make a video,

328
00:23:34,000 --> 00:23:39,000
 but to make it never crash, you have to deal with all these long tails of the distribution,

329
00:23:39,000 --> 00:23:43,000
 all the random things that could happen with the lighting conditions, with the stuff that's

330
00:23:43,000 --> 00:23:48,000
 in the sink, and taking this system to a maturity was an excellent exercise that really changed

331
00:23:48,000 --> 00:23:52,000
 my view of what are the hard problems.

332
00:23:52,000 --> 00:23:59,000
 If you look down at the details, the individual skills that it had to do were actually fairly

333
00:23:59,000 --> 00:24:04,000
 complicated from a control perspective, from a motion planning perspective in some cases.

334
00:24:04,000 --> 00:24:06,000
 It had to open the dishwasher door.

335
00:24:06,000 --> 00:24:09,000
 It would nudge things out of the corner.

336
00:24:09,000 --> 00:24:13,000
 I mean, this is partly because it had an enormous hand and a small knife stuck in the corner,

337
00:24:13,000 --> 00:24:17,000
 and so you can't do what a human would do, which is kind of, you know, pick it up.

338
00:24:17,000 --> 00:24:19,000
 It had to really take different tasks.

339
00:24:19,000 --> 00:24:22,000
 To pick up the plate is sort of my favorite one.

340
00:24:22,000 --> 00:24:24,000
 You had to pick up a plate from a stack of plates.

341
00:24:24,000 --> 00:24:30,000
 This big hand had to kind of, you know, go in, and this is a feedback law, which was

342
00:24:30,000 --> 00:24:36,000
 constantly monitoring its sensors to slide under until it knew that it had pushed far enough,

343
00:24:36,000 --> 00:24:38,000
 grab the plate, pick it up, and move.

344
00:24:38,000 --> 00:24:44,000
 Each of the individual pieces of this were actually pretty sophisticated, okay, but then

345
00:24:44,000 --> 00:24:48,000
 you had to assemble that all into this higher level machinery.

346
00:24:48,000 --> 00:24:52,000
 By the way, a big part of that was using simulation.

347
00:24:52,000 --> 00:24:58,000
 That actually, that video right there, if you can tell, that's actually a simulation,

348
00:24:58,000 --> 00:25:03,000
 right, of the robot picking up a plate, and the way we were able to get that to a pretty

349
00:25:03,000 --> 00:25:08,000
 high level of maturity was by having a very good match between simulation and reality

350
00:25:08,000 --> 00:25:13,000
 that we worked very hard on and getting to the point where we could stress test in simulation,

351
00:25:13,000 --> 00:25:19,000
 find the corner cases in simulation, and expect those corner cases to start disappearing in reality.

352
00:25:19,000 --> 00:25:25,000
 Okay, so, a lot of work on simulation, and that's a relatively new thing.

353
00:25:25,000 --> 00:25:29,000
 A few years ago, I said this in the text, right, a few years ago, I remember when we

354
00:25:29,000 --> 00:25:35,000
 were doing humanoid robots, and I was talking to my students in the lab about, you know,

355
00:25:35,000 --> 00:25:39,000
 we should be doing manipulation in simulation, too, because it's working so well for our

356
00:25:39,000 --> 00:25:44,000
 walking robots, and I remember, you know, they looked at me like, "Russ, you can't do

357
00:25:44,000 --> 00:25:46,000
 manipulation research in simulation.

358
00:25:46,000 --> 00:25:48,000
 It depends on perception.

359
00:25:48,000 --> 00:25:51,000
 You can't simulate perception well enough to do that in simulation.

360
00:25:51,000 --> 00:25:56,000
 You know, the dynamics of contact, like subtle things between a hand, you can't do that in

361
00:25:56,000 --> 00:25:57,000
 simulation.

362
00:25:57,000 --> 00:25:58,000
 Simulators aren't good enough."

363
00:25:58,000 --> 00:26:00,000
 Okay, and then, it changed.

364
00:26:00,000 --> 00:26:06,000
 Like, a few years ago, computer graphics renderers got good enough.

365
00:26:06,000 --> 00:26:08,000
 You used Blender, for instance, as your renderer.

366
00:26:08,000 --> 00:26:15,000
 It's an open source rendering engine, right, and everybody was suspicious that if the rendering

367
00:26:15,000 --> 00:26:20,000
 wasn't perfect, then, like, a machine learning computer vision system would cheat, and you

368
00:26:20,000 --> 00:26:25,000
 would know how to use the artifacts of the renderer to solve the problem, and it wouldn't

369
00:26:25,000 --> 00:26:27,000
 actually work in reality.

370
00:26:27,000 --> 00:26:28,000
 But guess what?

371
00:26:28,000 --> 00:26:32,000
 The renderers are good enough, and people train in simulation and get it to work in

372
00:26:32,000 --> 00:26:33,000
 reality.

373
00:26:33,000 --> 00:26:34,000
 That's changed.

374
00:26:34,000 --> 00:26:37,000
 People now think you can train perception systems.

375
00:26:37,000 --> 00:26:39,000
 The other big aspect of that is the physics.

376
00:26:39,000 --> 00:26:45,000
 The physics engines were not good enough, the real-time compatible physics engines.

377
00:26:45,000 --> 00:26:50,000
 They were working for legs, which are actually relatively easier for a walking robot to simulate,

378
00:26:50,000 --> 00:26:54,000
 but for the delicate interactions required in manipulation, they weren't good enough

379
00:26:54,000 --> 00:26:57,000
 a few years ago, and now, they're getting to be good enough.

380
00:26:57,000 --> 00:27:03,000
 And there's nuances in the different simulators, but we've seen dramatic success in transferring

381
00:27:03,000 --> 00:27:07,000
 results from simulation into reality, if you do the work to match.

382
00:27:07,000 --> 00:27:14,000
 You have to make sure your models match into the simulator.

383
00:27:14,000 --> 00:27:18,000
 And all of those components had to go together into this high-level planner, so that if someone

384
00:27:18,000 --> 00:27:24,000
 came and adversarially-- so in Boston Dynamics, they kick the robot, and that's cool.

385
00:27:24,000 --> 00:27:26,000
 We just close the dishwasher drawer.

386
00:27:26,000 --> 00:27:29,000
 That's not quite as cool, but it tries to make the same point, right?

387
00:27:29,000 --> 00:27:32,000
 If someone came and messed with your robot, it had to be smart enough to actually, in

388
00:27:32,000 --> 00:27:35,000
 that case, it was putting a mug in the top.

389
00:27:35,000 --> 00:27:37,000
 And I just realized, oh, someone closed the dishwasher drawer.

390
00:27:37,000 --> 00:27:41,000
 I'm going to set the mug back down, pick it up, because I've only got one hand.

391
00:27:41,000 --> 00:27:42,000
 That's pretty annoying, right?

392
00:27:42,000 --> 00:27:46,000
 And then, you know, you could do it all day long, and you feel bad for the robot.

393
00:27:46,000 --> 00:27:50,000
 But yeah, so that was a complete system, end-to-end.

394
00:27:50,000 --> 00:27:55,000
 And that's kind of the goal for the class, is to help you build out a complete system

395
00:27:55,000 --> 00:28:00,000
 and understand the nuances, some of the interesting parts of the algorithms at each level of those

396
00:28:00,000 --> 00:28:04,000
 hierarchy.

397
00:28:04,000 --> 00:28:06,000
 OK, so it really does go--

398
00:28:06,000 --> 00:28:11,000
 I'd say there's kind of a ladder of complexity in terms of high-level reasoning, if you will,

399
00:28:11,000 --> 00:28:15,000
 which involves scene understanding, being able to make sense of what objects are in

400
00:28:15,000 --> 00:28:16,000
 the world.

401
00:28:16,000 --> 00:28:19,000
 You know, where is the milk in the fridge?

402
00:28:19,000 --> 00:28:23,000
 Deciding to move the pickles before you pick the milk.

403
00:28:23,000 --> 00:28:28,000
 And then there's low-level, like how do I feel forces on my hand and decide I should

404
00:28:28,000 --> 00:28:30,000
 do something a little bit different?

405
00:28:30,000 --> 00:28:36,000
 So it's very interesting to try to span that whole space.

406
00:28:36,000 --> 00:28:40,000
 I come from the controls perspective towards this.

407
00:28:40,000 --> 00:28:42,000
 And some of you have taken it under-actuated with me.

408
00:28:42,000 --> 00:28:46,000
 So let me just sort of connect that to the-- you know, from my view of the world.

409
00:28:46,000 --> 00:28:50,000
 Like, why did I come from controls towards manipulation?

410
00:28:50,000 --> 00:28:51,000
 OK?

411
00:28:51,000 --> 00:28:55,000
 So, you know, before, we were doing humanoid robots.

412
00:28:55,000 --> 00:28:57,000
 This was the DARPA Robotics Challenge.

413
00:28:57,000 --> 00:28:59,000
 That was an early version of the Boston Dynamics robot.

414
00:28:59,000 --> 00:29:01,000
 It's doing backflips now.

415
00:29:01,000 --> 00:29:05,000
 It was a lot heavier and not as backflip-ready back then.

416
00:29:05,000 --> 00:29:09,000
 But we spent a lot of time on this robot and worked very hard on the control system and

417
00:29:09,000 --> 00:29:11,000
 very proud of what we did.

418
00:29:11,000 --> 00:29:15,000
 And we, you know, we worked very hard on understanding the dynamics of that system,

419
00:29:15,000 --> 00:29:19,000
 simulating that system, writing, you know, understanding its robustness properties and

420
00:29:19,000 --> 00:29:20,000
 the like.

421
00:29:20,000 --> 00:29:23,000
 We got to the point where, you know, even when it was getting out of the car-- that

422
00:29:23,000 --> 00:29:25,000
 was the hardest part of the challenge, by the way.

423
00:29:25,000 --> 00:29:29,000
 There was a-- we had to drive this little car with this enormously big robot.

424
00:29:29,000 --> 00:29:34,000
 And then getting out of that car was like solving a Jenga or something or, you know,

425
00:29:34,000 --> 00:29:36,000
 Twister to get out of the car.

426
00:29:36,000 --> 00:29:38,000
 But we worked hard on the feedback controller.

427
00:29:38,000 --> 00:29:42,000
 And you got to the point where even if Andres is jumping on the back of the car, the robot

428
00:29:42,000 --> 00:29:43,000
 really didn't fall down.

429
00:29:43,000 --> 00:29:47,000
 So we-- in some sense, I think we know a lot about feedback control.

430
00:29:47,000 --> 00:29:53,000
 Robotics has gotten pretty good at controlling complicated robots like that, right?

431
00:29:53,000 --> 00:29:57,000
 Even perception is a big part of walking around the world, right?

432
00:29:57,000 --> 00:30:04,000
 So we had to use our onboard sensors to sense the world, you know, understand it well enough

433
00:30:04,000 --> 00:30:08,000
 that we knew what surfaces we could step on, where we could-- where we should not step,

434
00:30:08,000 --> 00:30:09,000
 right?

435
00:30:09,000 --> 00:30:12,000
 So we were solving problems like that.

436
00:30:12,000 --> 00:30:15,000
 This was in, you know, 2015 kind of technology.

437
00:30:15,000 --> 00:30:20,000
 It was actually right before the machine learning boom, right?

438
00:30:20,000 --> 00:30:24,000
 So this was all much more geometric perception at the time.

439
00:30:24,000 --> 00:30:28,000
 And like a year later, we would have done it with deep learning probably.

440
00:30:28,000 --> 00:30:34,000
 But we had a good pipeline for perception.

441
00:30:34,000 --> 00:30:37,000
 But none of that gets me through the sink, right?

442
00:30:37,000 --> 00:30:41,000
 So it's interesting to see where that sort of dies, right?

443
00:30:41,000 --> 00:30:47,000
 Is that the amount of understanding you have to do to understand where to walk in the world

444
00:30:47,000 --> 00:30:52,000
 is so much less in terms of understanding perceptually the world

445
00:30:52,000 --> 00:30:56,000
 that it really-- it only scratched the surface on the really hard problems.

446
00:30:56,000 --> 00:31:00,000
 So this is an image of a sink with mugs in it.

447
00:31:00,000 --> 00:31:02,000
 And those points there are an estimate.

448
00:31:02,000 --> 00:31:07,000
 We're trying to estimate-- this is now a deep learning system that's estimating the poses of the object.

449
00:31:07,000 --> 00:31:11,000
 And those are representing the uncertainty of the object's pose,

450
00:31:11,000 --> 00:31:14,000
 those different colors and the size of those colors.

451
00:31:14,000 --> 00:31:16,000
 And there's someone threw a napkin in there, right?

452
00:31:16,000 --> 00:31:18,000
 And there's a mug underneath that napkin.

453
00:31:18,000 --> 00:31:22,000
 And it's pretty confused about whether that's actually a mug right now.

454
00:31:22,000 --> 00:31:25,000
 And even just knowing that those things are separate mugs versus one,

455
00:31:25,000 --> 00:31:29,000
 I mean, this is just a harder manipulation problem-- or a perception problem.

456
00:31:29,000 --> 00:31:35,000
 And it requires a lot more work, not only in perception for the sake of perception,

457
00:31:35,000 --> 00:31:41,000
 but the connections between perception and control.

458
00:31:41,000 --> 00:31:47,000
 Especially-- this is just an example of-- now I have to manipulate any mug.

459
00:31:47,000 --> 00:31:51,000
 And the number of mugs you can find if someone throws them in the sink,

460
00:31:51,000 --> 00:31:53,000
 they're pretty diverse.

461
00:31:53,000 --> 00:31:56,000
 So knowing how to manipulate a particular mug

462
00:31:56,000 --> 00:32:00,000
 is not enough to understand how to manipulate all the mugs.

463
00:32:00,000 --> 00:32:05,000
 How do you program a robot in a way that basically solves the mug problem

464
00:32:05,000 --> 00:32:08,000
 when someone could have gone to the Disney store

465
00:32:08,000 --> 00:32:13,000
 and came back with a mug that looks like one of the seven dwarfs?

466
00:32:13,000 --> 00:32:16,000
 That just totally breaks a lot of our perception systems.

467
00:32:16,000 --> 00:32:22,000
 And we've been trying to generalize their tools to do much more general--

468
00:32:22,000 --> 00:32:26,000
 have a robustness in a much more general sense.

469
00:32:26,000 --> 00:32:28,000
 I care a lot about feedback, right?

470
00:32:28,000 --> 00:32:31,000
 We made Atlas not fall down when Andres was jumping on the car.

471
00:32:31,000 --> 00:32:35,000
 How important is feedback in manipulation?

472
00:32:35,000 --> 00:32:38,000
 It's an open question, I would say.

473
00:32:38,000 --> 00:32:40,000
 I mean, I strongly believe in the answer,

474
00:32:40,000 --> 00:32:44,000
 but there's people out there that are absolutely not thinking about feedback

475
00:32:44,000 --> 00:32:47,000
 in the actual manipulation of the hand sort of problem.

476
00:32:47,000 --> 00:32:52,000
 What they're doing instead is building really clever graspers-- grippers, right?

477
00:32:52,000 --> 00:32:56,000
 So this is a soft robotic hand that you can--

478
00:32:56,000 --> 00:32:58,000
 I mean, it's just being told to squeeze,

479
00:32:58,000 --> 00:33:00,000
 but the dynamics of the hand are such that

480
00:33:00,000 --> 00:33:02,000
 pretty much anything you put down in front of it,

481
00:33:02,000 --> 00:33:06,000
 you've got to make a nice conformant grasp around this and pull it over.

482
00:33:06,000 --> 00:33:11,000
 And for some class of problems, these hands knock it out of the park.

483
00:33:11,000 --> 00:33:15,000
 But like I said, manipulation is more than pick and place.

484
00:33:15,000 --> 00:33:18,000
 And if you look at how humans-- if you just watch yourself--

485
00:33:18,000 --> 00:33:20,000
 go home and just-- when you're making dinner tonight

486
00:33:20,000 --> 00:33:25,000
 or loading the dishwasher or something, watch yourself.

487
00:33:25,000 --> 00:33:29,000
 The things we do with our hands, they're so hard for us to reason about.

488
00:33:29,000 --> 00:33:32,000
 They're subconscious.

489
00:33:32,000 --> 00:33:34,000
 But there's always these-- like right here, look at that.

490
00:33:34,000 --> 00:33:38,000
 She missed a little bit, right, and then does this corrective action.

491
00:33:38,000 --> 00:33:42,000
 And I believe that actually we're missing out a lot

492
00:33:42,000 --> 00:33:47,000
 by not having rich feedback loops connecting perception

493
00:33:47,000 --> 00:33:53,000
 and tactile sensors to the commands we send to our robot.

494
00:33:53,000 --> 00:33:56,000
 And the world is now seeing more and more success

495
00:33:56,000 --> 00:33:58,000
 of feedback control and manipulation.

496
00:33:58,000 --> 00:34:03,000
 And so that connects to my background as well.

497
00:34:03,000 --> 00:34:05,000
 This is just another-- you can watch--

498
00:34:05,000 --> 00:34:09,000
 if you watch high-speed video of yourself doing anything with your hands,

499
00:34:09,000 --> 00:34:11,000
 it's amazing what we do.

500
00:34:11,000 --> 00:34:13,000
 And you don't even think about it, right?

501
00:34:13,000 --> 00:34:15,000
 The way humans load a dishwasher is so different

502
00:34:15,000 --> 00:34:19,000
 than the way we were having our robots load the dishwasher.

503
00:34:19,000 --> 00:34:22,000
 The robot would try to line up the plate and stick it down in the slot.

504
00:34:22,000 --> 00:34:25,000
 Humans just go bang, boom, and just rely on the fact

505
00:34:25,000 --> 00:34:27,000
 that it kind of will fall into place.

506
00:34:27,000 --> 00:34:30,000
 We're so clever, and we're so dexterous,

507
00:34:30,000 --> 00:34:36,000
 and robots are not getting it done that way.

508
00:34:36,000 --> 00:34:39,000
 So when I think about control for manipulation,

509
00:34:39,000 --> 00:34:41,000
 when I think about control for the humanoid,

510
00:34:41,000 --> 00:34:44,000
 we have a problem, which is that the robot has some joints.

511
00:34:44,000 --> 00:34:47,000
 We want those joints and maybe the center of mass

512
00:34:47,000 --> 00:34:50,000
 to go through some trajectory, okay?

513
00:34:50,000 --> 00:34:53,000
 And we know how to think about that, and we know how to build models for that,

514
00:34:53,000 --> 00:34:56,000
 and we can build models that work even on various terrain.

515
00:34:56,000 --> 00:34:58,000
 But we're thinking about manipulation.

516
00:34:58,000 --> 00:35:00,000
 It's not just about controlling the robot anymore.

517
00:35:00,000 --> 00:35:03,000
 That's part of the problem. That's a sub-piece of the problem.

518
00:35:03,000 --> 00:35:05,000
 But it's not just controlling the arm.

519
00:35:05,000 --> 00:35:08,000
 The state you're trying to control is the state of the robot,

520
00:35:08,000 --> 00:35:10,000
 but also the state of the world,

521
00:35:10,000 --> 00:35:13,000
 in this case, the state of the red brick, okay?

522
00:35:13,000 --> 00:35:15,000
 And that's what makes it interesting.

523
00:35:15,000 --> 00:35:19,000
 That's what makes it underactuated, for instance, okay?

524
00:35:19,000 --> 00:35:21,000
 So for me, it lights me up.

525
00:35:21,000 --> 00:35:24,000
 I think to the point where I really think

526
00:35:24,000 --> 00:35:28,000
 the next big thing that controls has to do--

527
00:35:28,000 --> 00:35:30,000
 that's a biased opinion--

528
00:35:30,000 --> 00:35:34,000
 but the thing that will grow controls into the next set of great problems,

529
00:35:34,000 --> 00:35:38,000
 I think manipulation has a lot of that richness, and here's why.

530
00:35:38,000 --> 00:35:41,000
 You know, controlling the state of this red brick is sort of--

531
00:35:41,000 --> 00:35:44,000
 I kind of know how to formulate that problem at least.

532
00:35:44,000 --> 00:35:47,000
 Maybe it's a hard control problem because of the contact mechanics,

533
00:35:47,000 --> 00:35:50,000
 but I kind of know how to write that problem down.

534
00:35:50,000 --> 00:35:53,000
 But if I want to, like, chop an onion, okay?

535
00:35:53,000 --> 00:35:56,000
 Like, what's the state of the onion?

536
00:35:56,000 --> 00:35:59,000
 If I want to simulate that, you know,

537
00:35:59,000 --> 00:36:01,000
 is it changing every time the knife comes down?

538
00:36:01,000 --> 00:36:04,000
 Like, what trajectory am I trying to stabilize?

539
00:36:04,000 --> 00:36:06,000
 I don't know how to think about that.

540
00:36:06,000 --> 00:36:11,000
 Controls really doesn't have a lot to give yet

541
00:36:11,000 --> 00:36:13,000
 in terms of that state representation question.

542
00:36:13,000 --> 00:36:15,000
 Learning's starting to contribute a lot,

543
00:36:15,000 --> 00:36:18,000
 and actually learning plus controls are coming together, I think,

544
00:36:18,000 --> 00:36:23,000
 to address this grand challenge of state representation for control.

545
00:36:23,000 --> 00:36:29,000
 And once those states are coming in through a camera,

546
00:36:29,000 --> 00:36:35,000
 it opens up all kinds of interesting problems, too.

547
00:36:35,000 --> 00:36:43,000
 So we're now seeing more and more feedback-based control in manipulation.

548
00:36:43,000 --> 00:36:45,000
 I think it can make a huge difference.

549
00:36:45,000 --> 00:36:48,000
 It makes a huge difference in the reliability of the demos.

550
00:36:48,000 --> 00:36:53,000
 You can see, you know, in practice, the systems just feel much more real now.

551
00:36:53,000 --> 00:36:58,000
 And the big go-ahead technology, which we'll talk a lot about when it's time,

552
00:36:58,000 --> 00:37:02,000
 is the ability to make feedback control decisions directly from the camera.

553
00:37:02,000 --> 00:37:06,000
 It used to be that we would kind of look at the camera, decide what to do,

554
00:37:06,000 --> 00:37:09,000
 make a plan, and execute.

555
00:37:09,000 --> 00:37:12,000
 Sense, plan, act is sort of the old way to think about it.

556
00:37:12,000 --> 00:37:16,000
 But there are now visual motor policies where you're actually closing the loop

557
00:37:16,000 --> 00:37:20,000
 on the camera input at high rates.

558
00:37:20,000 --> 00:37:24,000
 And that really makes the difference between a robot demo where the robot--

559
00:37:24,000 --> 00:37:28,000
 there's like robot air balls, right, where the robot does something,

560
00:37:28,000 --> 00:37:32,000
 the world changed, and it continues to do something as if the world hadn't changed.

561
00:37:32,000 --> 00:37:34,000
 And it's really embarrassing.

562
00:37:34,000 --> 00:37:37,000
 And we've had robots that fall down because they just thought the valve was there

563
00:37:37,000 --> 00:37:39,000
 and it wasn't there.

564
00:37:39,000 --> 00:37:40,000
 And that's starting to change now.

565
00:37:40,000 --> 00:37:43,000
 We're starting to be able to close the feedback loop at high rates

566
00:37:43,000 --> 00:37:48,000
 through a perception system.

567
00:37:48,000 --> 00:37:52,000
 This one's just-- I love this particular example.

568
00:37:52,000 --> 00:37:55,000
 This is just that-- like we did picking up the plate from the sink,

569
00:37:55,000 --> 00:38:02,000
 but now it's trying to do it from rich camera-based real-time feedback.

570
00:38:02,000 --> 00:38:05,000
 That's the nominal behavior.

571
00:38:05,000 --> 00:38:14,000
 But now we're trying to make it robust to all kinds of perceptual changes.

572
00:38:14,000 --> 00:38:18,000
 And the feedback there is only from the cameras.

573
00:38:18,000 --> 00:38:22,000
 But we're getting to the point where we're seeing more and more demos

574
00:38:22,000 --> 00:38:32,000
 that do what they should do in these kind of situations.

575
00:38:32,000 --> 00:38:37,000
 But underneath that, I believe-- I really believe to my core that the way you get to that

576
00:38:37,000 --> 00:38:42,000
 is by breaking that super hard problem down into simple models,

577
00:38:42,000 --> 00:38:46,000
 the same way we talk about in underactuated, for those of you that have taken it,

578
00:38:46,000 --> 00:38:51,000
 and breaking it down into the sort of-- the place where you can think rigorously

579
00:38:51,000 --> 00:38:54,000
 about what's happening in the system at all the different levels.

580
00:38:54,000 --> 00:38:59,000
 So underneath that technology are these relatively simplified models of physics

581
00:38:59,000 --> 00:39:05,000
 that we can reason about, we can practice on, we can understand.

582
00:39:05,000 --> 00:39:08,000
 Here's another fun example from TRI.

583
00:39:08,000 --> 00:39:11,000
 We're not doing dishes anymore. We're making pizzas and the like.

584
00:39:11,000 --> 00:39:15,000
 I'll show you more videos of those throughout the term, but this is just rolling dough,

585
00:39:15,000 --> 00:39:20,000
 another example of manipulation being a lot more than pick and place.

586
00:39:20,000 --> 00:39:22,000
 And again, it's using visual feedback.

587
00:39:22,000 --> 00:39:25,000
 So if someone comes and throws down some more dough or whatever,

588
00:39:25,000 --> 00:39:29,000
 these systems are now getting more and more robust to real-time visual feedback,

589
00:39:29,000 --> 00:39:31,000
 changing the task.

590
00:39:31,000 --> 00:39:36,000
 And this is a case where I don't really know what the state of the dough is,

591
00:39:36,000 --> 00:39:41,000
 but I've still got to come up with a good controller that'll do the task.

592
00:39:41,000 --> 00:39:43,000
 And these are the problems of the day.

593
00:39:43,000 --> 00:39:51,000
 And these are the problems that I think control has to grow to address.

594
00:39:51,000 --> 00:40:01,000
 On the -- oops, I put it in -- oh, where did I --

595
00:40:01,000 --> 00:40:04,000
 okay, well, I have to find the other video.

596
00:40:04,000 --> 00:40:09,000
 But there's a success video before the interesting failure cases.

597
00:40:09,000 --> 00:40:11,000
 [laughter]

598
00:40:11,000 --> 00:40:17,000
 There's a robot also at Toyota where I've got the best videos from Toyota.

599
00:40:17,000 --> 00:40:19,000
 They built this incredible robot.

600
00:40:19,000 --> 00:40:22,000
 It's called the TTT robot.

601
00:40:22,000 --> 00:40:26,000
 And the task here is to go into a real grocery store, not just some --

602
00:40:26,000 --> 00:40:29,000
 we have a mock grocery store at Toyota,

603
00:40:29,000 --> 00:40:33,000
 but we also have a real grocery store down the street that we're collaborating with.

604
00:40:33,000 --> 00:40:40,000
 And the task is not easy, obviously, to pick up all these objects and be robust.

605
00:40:40,000 --> 00:40:44,000
 And actually, I'm very proud that they even let me show this video,

606
00:40:44,000 --> 00:40:48,000
 because they think very seriously about the failure cases,

607
00:40:48,000 --> 00:40:50,000
 and they just say, "This is a hard problem.

608
00:40:50,000 --> 00:40:52,000
 We're going to measure how often we fail,

609
00:40:52,000 --> 00:40:55,000
 and we're going to make it better and better and better the same way."

610
00:40:55,000 --> 00:41:00,000
 But this robot, in the success video, the task is,

611
00:41:00,000 --> 00:41:03,000
 wake up, you're in this grocery store that you've seen before,

612
00:41:03,000 --> 00:41:08,000
 but you're going to be told some number of items that you've never --

613
00:41:08,000 --> 00:41:10,000
 just from a list of hundreds of items,

614
00:41:10,000 --> 00:41:13,000
 you're going to pick these items and put them in my grocery basket.

615
00:41:13,000 --> 00:41:17,000
 And it drives through the store, and with an increasingly high success rate,

616
00:41:17,000 --> 00:41:21,000
 is able to sort of go through and understand and find the objects

617
00:41:21,000 --> 00:41:23,000
 and load a grocery basket.

618
00:41:23,000 --> 00:41:25,000
 This kind of stuff's coming.

619
00:41:25,000 --> 00:41:29,000
 Now, that doesn't -- apart from the complicated failure cases I just showed,

620
00:41:29,000 --> 00:41:33,000
 that doesn't stress as much the dynamics of a dexterous hand,

621
00:41:33,000 --> 00:41:37,000
 but the perceptual understanding of the world is really hard in this case.

622
00:41:37,000 --> 00:41:41,000
 And some of the failure cases where you thought you could pull the object out,

623
00:41:41,000 --> 00:41:44,000
 but it was actually in a box, and the whole box tips out, right?

624
00:41:44,000 --> 00:41:51,000
 These are really hard cases for a perception system to understand.

625
00:41:51,000 --> 00:41:55,000
 Okay. So that's kind of the motivation at a high level.

626
00:41:55,000 --> 00:41:58,000
 Those are the kind of things we want to cover.

627
00:41:58,000 --> 00:42:01,000
 And now I want to tell you how we're going to cover them,

628
00:42:01,000 --> 00:42:04,000
 and tell you a little bit about the sort of breakdown of the system

629
00:42:04,000 --> 00:42:08,000
 and the style, the way we're going to try to connect

630
00:42:08,000 --> 00:42:12,000
 the pieces of those manipulation systems to dynamics and control,

631
00:42:12,000 --> 00:42:17,000
 kind of dynamical systems.

632
00:42:17,000 --> 00:42:23,000
 And let me start by just saying that the anatomy of most manipulation systems

633
00:42:23,000 --> 00:42:26,000
 these days has ROS as a big part of it.

634
00:42:26,000 --> 00:42:30,000
 How many people have used ROS, or know what ROS is even?

635
00:42:30,000 --> 00:42:32,000
 I'm happy to -- yeah?

636
00:42:32,000 --> 00:42:34,000
 Okay. ROS is the robot operating system.

637
00:42:34,000 --> 00:42:39,000
 It's probably one of the best things that happened to robotics,

638
00:42:39,000 --> 00:42:43,000
 you know, a decade ago at this point.

639
00:42:43,000 --> 00:42:47,000
 And it's an ecosystem.

640
00:42:47,000 --> 00:42:51,000
 It's not an operating system in the Windows or Linux sense,

641
00:42:51,000 --> 00:42:54,000
 but it's an operating system in the -- it's an ecosystem

642
00:42:54,000 --> 00:42:57,000
 where people are contributing different modules,

643
00:42:57,000 --> 00:43:02,000
 perception systems, planning systems, simulators, for instance.

644
00:43:02,000 --> 00:43:05,000
 And ROS makes it easy to connect them together.

645
00:43:05,000 --> 00:43:07,000
 So those of you that raised your hand know this,

646
00:43:07,000 --> 00:43:10,000
 but let me just say a few things about it as a launching point

647
00:43:10,000 --> 00:43:12,000
 to the way we're going to think about things.

648
00:43:12,000 --> 00:43:17,000
 So we said that this is okay.

649
00:43:17,000 --> 00:43:21,000
 You guys can all see that even if I'm -- okay, great.

650
00:43:21,000 --> 00:43:27,000
 So in ROS, if I have a perception system,

651
00:43:27,000 --> 00:43:35,000
 I can build components in sort of a modular approach.

652
00:43:35,000 --> 00:43:42,000
 Okay, so maybe I have -- I start off, I have a camera driver.

653
00:43:42,000 --> 00:43:45,000
 Okay, and someone needs to write a camera driver,

654
00:43:45,000 --> 00:43:47,000
 and that takes a bunch of work, you know,

655
00:43:47,000 --> 00:43:49,000
 especially as cameras change or whatever.

656
00:43:49,000 --> 00:43:51,000
 I've got some camera driver that has to talk to firmware

657
00:43:51,000 --> 00:43:55,000
 and publish out an image, let's say.

658
00:43:55,000 --> 00:44:01,000
 Maybe a red, green, blue image, for instance, coming out.

659
00:44:01,000 --> 00:44:03,000
 Okay, there's another big chunk of work,

660
00:44:03,000 --> 00:44:13,000
 which is to come up with a perception system.

661
00:44:13,000 --> 00:44:18,000
 And maybe that takes RGB inputs in and outputs,

662
00:44:18,000 --> 00:44:29,000
 in the simple case, let's say, the position of my red brick.

663
00:44:29,000 --> 00:44:31,000
 Right, in the onion, it's a much harder question,

664
00:44:31,000 --> 00:44:33,000
 but in the red brick, I could just tell you where the red brick is,

665
00:44:33,000 --> 00:44:35,000
 and that's pretty good.

666
00:44:35,000 --> 00:44:41,000
 Okay, someone else needs to write a planning system, let's say.

667
00:44:41,000 --> 00:44:44,000
 Maybe two, maybe there's a high-level planning and a low-level planning,

668
00:44:44,000 --> 00:44:48,000
 let's say there's some sort of planning system that takes,

669
00:44:48,000 --> 00:44:51,000
 let's say, the positions of the brick and the positions of the robot,

670
00:44:51,000 --> 00:45:01,000
 for instance, and starts putting out a joint trajectories.

671
00:45:01,000 --> 00:45:09,000
 Okay, and then we've got some low-level controller

672
00:45:09,000 --> 00:45:12,000
 that thinks about maybe the dynamics of the arm

673
00:45:12,000 --> 00:45:16,000
 and tries to realize these joint trajectories,

674
00:45:16,000 --> 00:45:25,000
 and maybe it has to send low-level motor commands.

675
00:45:25,000 --> 00:45:34,000
 Okay, and at the other end of this, I've got a motor driver.

676
00:45:34,000 --> 00:45:37,000
 Okay, and every one of those is a research, you know,

677
00:45:37,000 --> 00:45:39,000
 maybe this one and this one are his research projects, see,

678
00:45:39,000 --> 00:45:44,000
 but certainly all of these are massive research challenges,

679
00:45:44,000 --> 00:45:52,000
 and traditionally it was very hard for one research group to do all of them well.

680
00:45:52,000 --> 00:46:00,000
 And the big thing that happened with ROS was it became a standard for sharing components,

681
00:46:00,000 --> 00:46:04,000
 where maybe I could use a perception system from Carnegie Mellon

682
00:46:04,000 --> 00:46:08,000
 and maybe a controller from DLR in Germany,

683
00:46:08,000 --> 00:46:13,000
 and maybe I'll focus my research on the planning system.

684
00:46:13,000 --> 00:46:30,000
 And the way it works is it's based on message-passing network interfaces,

685
00:46:30,000 --> 00:46:35,000
 and it's multiprocess.

686
00:46:35,000 --> 00:46:41,000
 Okay, so basically someone can write a program here that does camera drivers,

687
00:46:41,000 --> 00:46:47,000
 and they will just publish on a network, on an Ethernet for instance,

688
00:46:47,000 --> 00:46:53,000
 a particular type of message that contains the data for an RGB image.

689
00:46:53,000 --> 00:46:57,000
 And all that we have to agree on, if I'm going to use your camera driver,

690
00:46:57,000 --> 00:47:01,000
 is the format of that network message.

691
00:47:01,000 --> 00:47:07,000
 And then I'll write a perception system, and maybe I'll agree to use your RGB image network packet format,

692
00:47:07,000 --> 00:47:12,000
 and I'm going to try to produce a position format that everybody agrees on.

693
00:47:12,000 --> 00:47:18,000
 And if we just agree on a few of the common message types, and that's it,

694
00:47:18,000 --> 00:47:24,000
 and let everybody write their own individual executables,

695
00:47:24,000 --> 00:47:30,000
 this was the go-ahead idea that made people really start being able to share their code.

696
00:47:30,000 --> 00:47:36,000
 And it's subtle, actually. I don't know how many people do a lot of software engineering,

697
00:47:36,000 --> 00:47:40,000
 but it's for somehow subtle reasons.

698
00:47:40,000 --> 00:47:44,000
 Even compiling someone else's code on your machine,

699
00:47:44,000 --> 00:47:49,000
 and having the right version of the dependency libraries all work together,

700
00:47:49,000 --> 00:47:58,000
 can be a real roadblock to trying to get some code from CMU or something to run on my robot.

701
00:47:58,000 --> 00:48:05,000
 By separating out the concerns of compilation and making this executable-level decomposition of the task,

702
00:48:05,000 --> 00:48:09,000
 and only agreeing on the message type, where it's easy, everybody can,

703
00:48:09,000 --> 00:48:11,000
 whether you're using different programming languages, someone could write in Python,

704
00:48:11,000 --> 00:48:13,000
 someone could write in C++, someone could whatever,

705
00:48:13,000 --> 00:48:19,000
 all we have to agree on is the packet protocol on the network.

706
00:48:19,000 --> 00:48:23,000
 With things like Docker, people don't even have to agree on the operating system.

707
00:48:23,000 --> 00:48:28,000
 People will run a perception system on a Docker container on Ubuntu 14,

708
00:48:28,000 --> 00:48:31,000
 and I can still use that on my Mac.

709
00:48:31,000 --> 00:48:35,000
 These kind of things caused a level of modularity and abstraction

710
00:48:35,000 --> 00:48:41,000
 that got roboticists to finally start sharing their code, and really using each other's code.

711
00:48:41,000 --> 00:48:49,000
 A new lab could start up a serious robotics project by picking the best components here,

712
00:48:49,000 --> 00:48:53,000
 they'd get a system that would actually run and do some interesting things,

713
00:48:53,000 --> 00:48:57,000
 and then drill down and start to work on the different components.

714
00:48:57,000 --> 00:49:00,000
 That was a major good thing that happened in robotics.

715
00:49:00,000 --> 00:49:04,000
 And just even, it started the culture of open sourcing your code.

716
00:49:04,000 --> 00:49:09,000
 That wasn't a big culture beforehand.

717
00:49:09,000 --> 00:49:14,000
 But, we're not going to use ROS in the class.

718
00:49:14,000 --> 00:49:20,000
 This is the starting place, but it doesn't serve my pedagogical goals.

719
00:49:20,000 --> 00:49:28,000
 You can use ROS if you want to, but I think the connections here

720
00:49:28,000 --> 00:49:33,000
 of only talking about what are the message types is a little too weak

721
00:49:33,000 --> 00:49:38,000
 in order to really try to take, I think even a lot of companies struggle with it.

722
00:49:38,000 --> 00:49:42,000
 It's very good for getting an initial system off the ground,

723
00:49:42,000 --> 00:49:47,000
 but then once you start trying to take a system to really high levels of reliability,

724
00:49:47,000 --> 00:49:52,000
 the semantics of how these systems talk together gets much more subtle.

725
00:49:52,000 --> 00:49:57,000
 And just promising that I'm going to publish at some, whenever I want to publish,

726
00:49:57,000 --> 00:50:00,000
 for instance, some position of the brick, that might not be good enough.

727
00:50:00,000 --> 00:50:05,000
 And so, I think the field is on the path to higher levels of maturity.

728
00:50:05,000 --> 00:50:11,000
 ROS is growing in this direction too, of trying to do a little bit more reasoning

729
00:50:11,000 --> 00:50:14,000
 about not only the way to write these systems individually,

730
00:50:14,000 --> 00:50:17,000
 but the way to connect them together.

731
00:50:17,000 --> 00:50:22,000
 So, let me tell you about it from the perspective of dynamical systems and control.

732
00:50:22,000 --> 00:50:24,000
 The way a control theorist might have started this,

733
00:50:24,000 --> 00:50:28,000
 they would have drawn a very similar diagram, a block diagram.

734
00:50:28,000 --> 00:50:32,000
 So, maybe it starts, I'll start with the robot,

735
00:50:32,000 --> 00:50:36,000
 because that would be more standard in controls.

736
00:50:36,000 --> 00:50:40,000
 Maybe this is a simulation, for instance.

737
00:50:40,000 --> 00:50:49,000
 Okay, and I'm taking motor commands in.

738
00:50:49,000 --> 00:50:55,000
 And having some sensors come out, some sensor signals come out.

739
00:50:55,000 --> 00:50:58,000
 Now, this is something that control theorists have been doing forever, right?

740
00:50:58,000 --> 00:51:03,000
 Maybe not with onions, right? Or laundry or something.

741
00:51:03,000 --> 00:51:08,000
 But certainly for aircraft and for chemical plants and for all kinds of rich systems,

742
00:51:08,000 --> 00:51:11,000
 control theory has been incredibly successful.

743
00:51:11,000 --> 00:51:15,000
 And they have a modeling abstraction, a hierarchy, a modularity approach

744
00:51:15,000 --> 00:51:21,000
 that's very similar in the pictures I've drawn, but it's different in the details.

745
00:51:21,000 --> 00:51:25,000
 So, I'll still think of this as a block diagram,

746
00:51:25,000 --> 00:51:30,000
 but I'm going to be specific about the details inside here.

747
00:51:30,000 --> 00:51:45,000
 I typically will represent this as a dynamical system.

748
00:51:45,000 --> 00:51:59,000
 So, I'll write it in a generic way today, and it'll even be okay for a while.

749
00:51:59,000 --> 00:52:07,000
 So, this is a difference equation,

750
00:52:07,000 --> 00:52:16,000
 where X is used in a control sense to represent the state of the system,

751
00:52:16,000 --> 00:52:22,000
 which maybe in the case of my robot would be the positions

752
00:52:22,000 --> 00:52:36,000
 and velocities of the robot plus the brick.

753
00:52:36,000 --> 00:52:46,000
 U is my command inputs, so my motor commands coming in.

754
00:52:46,000 --> 00:52:56,000
 X is my next state. Xn+1 is my next state.

755
00:52:56,000 --> 00:53:02,000
 And so, in this setting, F has to be somehow my physics model, right?

756
00:53:02,000 --> 00:53:13,000
 My physics engine here.

757
00:53:13,000 --> 00:53:20,000
 It's somehow connecting equations of motion

758
00:53:20,000 --> 00:53:24,000
 that look like force equals mass times acceleration

759
00:53:24,000 --> 00:53:31,000
 with these notions of state and the notion of next state.

760
00:53:31,000 --> 00:53:35,000
 Acceleration is a continuous time idea, derivatives, right?

761
00:53:35,000 --> 00:53:39,000
 And somehow I've talked about a discrete jump from one state to the next,

762
00:53:39,000 --> 00:53:45,000
 and I'll talk about how to make those jumps.

763
00:53:45,000 --> 00:53:52,000
 Now, these equations might be familiar to you in simpler forms, right?

764
00:53:52,000 --> 00:54:03,000
 If you took 18.03 here, or a differential equations course,

765
00:54:03,000 --> 00:54:10,000
 then you would have seen them first as, let's say, a linear set of difference equations.

766
00:54:10,000 --> 00:54:14,000
 You might have seen it that looked like this.

767
00:54:14,000 --> 00:54:24,000
 Or the matrix form of that might be where X is a vector, right?

768
00:54:24,000 --> 00:54:36,000
 This would be a linear difference equation.

769
00:54:36,000 --> 00:54:44,000
 If you took an intro controls course, you would have seen

770
00:54:44,000 --> 00:54:52,000
 something that looked like this, the state space form of a linear difference equation.

771
00:54:52,000 --> 00:54:55,000
 OK, from controls, which would be just the--

772
00:54:55,000 --> 00:54:57,000
 it's now a control difference equation.

773
00:54:57,000 --> 00:55:07,000
 This would be a linear control difference equation.

774
00:55:07,000 --> 00:55:13,000
 OK, now I didn't require 18.03 or DIPI-Q or linear intro controls

775
00:55:13,000 --> 00:55:15,000
 as a prerequisite of the course.

776
00:55:15,000 --> 00:55:17,000
 If you were to take those courses when you took those--

777
00:55:17,000 --> 00:55:20,000
 many of you have taken 18.03 at least, right?

778
00:55:20,000 --> 00:55:23,000
 You would have been able to go start from those equations

779
00:55:23,000 --> 00:55:25,000
 and thought a lot about the time evolution of these.

780
00:55:25,000 --> 00:55:28,000
 You could solve the differential equation given initial conditions.

781
00:55:28,000 --> 00:55:30,000
 You could talk about its stability properties.

782
00:55:30,000 --> 00:55:32,000
 There's lots of things you could potentially do.

783
00:55:32,000 --> 00:55:36,000
 And we don't need all that right away for this class.

784
00:55:36,000 --> 00:55:42,000
 OK, so we're not going to use, let's say, the deeper content from 18.03,

785
00:55:42,000 --> 00:55:45,000
 but we are going to definitely use the modeling language.

786
00:55:45,000 --> 00:55:51,000
 OK, and you should see this f of x, u

787
00:55:51,000 --> 00:55:54,000
 as just a nonlinear generalization of these equations

788
00:55:54,000 --> 00:55:56,000
 that you would have seen in those classes.

789
00:55:56,000 --> 00:56:01,000
 OK, because f is complicated, it's now a physics engine,

790
00:56:01,000 --> 00:56:03,000
 it becomes harder to do the closed-form analysis

791
00:56:03,000 --> 00:56:05,000
 that you did in the intro classes.

792
00:56:05,000 --> 00:56:08,000
 OK, but we're still going to benefit a lot

793
00:56:08,000 --> 00:56:32,000
 by writing it down in this dynamical systems language.

794
00:56:32,000 --> 00:56:37,000
 OK, so we're going to talk a lot about having our block diagram of the system

795
00:56:37,000 --> 00:56:40,000
 and using equations of this form.

796
00:56:40,000 --> 00:56:43,000
 This is not quite enough.

797
00:56:43,000 --> 00:56:47,000
 OK, we also need to model the sensors.

798
00:56:47,000 --> 00:56:51,000
 So the sensors we'll typically use in the language of dynamical systems

799
00:56:51,000 --> 00:56:55,000
 as an output of that function, of that state.

800
00:56:55,000 --> 00:56:58,000
 It could be a function of x and u in general,

801
00:56:58,000 --> 00:57:04,000
 and y is now the outputs at time n.

802
00:57:04,000 --> 00:57:10,000
 OK, and in the case of my sensor being an RGB camera,

803
00:57:10,000 --> 00:57:13,000
 f might have been my physics engine,

804
00:57:13,000 --> 00:57:18,000
 but g is going to be my game engine quality renderer.

805
00:57:18,000 --> 00:57:22,000
 If I have to go from the positions of the robot and the brick

806
00:57:22,000 --> 00:57:27,000
 over into an RGB image, then g, I can write it as a function,

807
00:57:27,000 --> 00:57:30,000
 but down in the details, that's rendering.

808
00:57:30,000 --> 00:57:34,000
 So these get to be very complicated functions.

809
00:57:34,000 --> 00:57:38,000
 But what I hope to convince you over the course of the term, really,

810
00:57:38,000 --> 00:57:41,000
 is that by thinking about it through these equations,

811
00:57:41,000 --> 00:57:45,000
 it's going to ask you a little bit more than Ross does.

812
00:57:45,000 --> 00:57:50,000
 I want you, in particular, to tell me what the state is.

813
00:57:50,000 --> 00:57:53,000
 I want you to tell me what the timing semantics are.

814
00:57:53,000 --> 00:57:57,000
 How do I go from n to n plus 1 if my camera is running at 30 hertz

815
00:57:57,000 --> 00:58:00,000
 and my robot simulation is running at 100 hertz

816
00:58:00,000 --> 00:58:05,000
 and I've got events based on some other sensor that's doing some strange things?

817
00:58:05,000 --> 00:58:12,000
 I need a modeling language for talking about how those parts interact.

818
00:58:12,000 --> 00:58:13,000
 And that's going to ask more.

819
00:58:13,000 --> 00:58:16,000
 In fact, it's going to feel annoying when you start writing these systems,

820
00:58:16,000 --> 00:58:19,000
 and I'm not just going to say, "Give me a function."

821
00:58:19,000 --> 00:58:21,000
 I'm going to say, "What's the state variables?"

822
00:58:21,000 --> 00:58:24,000
 "What's the randomness?" You have to declare the randomness.

823
00:58:24,000 --> 00:58:27,000
 There's a couple things you have to declare.

824
00:58:27,000 --> 00:58:33,000
 But the advantage over the Ross, very light touch here,

825
00:58:33,000 --> 00:58:34,000
 for the purposes of the class,

826
00:58:34,000 --> 00:58:39,000
 is that you get to do more sophisticated things with the models.

827
00:58:39,000 --> 00:58:43,000
 If I only know that there's arbitrary executables behind the box

828
00:58:43,000 --> 00:58:44,000
 and they send messages out,

829
00:58:44,000 --> 00:58:48,000
 then there's limits to what I can say about what they do when they're connected.

830
00:58:48,000 --> 00:58:52,000
 If I know that these systems are deterministic functions

831
00:58:52,000 --> 00:58:55,000
 once you tell me the state,

832
00:58:55,000 --> 00:58:59,000
 then, for instance, I can run exactly the same simulation twice.

833
00:58:59,000 --> 00:59:04,000
 Anytime I just put the state in, I run the same controls through,

834
00:59:04,000 --> 00:59:06,000
 and I'll get the same outputs back out.

835
00:59:06,000 --> 00:59:09,000
 Deterministic simulation.

836
00:59:09,000 --> 00:59:12,000
 It sounds crazy to me, even for me to say that right there.

837
00:59:12,000 --> 00:59:16,000
 It sounds crazy, but most people in robotics

838
00:59:16,000 --> 00:59:19,000
 can't run the same experiment twice, even in simulation.

839
00:59:19,000 --> 00:59:20,000
 It's just a weird thing.

840
00:59:20,000 --> 00:59:23,000
 But everybody's got different processes running on different clocks

841
00:59:23,000 --> 00:59:25,000
 and sending messages when they want to send,

842
00:59:25,000 --> 00:59:28,000
 and nobody wrote down a specific contract saying,

843
00:59:28,000 --> 00:59:32,000
 "You must send at a certain rate. Messages must arrive in a certain order."

844
00:59:32,000 --> 00:59:34,000
 So if you see a bug in your simulator,

845
00:59:34,000 --> 00:59:36,000
 you see the robot fell down in some weird way

846
00:59:36,000 --> 00:59:38,000
 or threw a brick across the room,

847
00:59:38,000 --> 00:59:40,000
 and you say, "I'm going to reproduce that,"

848
00:59:40,000 --> 00:59:41,000
 and you run it again,

849
00:59:41,000 --> 00:59:44,000
 then your perception system might have sent a message

850
00:59:44,000 --> 00:59:46,000
 just a little bit before or different.

851
00:59:46,000 --> 00:59:50,000
 It's very hard to get a deterministic, repeatable simulation

852
00:59:50,000 --> 00:59:55,000
 out of a generic ecosystem like this.

853
00:59:55,000 --> 00:59:59,000
 But if you ask every one of the individual systems

854
00:59:59,000 --> 01:00:01,000
 to declare its state,

855
01:00:01,000 --> 01:00:03,000
 if you ask all of them to be deterministic

856
01:00:03,000 --> 01:00:06,000
 or declare its randomness,

857
01:00:06,000 --> 01:00:10,000
 then you get an extra power when you start combining them

858
01:00:10,000 --> 01:00:13,000
 and knowing how things are going to work.

859
01:00:13,000 --> 01:00:15,000
 So at the very least,

860
01:00:15,000 --> 01:00:17,000
 when you have a bug in your final project,

861
01:00:17,000 --> 01:00:20,000
 we'll be able to help.

862
01:00:20,000 --> 01:00:23,000
 And in general, we're going to try to keep things

863
01:00:23,000 --> 01:00:26,000
 running in a single process instead of multi-process.

864
01:00:26,000 --> 01:00:31,000
 I'm going to try to emphasize the interesting parts of the components,

865
01:00:31,000 --> 01:00:35,000
 and hopefully, if you do a little bit of work when you declare them,

866
01:00:35,000 --> 01:00:37,000
 then the details of multiple message passing and all that stuff

867
01:00:37,000 --> 01:00:41,000
 will just disappear, and you don't have to worry about it.

868
01:00:41,000 --> 01:00:43,000
 Okay?

869
01:00:43,000 --> 01:00:46,000
 So certainly, robot simulations,

870
01:00:46,000 --> 01:00:49,000
 if I think of F as a physics engine and G,

871
01:00:49,000 --> 01:00:51,000
 you can sort of imagine X as the state,

872
01:00:51,000 --> 01:00:53,000
 U as the positions and velocities,

873
01:00:53,000 --> 01:00:56,000
 U as the motor commands,

874
01:00:56,000 --> 01:01:00,000
 Y might be my camera image or my joint sensors.

875
01:01:00,000 --> 01:01:05,000
 But actually, I would argue that all of the systems

876
01:01:05,000 --> 01:01:07,000
 in our hierarchy

877
01:01:07,000 --> 01:01:11,000
 can be described nicely with those same sets of equations.

878
01:01:11,000 --> 01:01:13,000
 Okay?

879
01:01:13,000 --> 01:01:17,000
 So let's think about a perception system for a second.

880
01:01:17,000 --> 01:01:26,000
 Okay, so a modern perception system,

881
01:01:26,000 --> 01:01:29,000
 maybe it takes in an RGB image.

882
01:01:35,000 --> 01:01:38,000
 These days, let's say it goes through a deep network,

883
01:01:38,000 --> 01:01:44,000
 and it outputs the position of the brick.

884
01:01:44,000 --> 01:01:55,000
 Now, a lot of deep learning-based perception systems

885
01:01:55,000 --> 01:01:57,000
 really just look, in this case,

886
01:01:57,000 --> 01:02:02,000
 if the position of the brick is Y and the RGB image is U,

887
01:02:02,000 --> 01:02:05,000
 they can be modeled just as a static function.

888
01:02:05,000 --> 01:02:09,000
 Y of N is G of U of N.

889
01:02:09,000 --> 01:02:12,000
 So it certainly fits into the dynamical systems framework,

890
01:02:12,000 --> 01:02:15,000
 but maybe doesn't exercise the dynamical systems framework

891
01:02:15,000 --> 01:02:17,000
 because the state is empty.

892
01:02:17,000 --> 01:02:19,000
 There's no state.

893
01:02:19,000 --> 01:02:21,000
 Okay?

894
01:02:21,000 --> 01:02:24,000
 But that's not how we used to build perception systems.

895
01:02:24,000 --> 01:02:25,000
 Right?

896
01:02:25,000 --> 01:02:28,000
 If you've taken a class on state estimation

897
01:02:28,000 --> 01:02:30,000
 or if you've heard the terms,

898
01:02:30,000 --> 01:02:32,000
 like a Kalman filter, for instance, right?

899
01:02:32,000 --> 01:02:36,000
 A Kalman filter will take observations in,

900
01:02:36,000 --> 01:02:45,000
 and it keeps an internal estimate

901
01:02:45,000 --> 01:02:47,000
 of what's the state of the world.

902
01:02:47,000 --> 01:02:48,000
 Okay?

903
01:02:48,000 --> 01:02:51,000
 So it's got a state-space form,

904
01:02:51,000 --> 01:02:54,000
 and will output the estimated state.

905
01:02:59,000 --> 01:03:02,000
 This fits squarely into the--

906
01:03:02,000 --> 01:03:04,000
 even if it's an extended Kalman filter,

907
01:03:04,000 --> 01:03:09,000
 it fits squarely into the modeling paradigm.

908
01:03:09,000 --> 01:03:23,000
 And if you've worked with Kalman filters,

909
01:03:23,000 --> 01:03:25,000
 I mean, you might-- if you've done a--

910
01:03:25,000 --> 01:03:28,000
 you know, a summer at a autonomous driving startup

911
01:03:28,000 --> 01:03:30,000
 or anything like this, or, you know,

912
01:03:30,000 --> 01:03:32,000
 autonomous driving company, for instance,

913
01:03:32,000 --> 01:03:34,000
 you've probably come across Kalman filters.

914
01:03:34,000 --> 01:03:35,000
 Okay?

915
01:03:35,000 --> 01:03:37,000
 If you think about perception in this way,

916
01:03:37,000 --> 01:03:39,000
 where the goal of a perception system

917
01:03:39,000 --> 01:03:41,000
 is to summarize all of the things it's seen,

918
01:03:41,000 --> 01:03:43,000
 maybe in the recent history of the world,

919
01:03:43,000 --> 01:03:45,000
 into some coherent understanding

920
01:03:45,000 --> 01:03:47,000
 of what's happening in the world,

921
01:03:47,000 --> 01:03:49,000
 that's very different than what we see

922
01:03:49,000 --> 01:03:51,000
 when you go from a single RGB image

923
01:03:51,000 --> 01:03:54,000
 out to an estimate of the world.

924
01:03:54,000 --> 01:03:55,000
 Okay?

925
01:03:55,000 --> 01:03:57,000
 And it was actually pretty weird for a bunch of us, right,

926
01:03:57,000 --> 01:03:59,000
 when deep learning started to work really well.

927
01:03:59,000 --> 01:04:02,000
 Everybody was talking about few-shot--

928
01:04:02,000 --> 01:04:05,000
 or one-shot learning, right, or zero-shot, right?

929
01:04:05,000 --> 01:04:09,000
 And, you know, people were like, "That's crazy.

930
01:04:09,000 --> 01:04:11,000
 "Why would you not use multiple images?"

931
01:04:11,000 --> 01:04:12,000
 Or, like, "Why would you not remember

932
01:04:12,000 --> 01:04:13,000
 "what you've seen in the past

933
01:04:13,000 --> 01:04:15,000
 "when you're making your prediction?"

934
01:04:15,000 --> 01:04:19,000
 And, indeed, I mean, the deep learning perception systems

935
01:04:19,000 --> 01:04:22,000
 work incredibly well, even from a single image,

936
01:04:22,000 --> 01:04:25,000
 but the modern, you know, deep learning perception systems

937
01:04:25,000 --> 01:04:28,000
 are--actually look a little bit more like this,

938
01:04:28,000 --> 01:04:30,000
 where they'll have a recurrent neural network in the middle

939
01:04:30,000 --> 01:04:33,000
 or a visual transformer in the middle, okay?

940
01:04:33,000 --> 01:04:36,000
 And those, again, the recurrent--

941
01:04:36,000 --> 01:04:38,000
 the state of the recurrent network

942
01:04:38,000 --> 01:04:41,000
 is gonna be--can be declared as a state

943
01:04:41,000 --> 01:04:43,000
 in my dynamical systems framework.

944
01:04:43,000 --> 01:04:46,000
 And transformers are a little harder to think about that way,

945
01:04:46,000 --> 01:04:49,000
 but they totally fit in these frameworks, okay?

946
01:04:49,000 --> 01:04:51,000
 'Cause, really, the goal of perception

947
01:04:51,000 --> 01:04:54,000
 should be to accumulate information over time, right?

948
01:04:54,000 --> 01:04:57,000
 Certainly, the way I perceive the world

949
01:04:57,000 --> 01:04:59,000
 is not, you know, take an image

950
01:04:59,000 --> 01:05:01,000
 and then, you know, understand everything about it, right?

951
01:05:01,000 --> 01:05:05,000
 I'm accumulating information as I move through the world

952
01:05:05,000 --> 01:05:08,000
 and summarizing it in some belief, okay?

953
01:05:08,000 --> 01:05:11,000
 And so these kind of perception systems

954
01:05:11,000 --> 01:05:15,000
 fit beautifully into the dynamical systems framework.

955
01:05:15,000 --> 01:05:16,000
 If you go to think of--

956
01:05:16,000 --> 01:05:18,000
 I mean, control absolutely fits right into that.

957
01:05:18,000 --> 01:05:20,000
 The robot controllers, if you want to write

958
01:05:20,000 --> 01:05:22,000
 an impedance controller

959
01:05:22,000 --> 01:05:24,000
 or some inverse dynamics controller,

960
01:05:24,000 --> 01:05:27,000
 that absolutely snaps right into this framework.

961
01:05:27,000 --> 01:05:30,000
 It came from that world, okay?

962
01:05:30,000 --> 01:05:33,000
 Planning systems are more interesting,

963
01:05:33,000 --> 01:05:35,000
 and we'll talk about 'em when the time comes,

964
01:05:35,000 --> 01:05:36,000
 but, you know, a lot of times,

965
01:05:36,000 --> 01:05:40,000
 if I'm writing a planning system in a Ross ecosystem,

966
01:05:40,000 --> 01:05:43,000
 I'll, you know, listen to the perception system,

967
01:05:43,000 --> 01:05:45,000
 and then I'll, like, go think for a little while

968
01:05:45,000 --> 01:05:47,000
 and make no commitments whatsoever

969
01:05:47,000 --> 01:05:49,000
 about when I'm gonna return an answer, you know,

970
01:05:49,000 --> 01:05:51,000
 or if I will return an answer, actually,

971
01:05:51,000 --> 01:05:53,000
 and eventually say, "Oh, you should do this," right?

972
01:05:53,000 --> 01:05:55,000
 And, you know, the timing semantics

973
01:05:55,000 --> 01:05:56,000
 around a planning system,

974
01:05:56,000 --> 01:06:00,000
 when these are long-running, you know, planners, potentially,

975
01:06:00,000 --> 01:06:02,000
 can be very stochastic.

976
01:06:02,000 --> 01:06:05,000
 It can be a big source of either conservatism--

977
01:06:05,000 --> 01:06:07,000
 if you have to wait for your planner

978
01:06:07,000 --> 01:06:08,000
 to come up with an answer,

979
01:06:08,000 --> 01:06:10,000
 that's why the robots will, you know,

980
01:06:10,000 --> 01:06:12,000
 do something interesting and then wait for a little while

981
01:06:12,000 --> 01:06:13,000
 and then do something interesting

982
01:06:13,000 --> 01:06:14,000
 and wait for a little while--

983
01:06:14,000 --> 01:06:17,000
 or if you're trying to keep that system completely moving,

984
01:06:17,000 --> 01:06:19,000
 then the semantics of when this planning system

985
01:06:19,000 --> 01:06:22,000
 reports its answers gets pretty subtle, okay?

986
01:06:22,000 --> 01:06:24,000
 But when we get into the details,

987
01:06:24,000 --> 01:06:26,000
 that still fits into these dynamical systems.

988
01:06:26,000 --> 01:06:27,000
 You can still model that

989
01:06:27,000 --> 01:06:29,000
 in the language of dynamical systems.

990
01:06:29,000 --> 01:06:32,000
 Okay, so what we're gonna try to do in the class

991
01:06:32,000 --> 01:06:34,000
 is very much keep the good things

992
01:06:34,000 --> 01:06:36,000
 about the modular architectures,

993
01:06:36,000 --> 01:06:40,000
 but also try to declare our state variables,

994
01:06:40,000 --> 01:06:44,000
 our randomness, our inputs and outputs, okay?

995
01:06:44,000 --> 01:06:45,000
 And then you'll be able to compose

996
01:06:45,000 --> 01:06:48,000
 these modular components into a big system,

997
01:06:48,000 --> 01:06:49,000
 you know, get repeatable simulations.

998
01:06:49,000 --> 01:06:53,000
 And if you want, you can do advanced control analysis,

999
01:06:53,000 --> 01:06:57,000
 verification, you can do, you know, Monte Carlo analysis,

1000
01:06:57,000 --> 01:06:59,000
 but you can also try to prove

1001
01:06:59,000 --> 01:07:01,000
 that a system's gonna converge, you know,

1002
01:07:01,000 --> 01:07:03,000
 exponentially to some equilibrium,

1003
01:07:03,000 --> 01:07:05,000
 even if it's got some really complicated components

1004
01:07:05,000 --> 01:07:07,000
 in the way.

1005
01:07:07,000 --> 01:07:09,000
 Okay, so that's been a bit of my, you know--

1006
01:07:09,000 --> 01:07:12,000
 I would say not everybody believes this.

1007
01:07:12,000 --> 01:07:14,000
 This is my personal, you know, belief,

1008
01:07:14,000 --> 01:07:16,000
 my taste, I guess, if you will.

1009
01:07:16,000 --> 01:07:20,000
 I think some people see the complexity of manipulation,

1010
01:07:20,000 --> 01:07:22,000
 the complexity of all these components,

1011
01:07:22,000 --> 01:07:28,000
 and say, "It's so complex that you can't be rigorous," right?

1012
01:07:28,000 --> 01:07:31,000
 And I'm saying instead, "It's so complex,

1013
01:07:31,000 --> 01:07:34,000
 we must be rigorous or we will fail."

1014
01:07:34,000 --> 01:07:37,000
 And people still look at me and say,

1015
01:07:37,000 --> 01:07:39,000
 "Yeah, good luck."

1016
01:07:39,000 --> 01:07:40,000
 [laughs]

1017
01:07:40,000 --> 01:07:43,000
 Okay, but I'm gonna take you through my version

1018
01:07:43,000 --> 01:07:44,000
 in the class, and we're gonna--

1019
01:07:44,000 --> 01:07:46,000
 you know, I think it won't be a burden, I think,

1020
01:07:46,000 --> 01:07:48,000
 if you don't believe me, but you'll at least see

1021
01:07:48,000 --> 01:07:52,000
 my view of the world through the course of the class.

1022
01:07:52,000 --> 01:07:55,000
 Any questions about that at a high level?

1023
01:07:55,000 --> 01:07:57,000
 I know this is pretty high-level stuff, but...

1024
01:07:57,000 --> 01:08:08,000
 So this belief that I have

1025
01:08:08,000 --> 01:08:13,000
 has taken life in this thing called Drake,

1026
01:08:13,000 --> 01:08:16,000
 which has been something I've been working on

1027
01:08:16,000 --> 01:08:17,000
 for a long time.

1028
01:08:17,000 --> 01:08:21,000
 It grew up in the days of controlling Atlas,

1029
01:08:21,000 --> 01:08:22,000
 the humanoid robot.

1030
01:08:22,000 --> 01:08:25,000
 That's when we started getting much more serious

1031
01:08:25,000 --> 01:08:28,000
 here at MIT about software engineering.

1032
01:08:28,000 --> 01:08:32,000
 When Toyota started their research institute,

1033
01:08:32,000 --> 01:08:35,000
 Drake moved over to being supported

1034
01:08:35,000 --> 01:08:37,000
 by professional software engineers

1035
01:08:37,000 --> 01:08:39,000
 and grew into a serious project.

1036
01:08:39,000 --> 01:08:43,000
 And then, you know, now it's being used

1037
01:08:43,000 --> 01:08:46,000
 by big companies and small companies.

1038
01:08:46,000 --> 01:08:47,000
 Lots of startups are using it.

1039
01:08:47,000 --> 01:08:50,000
 Amazon Robotics is using it for their manipulation stack.

1040
01:08:50,000 --> 01:08:53,000
 So it's grown into something big and real,

1041
01:08:53,000 --> 01:08:56,000
 but at its heart, it is a modeling language

1042
01:08:56,000 --> 01:09:01,000
 that tries to capture the complexity of manipulation

1043
01:09:01,000 --> 01:09:04,000
 in these sort of dynamical systems framework,

1044
01:09:04,000 --> 01:09:07,000
 and it has these three components, right?

1045
01:09:07,000 --> 01:09:09,000
 That's the systems framework

1046
01:09:09,000 --> 01:09:10,000
 for modeling the dynamical systems,

1047
01:09:10,000 --> 01:09:13,000
 for declaring the state variables,

1048
01:09:13,000 --> 01:09:16,000
 the parameters, and the like, okay?

1049
01:09:16,000 --> 01:09:19,000
 It also has a really--

1050
01:09:19,000 --> 01:09:22,000
 there's some really advanced physics simulation inside it.

1051
01:09:22,000 --> 01:09:26,000
 We have some really, really talented

1052
01:09:26,000 --> 01:09:30,000
 physics-engineers, if you will, researchers.

1053
01:09:30,000 --> 01:09:31,000
 They've done world-class--

1054
01:09:31,000 --> 01:09:33,000
 in terms of the sim-to-real gap,

1055
01:09:33,000 --> 01:09:36,000
 I think I would put Drake against any simulator out there

1056
01:09:36,000 --> 01:09:38,000
 in terms of the capabilities.

1057
01:09:38,000 --> 01:09:40,000
 And then it has a lot of tools

1058
01:09:40,000 --> 01:09:42,000
 for motion planning and control

1059
01:09:42,000 --> 01:09:45,000
 that are based on optimization, okay?

1060
01:09:45,000 --> 01:09:49,000
 We're gonna use-- we've made this all capable

1061
01:09:49,000 --> 01:09:51,000
 to be used in the class and run on the cloud

1062
01:09:51,000 --> 01:09:52,000
 and all these things like that,

1063
01:09:52,000 --> 01:09:55,000
 so it's gonna be the glue

1064
01:09:55,000 --> 01:09:57,000
 that puts all these pieces together.

1065
01:09:57,000 --> 01:09:59,000
 It doesn't try to be a machine-learning toolbox,

1066
01:09:59,000 --> 01:10:02,000
 like PyTorch is extremely good at being PyTorch.

1067
01:10:02,000 --> 01:10:05,000
 This is filling in a different part of the stack,

1068
01:10:05,000 --> 01:10:07,000
 the dynamics, the planning, the control,

1069
01:10:07,000 --> 01:10:11,000
 and they can work together, they can work with Ross, okay?

1070
01:10:11,000 --> 01:10:13,000
 There's a bunch of tutorials out there.

1071
01:10:13,000 --> 01:10:17,000
 I've seen it-- so in the past, people have said,

1072
01:10:17,000 --> 01:10:20,000
 "I wish you had told us a bit more about how Drake works,"

1073
01:10:20,000 --> 01:10:22,000
 especially when the projects came along,

1074
01:10:22,000 --> 01:10:24,000
 that, you know, we did problem sets,

1075
01:10:24,000 --> 01:10:25,000
 we were successful in our problem sets,

1076
01:10:25,000 --> 01:10:27,000
 but now I wanted to do something completely different,

1077
01:10:27,000 --> 01:10:30,000
 and I didn't have, you know, everything I needed to do that.

1078
01:10:30,000 --> 01:10:31,000
 So we're gonna try to balance that.

1079
01:10:31,000 --> 01:10:33,000
 I don't want to teach a class on Drake,

1080
01:10:33,000 --> 01:10:35,000
 but I want to make sure you have the resources.

1081
01:10:35,000 --> 01:10:38,000
 But we've also been pushing a lot more tutorials.

1082
01:10:38,000 --> 01:10:42,000
 In this evolution of the open-source project,

1083
01:10:42,000 --> 01:10:45,000
 you know, TRI made it very capable

1084
01:10:45,000 --> 01:10:48,000
 and was using it for research in Toyota,

1085
01:10:48,000 --> 01:10:50,000
 and then as more companies and more people

1086
01:10:50,000 --> 01:10:51,000
 were starting to use it,

1087
01:10:51,000 --> 01:10:54,000
 they just relatively recently have decided

1088
01:10:54,000 --> 01:10:59,000
 to emphasize tutorials and adoption, basically.

1089
01:10:59,000 --> 01:11:02,000
 So even if you took Underactuated this past spring,

1090
01:11:02,000 --> 01:11:05,000
 you might see how much-- there's more documentation,

1091
01:11:05,000 --> 01:11:07,000
 there's more user-friendly stuff

1092
01:11:07,000 --> 01:11:09,000
 even now than there was a few months ago,

1093
01:11:09,000 --> 01:11:11,000
 and even just some of the syntax that was a little gross

1094
01:11:11,000 --> 01:11:13,000
 is getting better, like, super fast.

1095
01:11:13,000 --> 01:11:15,000
 Even in the last two weeks, we've done a lot.

1096
01:11:15,000 --> 01:11:19,000
 So--and there's--if you do want to use it with Ross

1097
01:11:19,000 --> 01:11:21,000
 or in some other project, you're welcome to,

1098
01:11:21,000 --> 01:11:25,000
 but we'll give you a complete self-contained

1099
01:11:25,000 --> 01:11:31,000
 deep-note workflow for the class.

1100
01:11:31,000 --> 01:11:34,000
 This is just an example of the tutorials

1101
01:11:34,000 --> 01:11:36,000
 that talk about a lot of what I just said here.

1102
01:11:36,000 --> 01:11:40,000
 There's a particular modeling dynamical systems tutorial

1103
01:11:40,000 --> 01:11:43,000
 that talks exactly about how you would declare

1104
01:11:43,000 --> 01:11:46,000
 your state, your input U.

1105
01:11:46,000 --> 01:11:47,000
 Okay.

1106
01:11:47,000 --> 01:11:49,000
 I'll just say one more thing about it here,

1107
01:11:49,000 --> 01:11:54,000
 because we're gonna need it for the first P-set,

1108
01:11:54,000 --> 01:11:59,000
 which is that it turns out that if you want to--

1109
01:11:59,000 --> 01:12:01,000
 you know, all the complexity of modeling

1110
01:12:01,000 --> 01:12:05,000
 all the different things we want to do in manipulation,

1111
01:12:05,000 --> 01:12:07,000
 you don't get to state quite as simple as that.

1112
01:12:07,000 --> 01:12:10,000
 You need to have systems that have multiple rates, mixed--

1113
01:12:10,000 --> 01:12:13,000
 you know, they can have randomness,

1114
01:12:13,000 --> 01:12:15,000
 they can have parameters that you might want to tune

1115
01:12:15,000 --> 01:12:17,000
 with a system identification engine

1116
01:12:17,000 --> 01:12:19,000
 or a machine learning algorithm.

1117
01:12:19,000 --> 01:12:22,000
 Okay, so it gets a little bit richer,

1118
01:12:22,000 --> 01:12:24,000
 but in the way-- in one particular way,

1119
01:12:24,000 --> 01:12:28,000
 it gets richer, which is that we tend to write

1120
01:12:28,000 --> 01:12:30,000
 all of our functions,

1121
01:12:30,000 --> 01:12:32,000
 whether it's the dynamics function

1122
01:12:32,000 --> 01:12:33,000
 or the output function,

1123
01:12:33,000 --> 01:12:37,000
 to be a function of state, of input,

1124
01:12:37,000 --> 01:12:42,000
 but also of any randomness that comes in from an input port.

1125
01:12:42,000 --> 01:12:46,000
 Okay, so this would be any random inputs.

1126
01:12:46,000 --> 01:12:50,000
 And the reason you declare your randomness

1127
01:12:50,000 --> 01:12:53,000
 is so that if I just give you one random seed,

1128
01:12:53,000 --> 01:12:55,000
 the whole thing, the whole system

1129
01:12:55,000 --> 01:12:59,000
 is completely repeatable, for instance, okay?

1130
01:12:59,000 --> 01:13:03,000
 Parameters, P, okay, which would be, let's say,

1131
01:13:03,000 --> 01:13:06,000
 masses or inertias or lengths of a robot,

1132
01:13:06,000 --> 01:13:08,000
 or it could be the weights of a neural network

1133
01:13:08,000 --> 01:13:10,000
 in a deep learning system.

1134
01:13:10,000 --> 01:13:15,000
 These are the parameters.

1135
01:13:15,000 --> 01:13:17,000
 Okay, and so the functions--

1136
01:13:17,000 --> 01:13:19,000
 most functions in Drake

1137
01:13:19,000 --> 01:13:23,000
 want to be able to be a function of all of those things.

1138
01:13:23,000 --> 01:13:28,000
 So instead of passing those around

1139
01:13:28,000 --> 01:13:32,000
 as four or, you know, arbitrary numbers of inputs

1140
01:13:32,000 --> 01:13:34,000
 all the time, we just say,

1141
01:13:34,000 --> 01:13:39,000
 let's put them all into a structure, okay?

1142
01:13:39,000 --> 01:13:45,000
 And we're going to call it the context, okay?

1143
01:13:45,000 --> 01:13:47,000
 And so instead of writing this,

1144
01:13:47,000 --> 01:13:54,000
 you'll see in Drake, you'll see f of context.

1145
01:13:54,000 --> 01:13:58,000
 Okay, you see, x of n plus 1 basically

1146
01:13:58,000 --> 01:14:04,000
 equals f of context, or sensors is g of context.

1147
01:14:04,000 --> 01:14:08,000
 When you see that, you should just realize

1148
01:14:08,000 --> 01:14:11,000
 that just means that's just the structure

1149
01:14:11,000 --> 01:14:14,000
 that contains the state, the inputs,

1150
01:14:14,000 --> 01:14:18,000
 any random inputs and parameters, okay?

1151
01:14:18,000 --> 01:14:23,000
 Time also, time varying, okay?

1152
01:14:23,000 --> 01:14:24,000
 So that's just a--

1153
01:14:24,000 --> 01:14:26,000
 that throws people sometimes when they first see it,

1154
01:14:26,000 --> 01:14:28,000
 but it's very natural once you think about it

1155
01:14:28,000 --> 01:14:36,000
 as a way to write code for lots of dynamical systems.

1156
01:14:36,000 --> 01:14:40,000
 Okay, so the strategy for the lectures

1157
01:14:40,000 --> 01:14:42,000
 is to take a deep dive into--

1158
01:14:42,000 --> 01:14:45,000
 maybe not writing drivers, but into perception,

1159
01:14:45,000 --> 01:14:49,000
 and we're going to talk about both geometric perception,

1160
01:14:49,000 --> 01:14:51,000
 thinking about point clouds,

1161
01:14:51,000 --> 01:14:53,000
 thinking about point cloud registration,

1162
01:14:53,000 --> 01:14:56,000
 how do you do object pose estimation,

1163
01:14:56,000 --> 01:14:58,000
 for instance, in a point cloud,

1164
01:14:58,000 --> 01:15:00,000
 how do you do filtering and the like in a point cloud,

1165
01:15:00,000 --> 01:15:02,000
 in messy point clouds,

1166
01:15:02,000 --> 01:15:04,000
 but we're also going to do, of course,

1167
01:15:04,000 --> 01:15:08,000
 some deep learning-based perception.

1168
01:15:08,000 --> 01:15:11,000
 We're going to talk about both motion planning,

1169
01:15:11,000 --> 01:15:14,000
 level planning, and some task-level planning.

1170
01:15:14,000 --> 01:15:16,000
 Okay, we're going to talk about some control,

1171
01:15:16,000 --> 01:15:18,000
 how do you do force control,

1172
01:15:18,000 --> 01:15:19,000
 how do you do impedance control,

1173
01:15:19,000 --> 01:15:21,000
 you've heard these terms, right?

1174
01:15:21,000 --> 01:15:23,000
 Or what does position control even mean?

1175
01:15:23,000 --> 01:15:26,000
 But rather than, like, spend the first third of the class

1176
01:15:26,000 --> 01:15:29,000
 on perception, and the next third of the class on planning,

1177
01:15:29,000 --> 01:15:31,000
 and then the next third on control,

1178
01:15:31,000 --> 01:15:34,000
 what we're going to try to do is that by chapter three,

1179
01:15:34,000 --> 01:15:40,000
 you'll have a limited but a fully functional robot

1180
01:15:40,000 --> 01:15:42,000
 that can pick up red bricks and move them around, okay,

1181
01:15:42,000 --> 01:15:44,000
 if someone tells you where the red brick is, right?

1182
01:15:44,000 --> 01:15:47,000
 And then we'll say, okay, now someone didn't tell you

1183
01:15:47,000 --> 01:15:49,000
 what the red brick is, you've got to make

1184
01:15:49,000 --> 01:15:51,000
 an initial perception system, okay?

1185
01:15:51,000 --> 01:15:54,000
 And then, all right, now the scene gets really cluttered.

1186
01:15:54,000 --> 01:15:56,000
 How do you--how does the system have to advance?

1187
01:15:56,000 --> 01:15:59,000
 And that requires, you know, more work from the planning system

1188
01:15:59,000 --> 01:16:01,000
 and more work from the perception system, okay?

1189
01:16:01,000 --> 01:16:03,000
 And we're going to spiral out in this way,

1190
01:16:03,000 --> 01:16:06,000
 trying to make a more and more capable robot,

1191
01:16:06,000 --> 01:16:09,000
 and only introduce the cool tools from these pipelines

1192
01:16:09,000 --> 01:16:14,000
 when they make the robot do something new and different.

1193
01:16:14,000 --> 01:16:16,000
 Okay?

1194
01:16:16,000 --> 01:16:20,000
 And one other high-level point I'd like to make

1195
01:16:20,000 --> 01:16:24,000
 is that I've already dropped a few terms, right?

1196
01:16:24,000 --> 01:16:27,000
 I just said visual transformers, I said Kalman filters, you know?

1197
01:16:27,000 --> 01:16:30,000
 Some of you know a lot about some of those things,

1198
01:16:30,000 --> 01:16:33,000
 some of you haven't heard those things yet, okay?

1199
01:16:33,000 --> 01:16:38,000
 So when I think about lecturing to such a diverse audience here

1200
01:16:38,000 --> 01:16:40,000
 and really lecturing about robotics,

1201
01:16:40,000 --> 01:16:44,000
 one of the great things about robotics is that it's a kind of a mixing--

1202
01:16:44,000 --> 01:16:47,000
 a melting pot of so many different fields, right?

1203
01:16:47,000 --> 01:16:50,000
 And it's very hard to know everything about all of them.

1204
01:16:50,000 --> 01:16:53,000
 Okay, so how do I try to do that in a lecture, right?

1205
01:16:53,000 --> 01:16:56,000
 So I think the best way I can do that in a lecture--

1206
01:16:56,000 --> 01:16:58,000
 I'll take feedback, of course--

1207
01:16:58,000 --> 01:17:03,000
 is I try to make sure that if you know those things,

1208
01:17:03,000 --> 01:17:04,000
 if you've seen those things,

1209
01:17:04,000 --> 01:17:06,000
 I want to be able to make connections, right?

1210
01:17:06,000 --> 01:17:08,000
 If connecting that to a Kalman filter,

1211
01:17:08,000 --> 01:17:10,000
 and you've thought about Kalman filters, is useful,

1212
01:17:10,000 --> 01:17:12,000
 and I can say Kalman filter,

1213
01:17:12,000 --> 01:17:15,000
 then the people who have seen that will benefit, I think.

1214
01:17:15,000 --> 01:17:18,000
 And I don't want to, you know, avoid the word Kalman filter

1215
01:17:18,000 --> 01:17:20,000
 because we haven't talked about it yet.

1216
01:17:20,000 --> 01:17:23,000
 But I also try very hard, and you can tell me--

1217
01:17:23,000 --> 01:17:26,000
 if I ever say things and you say, "I didn't know Kalman filter,"

1218
01:17:26,000 --> 01:17:28,000
 but, you know, I hope you still get the point, right?

1219
01:17:28,000 --> 01:17:31,000
 The point of that statement about Kalman filters

1220
01:17:31,000 --> 01:17:34,000
 and recurrent networks and transformers

1221
01:17:34,000 --> 01:17:37,000
 was that a perception system, a modern perception system,

1222
01:17:37,000 --> 01:17:39,000
 can still be described as a dynamical system--

1223
01:17:39,000 --> 01:17:41,000
 should be described as a dynamical system.

1224
01:17:41,000 --> 01:17:45,000
 And I want to make sure you capture that level of it, at least.

1225
01:17:45,000 --> 01:17:49,000
 And if you ever don't, call me on it, right?

1226
01:17:49,000 --> 01:17:53,000
 But expect--I try to make layers of the class, right?

1227
01:17:53,000 --> 01:17:55,000
 So I want to be able to talk to experts.

1228
01:17:55,000 --> 01:17:57,000
 I want to be able to be--

1229
01:17:57,000 --> 01:17:59,000
 if there's things that you haven't heard of yet,

1230
01:17:59,000 --> 01:18:00,000
 you'd write it down.

1231
01:18:00,000 --> 01:18:02,000
 Maybe that's the most important thing to go read about tonight.

1232
01:18:02,000 --> 01:18:03,000
 Maybe it's not.

1233
01:18:03,000 --> 01:18:06,000
 Okay, but you don't have to--

1234
01:18:06,000 --> 01:18:08,000
 I hope you'll permit me to say some things

1235
01:18:08,000 --> 01:18:10,000
 that you haven't heard before, right?

1236
01:18:10,000 --> 01:18:12,000
 Because robotics is just so broad,

1237
01:18:12,000 --> 01:18:15,000
 and really, maybe the field isn't mature enough

1238
01:18:15,000 --> 01:18:18,000
 to just assume that everybody has all the prerequisites

1239
01:18:18,000 --> 01:18:23,000
 so that I can say--I can only build on what you've taken.

1240
01:18:23,000 --> 01:18:26,000
 Okay, let me finish up here.

1241
01:18:26,000 --> 01:18:28,000
 So you can--

1242
01:18:28,000 --> 01:18:30,000
 when you draw these block diagrams in Drake,

1243
01:18:30,000 --> 01:18:34,000
 you can render them as diagrams, and we'll do that.

1244
01:18:34,000 --> 01:18:37,000
 And you'll see that there's big libraries in Drake

1245
01:18:37,000 --> 01:18:39,000
 of different ways to--you know, different systems

1246
01:18:39,000 --> 01:18:43,000
 that implement these different components.

1247
01:18:43,000 --> 01:18:47,000
 Okay, yeah, so the schedule is completely up.

1248
01:18:47,000 --> 01:18:49,000
 I just told you the basic storyline

1249
01:18:49,000 --> 01:18:51,000
 is we're going to do basic pick and place,

1250
01:18:51,000 --> 01:18:53,000
 learn basic kinematics,

1251
01:18:53,000 --> 01:18:55,000
 learn basic Jacobian-based control and the like.

1252
01:18:55,000 --> 01:18:58,000
 That's going to get us off the ground with a basic robot.

1253
01:18:58,000 --> 01:19:00,000
 And then we'll start doing perception,

1254
01:19:00,000 --> 01:19:01,000
 but basic perception.

1255
01:19:01,000 --> 01:19:03,000
 And then we'll go back and get more cluttered scenes,

1256
01:19:03,000 --> 01:19:04,000
 and we're going to do this.

1257
01:19:04,000 --> 01:19:06,000
 It's all outlined here.

1258
01:19:06,000 --> 01:19:09,000
 There's a few lectures that I've left to be determined,

1259
01:19:09,000 --> 01:19:12,000
 and I want you guys to tell us what you're most excited about.

1260
01:19:12,000 --> 01:19:14,000
 Maybe I can fill out some things.

1261
01:19:14,000 --> 01:19:16,000
 I've certainly got plenty of things

1262
01:19:16,000 --> 01:19:17,000
 that I could talk about there,

1263
01:19:17,000 --> 01:19:20,000
 but I'd like to hear what you guys are excited about.

1264
01:19:20,000 --> 01:19:23,000
 The projects--all the project-based deadlines

1265
01:19:23,000 --> 01:19:25,000
 are up.

1266
01:19:25,000 --> 01:19:29,000
 They're aligned for the two versions of the class,

1267
01:19:29,000 --> 01:19:31,000
 but there's a few more milestones

1268
01:19:31,000 --> 01:19:33,000
 for the CIM component.

1269
01:19:33,000 --> 01:19:36,000
 Okay, so make sure you take a look through there

1270
01:19:36,000 --> 01:19:38,000
 and understand.

1271
01:19:38,000 --> 01:19:41,000
 Your goal is to hop on Piazza.

1272
01:19:41,000 --> 01:19:42,000
 Make sure you're there.

1273
01:19:42,000 --> 01:19:45,000
 So if we're in a different room on Tuesday,

1274
01:19:45,000 --> 01:19:48,000
 I'll tell you about that on Piazza, okay?

1275
01:19:48,000 --> 01:19:53,000
 And your problem set will be released very soon.

1276
01:19:53,000 --> 01:19:55,000
 Awesome.

1277
01:19:55,000 --> 01:19:56,000
 I'm looking forward to a good semester.

1278
01:19:56,000 --> 01:19:58,000
 I'll see you on Tuesday.

1279
01:19:58,000 --> 01:20:09,000
 [indistinct chatter]

1280
01:20:09,000 --> 01:20:12,000
 I'm so sad I didn't show the successful video.

1281
01:20:12,000 --> 01:20:13,000
 Hey, how's it going?

1282
01:20:13,000 --> 01:20:14,000
 - Separate to the class,

1283
01:20:14,000 --> 01:20:16,000
 I was wondering if I could ask you

1284
01:20:16,000 --> 01:20:19,000
 for a quick input on a problem I've been looking at.

1285
01:20:19,000 --> 01:20:20,000
 - Okay.

1286
01:20:20,000 --> 01:20:21,000
 - A little to do with under-actuated.

1287
01:20:21,000 --> 01:20:24,000
 - Let me just make sure there's no questions.

1288
01:20:24,000 --> 01:20:27,000
 [no audio]

1289
01:20:27,000 --> 01:20:30,000
 [no audio]

1290
01:20:30,000 --> 01:20:33,000
 [no audio]

1291
01:20:34,000 --> 01:20:37,000
 [no audio]

1292
01:20:37,000 --> 01:20:47,000
 [BLANK_AUDIO]

1293
01:20:47,000 --> 01:20:57,000
 [BLANK_AUDIO]

1294
01:20:57,000 --> 01:21:07,000
 [BLANK_AUDIO]

1295
01:21:07,000 --> 01:21:17,000
 [BLANK_AUDIO]

1296
01:21:17,000 --> 01:21:27,000
 [BLANK_AUDIO]

1297
01:21:27,000 --> 01:21:37,000
 [BLANK_AUDIO]

1298
01:21:37,000 --> 01:21:47,000
 [BLANK_AUDIO]

1299
01:21:47,000 --> 01:21:57,000
 [BLANK_AUDIO]

1300
01:21:57,000 --> 01:22:07,000
 [BLANK_AUDIO]

1301
01:22:07,000 --> 01:22:17,000
 [BLANK_AUDIO]

1302
01:22:17,000 --> 01:22:27,000
 [BLANK_AUDIO]

1303
01:22:27,000 --> 01:22:37,000
 [BLANK_AUDIO]

1304
01:22:37,000 --> 01:22:47,000
 [BLANK_AUDIO]

1305
01:22:47,000 --> 01:22:57,000
 [BLANK_AUDIO]

1306
01:22:57,000 --> 01:23:07,000
 [BLANK_AUDIO]

1307
01:23:07,000 --> 01:23:17,000
 [BLANK_AUDIO]

1308
01:23:17,000 --> 01:23:27,000
 [BLANK_AUDIO]

1309
01:23:27,000 --> 01:23:37,000
 [BLANK_AUDIO]

1310
01:23:37,000 --> 01:23:47,000
 [BLANK_AUDIO]

1311
01:23:47,000 --> 01:23:57,000
 [BLANK_AUDIO]

1312
01:23:57,000 --> 01:24:07,000
 [BLANK_AUDIO]

1313
01:24:07,000 --> 01:24:17,000
 [BLANK_AUDIO]

1314
01:24:17,000 --> 01:24:27,000
 [BLANK_AUDIO]

1315
01:24:27,000 --> 01:24:37,000
 [BLANK_AUDIO]

1316
01:24:37,000 --> 01:24:47,000
 [BLANK_AUDIO]

1317
01:24:47,000 --> 01:24:57,000
 [BLANK_AUDIO]

1318
01:24:57,000 --> 01:25:07,000
 [BLANK_AUDIO]

1319
01:25:07,000 --> 01:25:17,000
 [BLANK_AUDIO]

1320
01:25:17,000 --> 01:25:27,000
 [BLANK_AUDIO]

1321
01:25:27,000 --> 01:25:37,000
 [BLANK_AUDIO]

1322
01:25:37,000 --> 01:25:47,000
 [BLANK_AUDIO]

1323
01:25:47,000 --> 01:25:57,000
 [BLANK_AUDIO]

1324
01:25:57,000 --> 01:26:07,000
 [BLANK_AUDIO]

1325
01:26:07,000 --> 01:26:17,000
 [BLANK_AUDIO]

1326
01:26:17,000 --> 01:26:27,000
 [BLANK_AUDIO]

1327
01:26:27,000 --> 01:26:37,000
 [BLANK_AUDIO]

1328
01:26:37,000 --> 01:26:47,000
 [BLANK_AUDIO]

1329
01:26:47,000 --> 01:26:57,000
 [BLANK_AUDIO]

1330
01:26:57,000 --> 01:27:07,000
 [BLANK_AUDIO]

1331
01:27:07,000 --> 01:27:17,000
 [BLANK_AUDIO]

1332
01:27:17,000 --> 01:27:27,000
 [BLANK_AUDIO]

1333
01:27:27,000 --> 01:27:37,000
 [BLANK_AUDIO]

1334
01:27:37,000 --> 01:27:47,000
 [BLANK_AUDIO]

1335
01:27:47,000 --> 01:27:57,000
 [BLANK_AUDIO]

1336
01:27:57,000 --> 01:28:07,000
 [BLANK_AUDIO]

1337
01:28:07,000 --> 01:28:17,000
 [BLANK_AUDIO]

1338
01:28:17,000 --> 01:28:27,000
 [BLANK_AUDIO]

1339
01:28:27,000 --> 01:28:37,000
 [BLANK_AUDIO]

1340
01:28:37,000 --> 01:28:47,000
 [BLANK_AUDIO]

1341
01:28:47,000 --> 01:28:57,000
 [BLANK_AUDIO]

1342
01:28:57,000 --> 01:29:07,000
 [BLANK_AUDIO]

1343
01:29:07,000 --> 01:29:17,000
 [BLANK_AUDIO]

1344
01:29:17,000 --> 01:29:27,000
 [BLANK_AUDIO]

1345
01:29:27,000 --> 01:29:37,000
 [BLANK_AUDIO]

1346
01:29:37,000 --> 01:29:47,000
 [BLANK_AUDIO]

1347
01:29:47,000 --> 01:29:57,000
 [BLANK_AUDIO]

1348
01:29:57,000 --> 01:30:07,000
 [BLANK_AUDIO]

