 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 >> Yeah, I'll start talking consistently now and it sounds like it's going
 through the room at least, so that's a good sign.
 [BLANK_AUDIO]
 Is that coming through on the live stream?
 [BLANK_AUDIO]
 Are you able to hear me?
 [BLANK_AUDIO]
 It's a little bit delayed, I guess.
 [BLANK_AUDIO]
 I guess I've already spoken, so.
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 There's some seats here.
 [BLANK_AUDIO]
 I think sitting on the stairs isn't terrible.
 [BLANK_AUDIO]
 Yeah, there's some seats here.
 [BLANK_AUDIO]
 You think it's making noise?
 Oh, nice.
 [BLANK_AUDIO]
 Okay, hi everybody.
 [BLANK_AUDIO]
 Thank you for coming.
 Thank you for approximately the right number of you coming.
 I didn't actually give you any information that would have been helpful to decide
 whether you should come or not, but exactly the right number of people came,
 well, plus or minus a few, so thank you for that.
 I apologize for the room scheduling.
 I'm glad that the class is popular, but I also don't envy the role of the scheduling
 office right now, who are-- it's not just that there's classes that have big numbers
 or something, but it's also that if you look at the registration numbers across the classes
 and a histogram of these, or like a time plot of these numbers, they go like this.
 And so the scheduling offices has no-- it's a very hard prediction problem for them.
 And they're going to do their very best, I hope, to get us a bigger room, but we're
 going to-- we'll roll with it as it happens.
 So welcome to robotic manipulation.
 It was important, I felt, to have in the class-- I would have loved to call the class just
 manipulation, but I thought if somebody who doesn't know that we're talking about robots
 saw that, you know, maybe from political science or something like that, they would think of
 something very different.
 So I thought, let's qualify that a little bit.
 There was an early version that we called it intelligent robotic manipulation, but I
 didn't want some other manipulation class to come around, and then it looked like a
 put-down or something like that.
 So robotic manipulation it is, and I think it's going to be a fun class.
 There's a lot of things.
 The field is alive with progress.
 There's more robots out there doing more cool things now than ever before, and it's just
 an incredibly exciting place to be.
 So I hope to capture some of that enthusiasm for you, and even today, but certainly throughout
 the course of the term.
 Let me start by introducing us.
 So I'm Russ.
 I've been here for a while teaching robotics.
 We've got an excellent teaching staff.
 They happen to all be sitting together right here.
 So Bo-Yan's here, and Anthony is here, and Rhea is here.
 They're our TAs for the class.
 If there are a lot of people in the class, we might get that last seat filled.
 We'll see.
 The other, I would say the one most important bit of information, which is not a hard thing
 to remember, is that the website is at manipulation.mit.edu, and I'm not giving out any handouts, but
 all of the course information, grading rubrics, collaboration policies, all of the things
 that I am officially giving you, I am officially giving you with that link right there.
 And if you have any questions or thoughts about that, I'm happy to take those.
 There's an extra piece of the course.
 It started last year, and it's continuing this year.
 If you're in the undergraduate version of the course, it counts as a CIM now.
 So as the department has grown, and now we have AI and decision-making as a core part
 of the department, we wanted more of the AI and decision-making courses to be able to
 count as a CIM requirement.
 So natural language processing and this course have now taken on the ability to be a communications
 intensive in the major class.
 We have excellent teaching staff from CMS that are helping us with that.
 David's here.
 I think maybe Nora and Liz decided to save seats for the rest of you today.
 So the way you should think about that, so it looks like 15, the exact distinction between
 the two different tracks, the 4.
 It also makes me sad, by the way.
 Last year we had the number 6.800, which was like, I felt like I won the lottery getting
 .800, and now we've got 4, 2, 1, 0, which is not as cool.
 But if you're in the 4, 2, 1, 0, that's the undergraduate version of the class.
 It counts as 15 credits.
 You get one extra recitation on Fridays.
 And I'll tell you a little bit about what comes with that, all the great things that
 come with that.
 And then if you're in 4, 2, 1, 2, you're in the graduate version of the class.
 So just a very, the detailed differences are on the website.
 We can answer questions.
 At a high level, both groups will be doing a final project.
 The undergraduate version will be doing a really good final project, coached by some
 of the best communication people we have.
 And I would say at the end of last year, the people that were in that track had some of
 the best project presentations and videos and reports that I've seen ever.
 So if you're, I think a big part of this class actually is this product that you'll have
 at the end, which is a pretty, we've seen some pretty amazing videos.
 You should go watch some of the videos from the previous years.
 We had a robot playing the piano, like a concerto on a simulated piano.
 We've had some amazing things.
 And it'll look great on your portfolio, your CV.
 And I mean, industry, I have people from industry saying, "Oh my gosh, who did that project?"
 Right?
 So I think it really is an amazing opportunity.
 The recitations don't start tomorrow.
 They start a week from tomorrow.
 Before we've taught significant topics in manipulation, we're going to be doing, in
 those recitations, reading some research papers and just understanding a rhetorical analysis
 of how to write a good paper in manipulation.
 And then that will graduate into the project, the work towards the project.
 The graduate version will do also a project.
 The emphasis will be more on the technical and less on the communication.
 And the graduate version also gets a few extra problems and other things that are expecting
 a higher level of maturity on the problem sets and the like.
 Okay.
 So, like I said, the information about the grading policy, collaboration policies, everything
 is on the website, including the differences.
 If you scroll around, it talks about the exact differences, what it counts for, what it doesn't
 count for.
 It all changed last year, so I completely expect questions and there might be details
 that we didn't get.
 And the last thing I'll say about sort of logistics before we dive into some robots
 is that right now, you know, I'm going to say this again at the end, but just go to
 the website, click on the link to join the Piazza group.
 Apart from the one email I sent through the registrar yesterday, we're going to do all
 of our communication through Piazza.
 You can please review the course guidelines now so you're not surprised on anything later.
 The lecture notes are all online at the same website.
 The plan, the schedule is online.
 We're going to have weekly problem sets throughout the class, typically due on a Wednesday cadence.
 We'll have office hours to support that, probably Friday, Monday, and then we'll see
 maybe if we give one right before it's due, too.
 The first one will be released late tonight or tomorrow.
 It'll be a light one, just a warm up, and it'll be due next Wednesday.
 And then there's a lot of emphasis on the final project, and they've been really, really good.
 In fact, I should just, I'll queue up a few of the good ones to make sure that you've
 all seen some of them from last year, but they can be really exceptional.
 Okay, the course notes, which are there, when I say read and please comment on the course notes.
 So this is what you get when you go to the manipulation website.
 It's HTML notes.
 Some people hate that.
 They want PDF, and you can get the PDF if you want, but the HTML is, I'm trying to do more
 than you can do in a PDF.
 I'm trying to have really interactive content.
 There's animations that you'll be able to play.
 There's interactive simulations you'll be able to play.
 It's interactive in the sense that you can go in and you can comment, kind of like a
 Google Doc, and we have discussions on there about, like, I have no idea what you meant
 by this sentence, Russ, and so I'll try to say, I tried to say this, and people really
 do help the notes along and I think get to the bottom of some issues.
 Okay, and the link to the course part of the website is right in this course being taught
 at MIT.
 Okay, so I hope you take a look at that.
 I hope you use that, and I'm happy to take feedback on it.
 There's also all these links to the online notebooks that go along with the course.
 One of the cool things about it, I'm testing now my suspiciously bad connection to the
 MIT network, which is interfering with the Stata network, but one of the great things
 about all the class infrastructure is that now we can allow simulations to load over
 the internet with no installation.
 So, it used to be that we would try to help people through, limp through sort of making
 all of our robot code run on your machine and people would come with a Win32 machine
 or like, you know, Ubuntu 12 or something like this, right?
 And, that's all gone now because there's online cloud resources that come provisioned.
 You just log on to DeepNote, which if you've used Google Co-Laboratory, we used Google
 Co-Laboratory for a while, we've switched to DeepNote because it's easier to provision
 a stable, Colab can change the requirements out from under me and they always do it like
 on the first day of class, but DeepNote, I get to provision it with a Docker image.
 And so, you could basically be able to go to the website, instantly run the code, even
 the visualization, which is not loading very well for me right here, I'll run a local version.
 Should all just run with no installation on any machine you've got.
 I'll just run a local one here.
 So, it looks like this.
 This is the intro notebook just running locally.
 You'll get a little visualization just in the web.
 And then, if you run the very first example, you'll get a little robot up here and you
 can go into the controls and right through the web, you can sort of drive your robot
 around and I'll see if I can pick up the little red brick here for us here.
 Be sad if I can't.
 It's close.
 A little back up.
 It's easier with a joystick, but.
 It's a full physics engine if I grab the brick, pull it up, right?
 I can probably even throw it if I was really good.
 Okay.
 And that's just going to hopefully just work seamlessly for you.
 That's a long road to get to the point where it really mostly just works.
 Every once in a while, the cloud services will have an issue or something, but we've been
 pretty lucky with that.
 It works pretty darn well.
 Ah.
 See?
 It just loaded slowly over the MIT network, but it was there.
 Okay.
 So, my goals for today are to give you kind of a tour through what you're going to learn
 in the course and also to give you a little bit of the sort of initial thinking about
 not only the components of a manipulation system that we're going to talk about from
 perception and planning and control, but also the way that we think about it in this class,
 which is a little bit different, I think, than your average manipulation class.
 So, I try to take a little bit of a systems theory perspective when it fits, and I want
 to make sure I make some of those connections for you today.
 But let me start about just making sure we know what I mean by manipulation, because
 actually, I find a lot of people when they hear robot manipulation, they think of fairly
 narrow examples of robots that are just doing pick and place, for instance.
 And actually, manipulation is much more than pick and place.
 If you take away one thing from today's lecture, I hope you'll say manipulation is more than
 pick and place.
 Okay?
 Matt Mason, who's one of the leaders and big names in the field from Carnegie Mellon,
 he wrote an excellent review on robotic manipulation a few years ago.
 And one of the interesting things he did is he really tried to define, think deeply about
 what does it mean to be manipulation.
 Okay?
 And he actually gave five definitions, because he couldn't decide.
 He couldn't narrow it down, I guess.
 You know, the first one was, you know, just manipulation means activities performed by
 the hands.
 I won't take you through all of them, but he kind of eventually got to, you know, manipulation
 refers to an agent's control of its environment through contact.
 And I like that very much, I think, through selective contact.
 And I think it captures some of the, I mean, what robots are supposed to do, right?
 What makes robotics special compared to, let's say, computer vision or natural language or
 something like this is you get to move stuff.
 You get to change the world.
 And that, I mean, you could argue, maybe I will argue, that if we're really going to
 solve intelligence, it seems hard to imagine solving intelligence as a passive observer
 of the world through cameras.
 I think being able to pick stuff up and move it around and interact with the world seems
 pretty essential to our natural intelligence.
 And that's what this class is about, is filling in that part of the artificial intelligence
 spectrum.
 Okay?
 So, if I take that little example, like I just showed you, of a robot moving a brick
 around, we can sort of, you know, think through what that's going to look like.
 That's a pick and place example.
 But I really want to say, you know, robotic manipulation is not just pick and place.
 This is clearly robot manipulation by Matt Mason's definition.
 And it's way harder than picking up a red brick and moving it to the side, right?
 And if you look at the rich contact that's happening between his fingers and the shoelaces,
 and even just the dynamics of the shoelaces, you know, robots aren't doing this yet.
 Okay?
 So we have grand challenges just sort of in the mechanics of manipulation.
 But I'll show you some examples, too, that even if we're doing pick and place, if we're
 trying to do it out in the wild, things get pretty rich and pretty complicated in other
 axes, too.
 So I would say that's where I feel like Matt's definition doesn't completely capture the
 goals for the course.
 Matt says manipulation is about contact.
 And I think that's, of course, true.
 But if you think about doing manipulation not just in a closed environment or in a factory
 or something, but if you are out in the broad world and you want to send a robot out and
 do manipulation, then there's more broad requirements that come into play.
 I think it requires, in order to be in control of the environment, that's like an arbitrary
 loophole in the definition where we can inject having to understand everything about the
 world, right?
 Having a very rich perceptual understanding of the environment.
 I don't mean putting a bounding box around a person.
 That's good.
 But I need to know how much mass, the things I'm going to, you know, what's an object,
 what's not an object, what happens when I push it.
 These are demanding things that a computer vision system doesn't typically give you out
 of the box.
 This common sense of understanding, like what's going to happen if I push something?
 Am I going to topple a pile if I push it in the bottom?
 You know, these kind of things are grand challenges in AI.
 And I feel like they're part, they're under the umbrella of manipulation.
 Okay?
 The ability to make very long-term plans at the task level, like what am I going to do
 to get the milk out of the fridge?
 Okay?
 I've got to, first I've got to open the fridge, you know, then I've got to move the pickles
 out of the way.
 I'm not sure if you keep your pickles close to your milk, but you know what I mean, right?
 And then you reach back to the, there's a lot of steps involved to do a manipulation
 task which require a pretty high-level understanding of the world and reasoning long into the future.
 So, that's in the course material, too.
 And then you have to, once you've decided to do that, you've got to figure out how to
 use your motors and your joints to make that happen.
 Right?
 So, combining those different levels of abstraction is a grand challenge that we try to face.
 So let me show you a system that exemplifies some of that.
 It was a project a few years ago at the Toyota Research Institute, TRI, which is just down
 the street.
 I've been working with them for a number of years now to try to make some of the larger-scale
 examples of manipulation and take them to higher levels of maturity.
 And this is an example that I learned a lot from, trying to just, let's see, if you could
 take a big robot, this is not something that we are advertising you put in the home, but
 it is a robot that we have today that works pretty well, and we asked, could that robot,
 if someone put it in front of their sink and asked it to load the dishwasher, could it
 do it?
 What are all the problems involved in doing that?
 So, the problem in the open-world manipulation sense is someone, that's the one, he comes
 and he dumped whatever random things into the sink.
 Amongst them, some of them are dishes.
 Some mugs, some plates, some spoons.
 There's a dishwasher right next to it, and the task is to open up the dishwasher and
 to start putting the mugs in the top rack, the plates in the bottom rack, the trash off
 to the side, and the silverware in the little silverware rack.
 Okay?
 And this is a complete manipulation stack that did all of those components of perception,
 planning, control, high-level reasoning, okay?
 And it took a lot of work to put it all together, and then it took a lot more work to try to
 get it to operate at a very high level of repeatability.
 Like, if the same challenges that autonomous driving companies are facing these days to
 try to, you know, it's one thing to make a car drive down the road and make a video,
 but to make it never crash, you have to deal with all these long tails of the distribution,
 all the random things that could happen with the lighting conditions, with the stuff that's
 in the sink, and taking this system to a maturity was an excellent exercise that really changed
 my view of what are the hard problems.
 If you look down at the details, the individual skills that it had to do were actually fairly
 complicated from a control perspective, from a motion planning perspective in some cases.
 It had to open the dishwasher door.
 It would nudge things out of the corner.
 I mean, this is partly because it had an enormous hand and a small knife stuck in the corner,
 and so you can't do what a human would do, which is kind of, you know, pick it up.
 It had to really take different tasks.
 To pick up the plate is sort of my favorite one.
 You had to pick up a plate from a stack of plates.
 This big hand had to kind of, you know, go in, and this is a feedback law, which was
 constantly monitoring its sensors to slide under until it knew that it had pushed far enough,
 grab the plate, pick it up, and move.
 Each of the individual pieces of this were actually pretty sophisticated, okay, but then
 you had to assemble that all into this higher level machinery.
 By the way, a big part of that was using simulation.
 That actually, that video right there, if you can tell, that's actually a simulation,
 right, of the robot picking up a plate, and the way we were able to get that to a pretty
 high level of maturity was by having a very good match between simulation and reality
 that we worked very hard on and getting to the point where we could stress test in simulation,
 find the corner cases in simulation, and expect those corner cases to start disappearing in reality.
 Okay, so, a lot of work on simulation, and that's a relatively new thing.
 A few years ago, I said this in the text, right, a few years ago, I remember when we
 were doing humanoid robots, and I was talking to my students in the lab about, you know,
 we should be doing manipulation in simulation, too, because it's working so well for our
 walking robots, and I remember, you know, they looked at me like, "Russ, you can't do
 manipulation research in simulation.
 It depends on perception.
 You can't simulate perception well enough to do that in simulation.
 You know, the dynamics of contact, like subtle things between a hand, you can't do that in
 simulation.
 Simulators aren't good enough."
 Okay, and then, it changed.
 Like, a few years ago, computer graphics renderers got good enough.
 You used Blender, for instance, as your renderer.
 It's an open source rendering engine, right, and everybody was suspicious that if the rendering
 wasn't perfect, then, like, a machine learning computer vision system would cheat, and you
 would know how to use the artifacts of the renderer to solve the problem, and it wouldn't
 actually work in reality.
 But guess what?
 The renderers are good enough, and people train in simulation and get it to work in
 reality.
 That's changed.
 People now think you can train perception systems.
 The other big aspect of that is the physics.
 The physics engines were not good enough, the real-time compatible physics engines.
 They were working for legs, which are actually relatively easier for a walking robot to simulate,
 but for the delicate interactions required in manipulation, they weren't good enough
 a few years ago, and now, they're getting to be good enough.
 And there's nuances in the different simulators, but we've seen dramatic success in transferring
 results from simulation into reality, if you do the work to match.
 You have to make sure your models match into the simulator.
 And all of those components had to go together into this high-level planner, so that if someone
 came and adversarially-- so in Boston Dynamics, they kick the robot, and that's cool.
 We just close the dishwasher drawer.
 That's not quite as cool, but it tries to make the same point, right?
 If someone came and messed with your robot, it had to be smart enough to actually, in
 that case, it was putting a mug in the top.
 And I just realized, oh, someone closed the dishwasher drawer.
 I'm going to set the mug back down, pick it up, because I've only got one hand.
 That's pretty annoying, right?
 And then, you know, you could do it all day long, and you feel bad for the robot.
 But yeah, so that was a complete system, end-to-end.
 And that's kind of the goal for the class, is to help you build out a complete system
 and understand the nuances, some of the interesting parts of the algorithms at each level of those
 hierarchy.
 OK, so it really does go--
 I'd say there's kind of a ladder of complexity in terms of high-level reasoning, if you will,
 which involves scene understanding, being able to make sense of what objects are in
 the world.
 You know, where is the milk in the fridge?
 Deciding to move the pickles before you pick the milk.
 And then there's low-level, like how do I feel forces on my hand and decide I should
 do something a little bit different?
 So it's very interesting to try to span that whole space.
 I come from the controls perspective towards this.
 And some of you have taken it under-actuated with me.
 So let me just sort of connect that to the-- you know, from my view of the world.
 Like, why did I come from controls towards manipulation?
 OK?
 So, you know, before, we were doing humanoid robots.
 This was the DARPA Robotics Challenge.
 That was an early version of the Boston Dynamics robot.
 It's doing backflips now.
 It was a lot heavier and not as backflip-ready back then.
 But we spent a lot of time on this robot and worked very hard on the control system and
 very proud of what we did.
 And we, you know, we worked very hard on understanding the dynamics of that system,
 simulating that system, writing, you know, understanding its robustness properties and
 the like.
 We got to the point where, you know, even when it was getting out of the car-- that
 was the hardest part of the challenge, by the way.
 There was a-- we had to drive this little car with this enormously big robot.
 And then getting out of that car was like solving a Jenga or something or, you know,
 Twister to get out of the car.
 But we worked hard on the feedback controller.
 And you got to the point where even if Andres is jumping on the back of the car, the robot
 really didn't fall down.
 So we-- in some sense, I think we know a lot about feedback control.
 Robotics has gotten pretty good at controlling complicated robots like that, right?
 Even perception is a big part of walking around the world, right?
 So we had to use our onboard sensors to sense the world, you know, understand it well enough
 that we knew what surfaces we could step on, where we could-- where we should not step,
 right?
 So we were solving problems like that.
 This was in, you know, 2015 kind of technology.
 It was actually right before the machine learning boom, right?
 So this was all much more geometric perception at the time.
 And like a year later, we would have done it with deep learning probably.
 But we had a good pipeline for perception.
 But none of that gets me through the sink, right?
 So it's interesting to see where that sort of dies, right?
 Is that the amount of understanding you have to do to understand where to walk in the world
 is so much less in terms of understanding perceptually the world
 that it really-- it only scratched the surface on the really hard problems.
 So this is an image of a sink with mugs in it.
 And those points there are an estimate.
 We're trying to estimate-- this is now a deep learning system that's estimating the poses of the object.
 And those are representing the uncertainty of the object's pose,
 those different colors and the size of those colors.
 And there's someone threw a napkin in there, right?
 And there's a mug underneath that napkin.
 And it's pretty confused about whether that's actually a mug right now.
 And even just knowing that those things are separate mugs versus one,
 I mean, this is just a harder manipulation problem-- or a perception problem.
 And it requires a lot more work, not only in perception for the sake of perception,
 but the connections between perception and control.
 Especially-- this is just an example of-- now I have to manipulate any mug.
 And the number of mugs you can find if someone throws them in the sink,
 they're pretty diverse.
 So knowing how to manipulate a particular mug
 is not enough to understand how to manipulate all the mugs.
 How do you program a robot in a way that basically solves the mug problem
 when someone could have gone to the Disney store
 and came back with a mug that looks like one of the seven dwarfs?
 That just totally breaks a lot of our perception systems.
 And we've been trying to generalize their tools to do much more general--
 have a robustness in a much more general sense.
 I care a lot about feedback, right?
 We made Atlas not fall down when Andres was jumping on the car.
 How important is feedback in manipulation?
 It's an open question, I would say.
 I mean, I strongly believe in the answer,
 but there's people out there that are absolutely not thinking about feedback
 in the actual manipulation of the hand sort of problem.
 What they're doing instead is building really clever graspers-- grippers, right?
 So this is a soft robotic hand that you can--
 I mean, it's just being told to squeeze,
 but the dynamics of the hand are such that
 pretty much anything you put down in front of it,
 you've got to make a nice conformant grasp around this and pull it over.
 And for some class of problems, these hands knock it out of the park.
 But like I said, manipulation is more than pick and place.
 And if you look at how humans-- if you just watch yourself--
 go home and just-- when you're making dinner tonight
 or loading the dishwasher or something, watch yourself.
 The things we do with our hands, they're so hard for us to reason about.
 They're subconscious.
 But there's always these-- like right here, look at that.
 She missed a little bit, right, and then does this corrective action.
 And I believe that actually we're missing out a lot
 by not having rich feedback loops connecting perception
 and tactile sensors to the commands we send to our robot.
 And the world is now seeing more and more success
 of feedback control and manipulation.
 And so that connects to my background as well.
 This is just another-- you can watch--
 if you watch high-speed video of yourself doing anything with your hands,
 it's amazing what we do.
 And you don't even think about it, right?
 The way humans load a dishwasher is so different
 than the way we were having our robots load the dishwasher.
 The robot would try to line up the plate and stick it down in the slot.
 Humans just go bang, boom, and just rely on the fact
 that it kind of will fall into place.
 We're so clever, and we're so dexterous,
 and robots are not getting it done that way.
 So when I think about control for manipulation,
 when I think about control for the humanoid,
 we have a problem, which is that the robot has some joints.
 We want those joints and maybe the center of mass
 to go through some trajectory, okay?
 And we know how to think about that, and we know how to build models for that,
 and we can build models that work even on various terrain.
 But we're thinking about manipulation.
 It's not just about controlling the robot anymore.
 That's part of the problem. That's a sub-piece of the problem.
 But it's not just controlling the arm.
 The state you're trying to control is the state of the robot,
 but also the state of the world,
 in this case, the state of the red brick, okay?
 And that's what makes it interesting.
 That's what makes it underactuated, for instance, okay?
 So for me, it lights me up.
 I think to the point where I really think
 the next big thing that controls has to do--
 that's a biased opinion--
 but the thing that will grow controls into the next set of great problems,
 I think manipulation has a lot of that richness, and here's why.
 You know, controlling the state of this red brick is sort of--
 I kind of know how to formulate that problem at least.
 Maybe it's a hard control problem because of the contact mechanics,
 but I kind of know how to write that problem down.
 But if I want to, like, chop an onion, okay?
 Like, what's the state of the onion?
 If I want to simulate that, you know,
 is it changing every time the knife comes down?
 Like, what trajectory am I trying to stabilize?
 I don't know how to think about that.
 Controls really doesn't have a lot to give yet
 in terms of that state representation question.
 Learning's starting to contribute a lot,
 and actually learning plus controls are coming together, I think,
 to address this grand challenge of state representation for control.
 And once those states are coming in through a camera,
 it opens up all kinds of interesting problems, too.
 So we're now seeing more and more feedback-based control in manipulation.
 I think it can make a huge difference.
 It makes a huge difference in the reliability of the demos.
 You can see, you know, in practice, the systems just feel much more real now.
 And the big go-ahead technology, which we'll talk a lot about when it's time,
 is the ability to make feedback control decisions directly from the camera.
 It used to be that we would kind of look at the camera, decide what to do,
 make a plan, and execute.
 Sense, plan, act is sort of the old way to think about it.
 But there are now visual motor policies where you're actually closing the loop
 on the camera input at high rates.
 And that really makes the difference between a robot demo where the robot--
 there's like robot air balls, right, where the robot does something,
 the world changed, and it continues to do something as if the world hadn't changed.
 And it's really embarrassing.
 And we've had robots that fall down because they just thought the valve was there
 and it wasn't there.
 And that's starting to change now.
 We're starting to be able to close the feedback loop at high rates
 through a perception system.
 This one's just-- I love this particular example.
 This is just that-- like we did picking up the plate from the sink,
 but now it's trying to do it from rich camera-based real-time feedback.
 That's the nominal behavior.
 But now we're trying to make it robust to all kinds of perceptual changes.
 And the feedback there is only from the cameras.
 But we're getting to the point where we're seeing more and more demos
 that do what they should do in these kind of situations.
 But underneath that, I believe-- I really believe to my core that the way you get to that
 is by breaking that super hard problem down into simple models,
 the same way we talk about in underactuated, for those of you that have taken it,
 and breaking it down into the sort of-- the place where you can think rigorously
 about what's happening in the system at all the different levels.
 So underneath that technology are these relatively simplified models of physics
 that we can reason about, we can practice on, we can understand.
 Here's another fun example from TRI.
 We're not doing dishes anymore. We're making pizzas and the like.
 I'll show you more videos of those throughout the term, but this is just rolling dough,
 another example of manipulation being a lot more than pick and place.
 And again, it's using visual feedback.
 So if someone comes and throws down some more dough or whatever,
 these systems are now getting more and more robust to real-time visual feedback,
 changing the task.
 And this is a case where I don't really know what the state of the dough is,
 but I've still got to come up with a good controller that'll do the task.
 And these are the problems of the day.
 And these are the problems that I think control has to grow to address.
 On the -- oops, I put it in -- oh, where did I --
 okay, well, I have to find the other video.
 But there's a success video before the interesting failure cases.
 [laughter]
 There's a robot also at Toyota where I've got the best videos from Toyota.
 They built this incredible robot.
 It's called the TTT robot.
 And the task here is to go into a real grocery store, not just some --
 we have a mock grocery store at Toyota,
 but we also have a real grocery store down the street that we're collaborating with.
 And the task is not easy, obviously, to pick up all these objects and be robust.
 And actually, I'm very proud that they even let me show this video,
 because they think very seriously about the failure cases,
 and they just say, "This is a hard problem.
 We're going to measure how often we fail,
 and we're going to make it better and better and better the same way."
 But this robot, in the success video, the task is,
 wake up, you're in this grocery store that you've seen before,
 but you're going to be told some number of items that you've never --
 just from a list of hundreds of items,
 you're going to pick these items and put them in my grocery basket.
 And it drives through the store, and with an increasingly high success rate,
 is able to sort of go through and understand and find the objects
 and load a grocery basket.
 This kind of stuff's coming.
 Now, that doesn't -- apart from the complicated failure cases I just showed,
 that doesn't stress as much the dynamics of a dexterous hand,
 but the perceptual understanding of the world is really hard in this case.
 And some of the failure cases where you thought you could pull the object out,
 but it was actually in a box, and the whole box tips out, right?
 These are really hard cases for a perception system to understand.
 Okay. So that's kind of the motivation at a high level.
 Those are the kind of things we want to cover.
 And now I want to tell you how we're going to cover them,
 and tell you a little bit about the sort of breakdown of the system
 and the style, the way we're going to try to connect
 the pieces of those manipulation systems to dynamics and control,
 kind of dynamical systems.
 And let me start by just saying that the anatomy of most manipulation systems
 these days has ROS as a big part of it.
 How many people have used ROS, or know what ROS is even?
 I'm happy to -- yeah?
 Okay. ROS is the robot operating system.
 It's probably one of the best things that happened to robotics,
 you know, a decade ago at this point.
 And it's an ecosystem.
 It's not an operating system in the Windows or Linux sense,
 but it's an operating system in the -- it's an ecosystem
 where people are contributing different modules,
 perception systems, planning systems, simulators, for instance.
 And ROS makes it easy to connect them together.
 So those of you that raised your hand know this,
 but let me just say a few things about it as a launching point
 to the way we're going to think about things.
 So we said that this is okay.
 You guys can all see that even if I'm -- okay, great.
 So in ROS, if I have a perception system,
 I can build components in sort of a modular approach.
 Okay, so maybe I have -- I start off, I have a camera driver.
 Okay, and someone needs to write a camera driver,
 and that takes a bunch of work, you know,
 especially as cameras change or whatever.
 I've got some camera driver that has to talk to firmware
 and publish out an image, let's say.
 Maybe a red, green, blue image, for instance, coming out.
 Okay, there's another big chunk of work,
 which is to come up with a perception system.
 And maybe that takes RGB inputs in and outputs,
 in the simple case, let's say, the position of my red brick.
 Right, in the onion, it's a much harder question,
 but in the red brick, I could just tell you where the red brick is,
 and that's pretty good.
 Okay, someone else needs to write a planning system, let's say.
 Maybe two, maybe there's a high-level planning and a low-level planning,
 let's say there's some sort of planning system that takes,
 let's say, the positions of the brick and the positions of the robot,
 for instance, and starts putting out a joint trajectories.
 Okay, and then we've got some low-level controller
 that thinks about maybe the dynamics of the arm
 and tries to realize these joint trajectories,
 and maybe it has to send low-level motor commands.
 Okay, and at the other end of this, I've got a motor driver.
 Okay, and every one of those is a research, you know,
 maybe this one and this one are his research projects, see,
 but certainly all of these are massive research challenges,
 and traditionally it was very hard for one research group to do all of them well.
 And the big thing that happened with ROS was it became a standard for sharing components,
 where maybe I could use a perception system from Carnegie Mellon
 and maybe a controller from DLR in Germany,
 and maybe I'll focus my research on the planning system.
 And the way it works is it's based on message-passing network interfaces,
 and it's multiprocess.
 Okay, so basically someone can write a program here that does camera drivers,
 and they will just publish on a network, on an Ethernet for instance,
 a particular type of message that contains the data for an RGB image.
 And all that we have to agree on, if I'm going to use your camera driver,
 is the format of that network message.
 And then I'll write a perception system, and maybe I'll agree to use your RGB image network packet format,
 and I'm going to try to produce a position format that everybody agrees on.
 And if we just agree on a few of the common message types, and that's it,
 and let everybody write their own individual executables,
 this was the go-ahead idea that made people really start being able to share their code.
 And it's subtle, actually. I don't know how many people do a lot of software engineering,
 but it's for somehow subtle reasons.
 Even compiling someone else's code on your machine,
 and having the right version of the dependency libraries all work together,
 can be a real roadblock to trying to get some code from CMU or something to run on my robot.
 By separating out the concerns of compilation and making this executable-level decomposition of the task,
 and only agreeing on the message type, where it's easy, everybody can,
 whether you're using different programming languages, someone could write in Python,
 someone could write in C++, someone could whatever,
 all we have to agree on is the packet protocol on the network.
 With things like Docker, people don't even have to agree on the operating system.
 People will run a perception system on a Docker container on Ubuntu 14,
 and I can still use that on my Mac.
 These kind of things caused a level of modularity and abstraction
 that got roboticists to finally start sharing their code, and really using each other's code.
 A new lab could start up a serious robotics project by picking the best components here,
 they'd get a system that would actually run and do some interesting things,
 and then drill down and start to work on the different components.
 That was a major good thing that happened in robotics.
 And just even, it started the culture of open sourcing your code.
 That wasn't a big culture beforehand.
 But, we're not going to use ROS in the class.
 This is the starting place, but it doesn't serve my pedagogical goals.
 You can use ROS if you want to, but I think the connections here
 of only talking about what are the message types is a little too weak
 in order to really try to take, I think even a lot of companies struggle with it.
 It's very good for getting an initial system off the ground,
 but then once you start trying to take a system to really high levels of reliability,
 the semantics of how these systems talk together gets much more subtle.
 And just promising that I'm going to publish at some, whenever I want to publish,
 for instance, some position of the brick, that might not be good enough.
 And so, I think the field is on the path to higher levels of maturity.
 ROS is growing in this direction too, of trying to do a little bit more reasoning
 about not only the way to write these systems individually,
 but the way to connect them together.
 So, let me tell you about it from the perspective of dynamical systems and control.
 The way a control theorist might have started this,
 they would have drawn a very similar diagram, a block diagram.
 So, maybe it starts, I'll start with the robot,
 because that would be more standard in controls.
 Maybe this is a simulation, for instance.
 Okay, and I'm taking motor commands in.
 And having some sensors come out, some sensor signals come out.
 Now, this is something that control theorists have been doing forever, right?
 Maybe not with onions, right? Or laundry or something.
 But certainly for aircraft and for chemical plants and for all kinds of rich systems,
 control theory has been incredibly successful.
 And they have a modeling abstraction, a hierarchy, a modularity approach
 that's very similar in the pictures I've drawn, but it's different in the details.
 So, I'll still think of this as a block diagram,
 but I'm going to be specific about the details inside here.
 I typically will represent this as a dynamical system.
 So, I'll write it in a generic way today, and it'll even be okay for a while.
 So, this is a difference equation,
 where X is used in a control sense to represent the state of the system,
 which maybe in the case of my robot would be the positions
 and velocities of the robot plus the brick.
 U is my command inputs, so my motor commands coming in.
 X is my next state. Xn+1 is my next state.
 And so, in this setting, F has to be somehow my physics model, right?
 My physics engine here.
 It's somehow connecting equations of motion
 that look like force equals mass times acceleration
 with these notions of state and the notion of next state.
 Acceleration is a continuous time idea, derivatives, right?
 And somehow I've talked about a discrete jump from one state to the next,
 and I'll talk about how to make those jumps.
 Now, these equations might be familiar to you in simpler forms, right?
 If you took 18.03 here, or a differential equations course,
 then you would have seen them first as, let's say, a linear set of difference equations.
 You might have seen it that looked like this.
 Or the matrix form of that might be where X is a vector, right?
 This would be a linear difference equation.
 If you took an intro controls course, you would have seen
 something that looked like this, the state space form of a linear difference equation.
 OK, from controls, which would be just the--
 it's now a control difference equation.
 This would be a linear control difference equation.
 OK, now I didn't require 18.03 or DIPI-Q or linear intro controls
 as a prerequisite of the course.
 If you were to take those courses when you took those--
 many of you have taken 18.03 at least, right?
 You would have been able to go start from those equations
 and thought a lot about the time evolution of these.
 You could solve the differential equation given initial conditions.
 You could talk about its stability properties.
 There's lots of things you could potentially do.
 And we don't need all that right away for this class.
 OK, so we're not going to use, let's say, the deeper content from 18.03,
 but we are going to definitely use the modeling language.
 OK, and you should see this f of x, u
 as just a nonlinear generalization of these equations
 that you would have seen in those classes.
 OK, because f is complicated, it's now a physics engine,
 it becomes harder to do the closed-form analysis
 that you did in the intro classes.
 OK, but we're still going to benefit a lot
 by writing it down in this dynamical systems language.
 OK, so we're going to talk a lot about having our block diagram of the system
 and using equations of this form.
 This is not quite enough.
 OK, we also need to model the sensors.
 So the sensors we'll typically use in the language of dynamical systems
 as an output of that function, of that state.
 It could be a function of x and u in general,
 and y is now the outputs at time n.
 OK, and in the case of my sensor being an RGB camera,
 f might have been my physics engine,
 but g is going to be my game engine quality renderer.
 If I have to go from the positions of the robot and the brick
 over into an RGB image, then g, I can write it as a function,
 but down in the details, that's rendering.
 So these get to be very complicated functions.
 But what I hope to convince you over the course of the term, really,
 is that by thinking about it through these equations,
 it's going to ask you a little bit more than Ross does.
 I want you, in particular, to tell me what the state is.
 I want you to tell me what the timing semantics are.
 How do I go from n to n plus 1 if my camera is running at 30 hertz
 and my robot simulation is running at 100 hertz
 and I've got events based on some other sensor that's doing some strange things?
 I need a modeling language for talking about how those parts interact.
 And that's going to ask more.
 In fact, it's going to feel annoying when you start writing these systems,
 and I'm not just going to say, "Give me a function."
 I'm going to say, "What's the state variables?"
 "What's the randomness?" You have to declare the randomness.
 There's a couple things you have to declare.
 But the advantage over the Ross, very light touch here,
 for the purposes of the class,
 is that you get to do more sophisticated things with the models.
 If I only know that there's arbitrary executables behind the box
 and they send messages out,
 then there's limits to what I can say about what they do when they're connected.
 If I know that these systems are deterministic functions
 once you tell me the state,
 then, for instance, I can run exactly the same simulation twice.
 Anytime I just put the state in, I run the same controls through,
 and I'll get the same outputs back out.
 Deterministic simulation.
 It sounds crazy to me, even for me to say that right there.
 It sounds crazy, but most people in robotics
 can't run the same experiment twice, even in simulation.
 It's just a weird thing.
 But everybody's got different processes running on different clocks
 and sending messages when they want to send,
 and nobody wrote down a specific contract saying,
 "You must send at a certain rate. Messages must arrive in a certain order."
 So if you see a bug in your simulator,
 you see the robot fell down in some weird way
 or threw a brick across the room,
 and you say, "I'm going to reproduce that,"
 and you run it again,
 then your perception system might have sent a message
 just a little bit before or different.
 It's very hard to get a deterministic, repeatable simulation
 out of a generic ecosystem like this.
 But if you ask every one of the individual systems
 to declare its state,
 if you ask all of them to be deterministic
 or declare its randomness,
 then you get an extra power when you start combining them
 and knowing how things are going to work.
 So at the very least,
 when you have a bug in your final project,
 we'll be able to help.
 And in general, we're going to try to keep things
 running in a single process instead of multi-process.
 I'm going to try to emphasize the interesting parts of the components,
 and hopefully, if you do a little bit of work when you declare them,
 then the details of multiple message passing and all that stuff
 will just disappear, and you don't have to worry about it.
 Okay?
 So certainly, robot simulations,
 if I think of F as a physics engine and G,
 you can sort of imagine X as the state,
 U as the positions and velocities,
 U as the motor commands,
 Y might be my camera image or my joint sensors.
 But actually, I would argue that all of the systems
 in our hierarchy
 can be described nicely with those same sets of equations.
 Okay?
 So let's think about a perception system for a second.
 Okay, so a modern perception system,
 maybe it takes in an RGB image.
 These days, let's say it goes through a deep network,
 and it outputs the position of the brick.
 Now, a lot of deep learning-based perception systems
 really just look, in this case,
 if the position of the brick is Y and the RGB image is U,
 they can be modeled just as a static function.
 Y of N is G of U of N.
 So it certainly fits into the dynamical systems framework,
 but maybe doesn't exercise the dynamical systems framework
 because the state is empty.
 There's no state.
 Okay?
 But that's not how we used to build perception systems.
 Right?
 If you've taken a class on state estimation
 or if you've heard the terms,
 like a Kalman filter, for instance, right?
 A Kalman filter will take observations in,
 and it keeps an internal estimate
 of what's the state of the world.
 Okay?
 So it's got a state-space form,
 and will output the estimated state.
 This fits squarely into the--
 even if it's an extended Kalman filter,
 it fits squarely into the modeling paradigm.
 And if you've worked with Kalman filters,
 I mean, you might-- if you've done a--
 you know, a summer at a autonomous driving startup
 or anything like this, or, you know,
 autonomous driving company, for instance,
 you've probably come across Kalman filters.
 Okay?
 If you think about perception in this way,
 where the goal of a perception system
 is to summarize all of the things it's seen,
 maybe in the recent history of the world,
 into some coherent understanding
 of what's happening in the world,
 that's very different than what we see
 when you go from a single RGB image
 out to an estimate of the world.
 Okay?
 And it was actually pretty weird for a bunch of us, right,
 when deep learning started to work really well.
 Everybody was talking about few-shot--
 or one-shot learning, right, or zero-shot, right?
 And, you know, people were like, "That's crazy.
 "Why would you not use multiple images?"
 Or, like, "Why would you not remember
 "what you've seen in the past
 "when you're making your prediction?"
 And, indeed, I mean, the deep learning perception systems
 work incredibly well, even from a single image,
 but the modern, you know, deep learning perception systems
 are--actually look a little bit more like this,
 where they'll have a recurrent neural network in the middle
 or a visual transformer in the middle, okay?
 And those, again, the recurrent--
 the state of the recurrent network
 is gonna be--can be declared as a state
 in my dynamical systems framework.
 And transformers are a little harder to think about that way,
 but they totally fit in these frameworks, okay?
 'Cause, really, the goal of perception
 should be to accumulate information over time, right?
 Certainly, the way I perceive the world
 is not, you know, take an image
 and then, you know, understand everything about it, right?
 I'm accumulating information as I move through the world
 and summarizing it in some belief, okay?
 And so these kind of perception systems
 fit beautifully into the dynamical systems framework.
 If you go to think of--
 I mean, control absolutely fits right into that.
 The robot controllers, if you want to write
 an impedance controller
 or some inverse dynamics controller,
 that absolutely snaps right into this framework.
 It came from that world, okay?
 Planning systems are more interesting,
 and we'll talk about 'em when the time comes,
 but, you know, a lot of times,
 if I'm writing a planning system in a Ross ecosystem,
 I'll, you know, listen to the perception system,
 and then I'll, like, go think for a little while
 and make no commitments whatsoever
 about when I'm gonna return an answer, you know,
 or if I will return an answer, actually,
 and eventually say, "Oh, you should do this," right?
 And, you know, the timing semantics
 around a planning system,
 when these are long-running, you know, planners, potentially,
 can be very stochastic.
 It can be a big source of either conservatism--
 if you have to wait for your planner
 to come up with an answer,
 that's why the robots will, you know,
 do something interesting and then wait for a little while
 and then do something interesting
 and wait for a little while--
 or if you're trying to keep that system completely moving,
 then the semantics of when this planning system
 reports its answers gets pretty subtle, okay?
 But when we get into the details,
 that still fits into these dynamical systems.
 You can still model that
 in the language of dynamical systems.
 Okay, so what we're gonna try to do in the class
 is very much keep the good things
 about the modular architectures,
 but also try to declare our state variables,
 our randomness, our inputs and outputs, okay?
 And then you'll be able to compose
 these modular components into a big system,
 you know, get repeatable simulations.
 And if you want, you can do advanced control analysis,
 verification, you can do, you know, Monte Carlo analysis,
 but you can also try to prove
 that a system's gonna converge, you know,
 exponentially to some equilibrium,
 even if it's got some really complicated components
 in the way.
 Okay, so that's been a bit of my, you know--
 I would say not everybody believes this.
 This is my personal, you know, belief,
 my taste, I guess, if you will.
 I think some people see the complexity of manipulation,
 the complexity of all these components,
 and say, "It's so complex that you can't be rigorous," right?
 And I'm saying instead, "It's so complex,
 we must be rigorous or we will fail."
 And people still look at me and say,
 "Yeah, good luck."
 [laughs]
 Okay, but I'm gonna take you through my version
 in the class, and we're gonna--
 you know, I think it won't be a burden, I think,
 if you don't believe me, but you'll at least see
 my view of the world through the course of the class.
 Any questions about that at a high level?
 I know this is pretty high-level stuff, but...
 So this belief that I have
 has taken life in this thing called Drake,
 which has been something I've been working on
 for a long time.
 It grew up in the days of controlling Atlas,
 the humanoid robot.
 That's when we started getting much more serious
 here at MIT about software engineering.
 When Toyota started their research institute,
 Drake moved over to being supported
 by professional software engineers
 and grew into a serious project.
 And then, you know, now it's being used
 by big companies and small companies.
 Lots of startups are using it.
 Amazon Robotics is using it for their manipulation stack.
 So it's grown into something big and real,
 but at its heart, it is a modeling language
 that tries to capture the complexity of manipulation
 in these sort of dynamical systems framework,
 and it has these three components, right?
 That's the systems framework
 for modeling the dynamical systems,
 for declaring the state variables,
 the parameters, and the like, okay?
 It also has a really--
 there's some really advanced physics simulation inside it.
 We have some really, really talented
 physics-engineers, if you will, researchers.
 They've done world-class--
 in terms of the sim-to-real gap,
 I think I would put Drake against any simulator out there
 in terms of the capabilities.
 And then it has a lot of tools
 for motion planning and control
 that are based on optimization, okay?
 We're gonna use-- we've made this all capable
 to be used in the class and run on the cloud
 and all these things like that,
 so it's gonna be the glue
 that puts all these pieces together.
 It doesn't try to be a machine-learning toolbox,
 like PyTorch is extremely good at being PyTorch.
 This is filling in a different part of the stack,
 the dynamics, the planning, the control,
 and they can work together, they can work with Ross, okay?
 There's a bunch of tutorials out there.
 I've seen it-- so in the past, people have said,
 "I wish you had told us a bit more about how Drake works,"
 especially when the projects came along,
 that, you know, we did problem sets,
 we were successful in our problem sets,
 but now I wanted to do something completely different,
 and I didn't have, you know, everything I needed to do that.
 So we're gonna try to balance that.
 I don't want to teach a class on Drake,
 but I want to make sure you have the resources.
 But we've also been pushing a lot more tutorials.
 In this evolution of the open-source project,
 you know, TRI made it very capable
 and was using it for research in Toyota,
 and then as more companies and more people
 were starting to use it,
 they just relatively recently have decided
 to emphasize tutorials and adoption, basically.
 So even if you took Underactuated this past spring,
 you might see how much-- there's more documentation,
 there's more user-friendly stuff
 even now than there was a few months ago,
 and even just some of the syntax that was a little gross
 is getting better, like, super fast.
 Even in the last two weeks, we've done a lot.
 So--and there's--if you do want to use it with Ross
 or in some other project, you're welcome to,
 but we'll give you a complete self-contained
 deep-note workflow for the class.
 This is just an example of the tutorials
 that talk about a lot of what I just said here.
 There's a particular modeling dynamical systems tutorial
 that talks exactly about how you would declare
 your state, your input U.
 Okay.
 I'll just say one more thing about it here,
 because we're gonna need it for the first P-set,
 which is that it turns out that if you want to--
 you know, all the complexity of modeling
 all the different things we want to do in manipulation,
 you don't get to state quite as simple as that.
 You need to have systems that have multiple rates, mixed--
 you know, they can have randomness,
 they can have parameters that you might want to tune
 with a system identification engine
 or a machine learning algorithm.
 Okay, so it gets a little bit richer,
 but in the way-- in one particular way,
 it gets richer, which is that we tend to write
 all of our functions,
 whether it's the dynamics function
 or the output function,
 to be a function of state, of input,
 but also of any randomness that comes in from an input port.
 Okay, so this would be any random inputs.
 And the reason you declare your randomness
 is so that if I just give you one random seed,
 the whole thing, the whole system
 is completely repeatable, for instance, okay?
 Parameters, P, okay, which would be, let's say,
 masses or inertias or lengths of a robot,
 or it could be the weights of a neural network
 in a deep learning system.
 These are the parameters.
 Okay, and so the functions--
 most functions in Drake
 want to be able to be a function of all of those things.
 So instead of passing those around
 as four or, you know, arbitrary numbers of inputs
 all the time, we just say,
 let's put them all into a structure, okay?
 And we're going to call it the context, okay?
 And so instead of writing this,
 you'll see in Drake, you'll see f of context.
 Okay, you see, x of n plus 1 basically
 equals f of context, or sensors is g of context.
 When you see that, you should just realize
 that just means that's just the structure
 that contains the state, the inputs,
 any random inputs and parameters, okay?
 Time also, time varying, okay?
 So that's just a--
 that throws people sometimes when they first see it,
 but it's very natural once you think about it
 as a way to write code for lots of dynamical systems.
 Okay, so the strategy for the lectures
 is to take a deep dive into--
 maybe not writing drivers, but into perception,
 and we're going to talk about both geometric perception,
 thinking about point clouds,
 thinking about point cloud registration,
 how do you do object pose estimation,
 for instance, in a point cloud,
 how do you do filtering and the like in a point cloud,
 in messy point clouds,
 but we're also going to do, of course,
 some deep learning-based perception.
 We're going to talk about both motion planning,
 level planning, and some task-level planning.
 Okay, we're going to talk about some control,
 how do you do force control,
 how do you do impedance control,
 you've heard these terms, right?
 Or what does position control even mean?
 But rather than, like, spend the first third of the class
 on perception, and the next third of the class on planning,
 and then the next third on control,
 what we're going to try to do is that by chapter three,
 you'll have a limited but a fully functional robot
 that can pick up red bricks and move them around, okay,
 if someone tells you where the red brick is, right?
 And then we'll say, okay, now someone didn't tell you
 what the red brick is, you've got to make
 an initial perception system, okay?
 And then, all right, now the scene gets really cluttered.
 How do you--how does the system have to advance?
 And that requires, you know, more work from the planning system
 and more work from the perception system, okay?
 And we're going to spiral out in this way,
 trying to make a more and more capable robot,
 and only introduce the cool tools from these pipelines
 when they make the robot do something new and different.
 Okay?
 And one other high-level point I'd like to make
 is that I've already dropped a few terms, right?
 I just said visual transformers, I said Kalman filters, you know?
 Some of you know a lot about some of those things,
 some of you haven't heard those things yet, okay?
 So when I think about lecturing to such a diverse audience here
 and really lecturing about robotics,
 one of the great things about robotics is that it's a kind of a mixing--
 a melting pot of so many different fields, right?
 And it's very hard to know everything about all of them.
 Okay, so how do I try to do that in a lecture, right?
 So I think the best way I can do that in a lecture--
 I'll take feedback, of course--
 is I try to make sure that if you know those things,
 if you've seen those things,
 I want to be able to make connections, right?
 If connecting that to a Kalman filter,
 and you've thought about Kalman filters, is useful,
 and I can say Kalman filter,
 then the people who have seen that will benefit, I think.
 And I don't want to, you know, avoid the word Kalman filter
 because we haven't talked about it yet.
 But I also try very hard, and you can tell me--
 if I ever say things and you say, "I didn't know Kalman filter,"
 but, you know, I hope you still get the point, right?
 The point of that statement about Kalman filters
 and recurrent networks and transformers
 was that a perception system, a modern perception system,
 can still be described as a dynamical system--
 should be described as a dynamical system.
 And I want to make sure you capture that level of it, at least.
 And if you ever don't, call me on it, right?
 But expect--I try to make layers of the class, right?
 So I want to be able to talk to experts.
 I want to be able to be--
 if there's things that you haven't heard of yet,
 you'd write it down.
 Maybe that's the most important thing to go read about tonight.
 Maybe it's not.
 Okay, but you don't have to--
 I hope you'll permit me to say some things
 that you haven't heard before, right?
 Because robotics is just so broad,
 and really, maybe the field isn't mature enough
 to just assume that everybody has all the prerequisites
 so that I can say--I can only build on what you've taken.
 Okay, let me finish up here.
 So you can--
 when you draw these block diagrams in Drake,
 you can render them as diagrams, and we'll do that.
 And you'll see that there's big libraries in Drake
 of different ways to--you know, different systems
 that implement these different components.
 Okay, yeah, so the schedule is completely up.
 I just told you the basic storyline
 is we're going to do basic pick and place,
 learn basic kinematics,
 learn basic Jacobian-based control and the like.
 That's going to get us off the ground with a basic robot.
 And then we'll start doing perception,
 but basic perception.
 And then we'll go back and get more cluttered scenes,
 and we're going to do this.
 It's all outlined here.
 There's a few lectures that I've left to be determined,
 and I want you guys to tell us what you're most excited about.
 Maybe I can fill out some things.
 I've certainly got plenty of things
 that I could talk about there,
 but I'd like to hear what you guys are excited about.
 The projects--all the project-based deadlines
 are up.
 They're aligned for the two versions of the class,
 but there's a few more milestones
 for the CIM component.
 Okay, so make sure you take a look through there
 and understand.
 Your goal is to hop on Piazza.
 Make sure you're there.
 So if we're in a different room on Tuesday,
 I'll tell you about that on Piazza, okay?
 And your problem set will be released very soon.
 Awesome.
 I'm looking forward to a good semester.
 I'll see you on Tuesday.
 [indistinct chatter]
 I'm so sad I didn't show the successful video.
 Hey, how's it going?
 - Separate to the class,
 I was wondering if I could ask you
 for a quick input on a problem I've been looking at.
 - Okay.
 - A little to do with under-actuated.
 - Let me just make sure there's no questions.
 [no audio]
 [no audio]
 [no audio]
 [no audio]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
