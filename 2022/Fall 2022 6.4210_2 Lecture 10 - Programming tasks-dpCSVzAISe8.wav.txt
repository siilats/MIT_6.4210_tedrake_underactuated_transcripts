 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 [BLANK_AUDIO]
 OK.
 So last time, I was drawing force diagrams.
 I was trying to use my thumb coming out of the blackboard.
 And I really wanted to have a little bit better picture for you.
 So I went and made that over-- this is what I did with the long weekend.
 And you also see the state machines I came up with, too.
 But let me try again just to say the friction cone, force cone,
 contact wrench story with a little bit better pictures.
 OK?
 So we talked about the forces.
 I'm going to just do it on slides, because it's mostly
 what I wrote on the board last time.
 And it'll go faster if I don't write everything again.
 But we talked a lot about forces.
 And in general, the spatial force, the generalization,
 is both a torque plus a force.
 So we said torque plus a force.
 This is applied at some point in space.
 You can name it.
 It can have an expressed in frame.
 You can add them.
 There's the spatial algebra of spatial forces, which sort of fits in.
 But the most important one that came up was that there's this--
 if you are considering a force applied on a body--
 I'll draw just the simple version of this to foreshadow my animation here.
 If I have a force being applied directly at a body, it's kind of weird.
 You might think it's weird to think about how can I apply a torque.
 That's not why the torques appear.
 Maybe at every point that there's a force being applied,
 it might be just a 3D force with no torque immediately.
 But I can summarize the result of this force at any point on the body.
 And if I want to think about its effect as if there was something applied
 at this point on the body, then I can write this as a force plus a torque.
 And if I want to think about the net effect of multiple forces applied
 to multiple bodies, I can move them to the same point of reference,
 sum them up.
 So the way that you move between thinking about a particular force
 being applied at a particular point on the body to a different point
 on the body, the torques come from the cross product, which
 is something you know from physics, from intro physics kind of stuff.
 So the cross product of this vector with the force
 gives me the torque.
 And then we talked about-- please ask any questions if you have them.
 I know I said that last time.
 And then we drew these diagrams and we talked about the friction cone.
 And I got a couple of good questions after just like,
 what is going on with the cone?
 Why is it not just one force?
 Why can't the world decide and pick one force?
 And the world will, of course, decide and pick one force.
 But the laws of friction that we describe
 are saying that friction will do whatever it needs to to resist motion,
 but it'll pick one element from this set, which
 is the range of possible forces.
 They all live in that cone.
 That's the way to think about this friction cone.
 The rules of friction say, I will pick whatever force inside that cone that
 will keep me from moving at all, relative to the point
 that it's being applied.
 And then if it is moving, it's just unable to completely stop motion.
 It will dissipate energy as much as possible.
 So as an abstraction, to think about the things that friction could do to me,
 it's useful to think about a whole cone of possible forces.
 And if I say there exists a force inside the cone,
 then I know friction has the ability to stop my motion.
 So it's a little bit weird to say, I'm not
 picking a force right off the bat.
 I'm going to think about all the things that friction could possibly
 do in order to resist motion.
 And that's what the friction cone is.
 Get into my animation here.
 So yeah, I guess I answered the check yourself.
 That was a flub.
 The interesting question then is, if you
 think about the set of possible things that friction could
 do to you to resist your motion, and you
 think about the basic algebra of moving forces,
 applying cross products, the algebra, given
 that the points of contact are all fixed, is all linear operators.
 These equations, if P is known, you're not moving the point of contact,
 then the way that forces change through these equations
 is always linear.
 And if you replace a particular force with a whole cone,
 and you ask how a matrix changes the cone, it turns out it stays a cone.
 And that's the magic of what I was trying to talk about last time,
 but I didn't have the animation to help me.
 I didn't write this on the board last time,
 but I'm putting it into the notes more carefully.
 The point is that you can think about shifting-- so the friction cone,
 right at the point of application, you might say there's no torsion.
 There's no torque at the point of application.
 And the cone at the point of application is defined by the xy components
 in my contact frame being less than the coefficient of friction times
 my z component.
 That's what defined my cone.
 That's just the friction cone definition.
 If I want to shift that whole cone, if I
 want to shift this whole cone of possibilities to here,
 then I can apply my same linear operators of just shifting the force
 and then getting a cross product to that whole cone.
 And that's what I'm going to animate now.
 So here's my little animation.
 So I've got a box.
 I'm not even gravity.
 It's just a box stuck to the world.
 And I'm applying a point on this box.
 The green is the immediate friction cone.
 And the red is the friction cone if I thought of it
 from the point of-- as if the point of application
 was the center of the body.
 Is that clear?
 Now here's the weird thing.
 The friction cone has six elements.
 So how do I animate a cone in six degrees of freedom?
 I'm going to just cheat a little bit.
 And there's a place where it's already going to be misleading.
 I'm going to draw this cone, the 3xyz cone.
 I'm also going to draw the torque cone as if it was independent,
 but it's not actually independent.
 So it's like the projection of the six-dimensional cone into two cones.
 I think that'll be clear in my picture.
 But that's what's happening is I'm going to draw two cones
 to represent the six-dimensional space.
 [AUDIO OUT]
 OK.
 And then I can move the body around.
 And what you see here is that no matter where
 I apply the forces on this body, I still
 get the same xyz component of that force at the body.
 That doesn't change.
 What changes is the torsional component.
 Depending on where I move around the body,
 I get a different torque, a different wrench
 that's representing my cone.
 And it's a funny shape, right?
 It's low dimensional.
 You have to look at it from here to see what's happening.
 Why is it low dimensional?
 Well, we know every element in that set
 has to be orthogonal to the cross product.
 It has to be orthogonal to the line from here to here.
 You can't produce torque except for orthogonal to that line.
 And as I move further away, I'll get this elongated cone.
 I'm drawing a truncated cone.
 The real cone, if there's no limits,
 would go on forever, but in those directions.
 Does that picture help at all?
 Oh, man.
 Does that picture help at all?
 So why is it torque over here now?
 Is that if I am applying a force that goes here,
 then my friction cone should be in--
 did I get the right direction?
 Did I get the right direction?
 I can only resist motion in one axis.
 So I resist motion like that.
 Yes?
 [INAUDIBLE]
 That's correct.
 So if I were to come up to this body and apply a perturbation,
 and you ask what can that frictional force resist,
 if the perturbation torque I applied is inside that cone,
 then friction can stop me.
 So if I apply a torque that's pushing me into the finger,
 it can stop me.
 But there's a whole other direction.
 If I apply the torque in the other direction,
 where I'm just going to move away from the finger,
 the finger won't stop me.
 That's why it's one-sided.
 The point I wanted to make last time
 was that the antipodal grasping is a good strategy.
 I think with this picture I can land that idea.
 I'm not as confident now as I was a few minutes ago,
 but let's try.
 I need a second finger.
 OK, two fingers now.
 I can move them around.
 If they're down here, what happens
 is that the friction, the torsional effect
 that that friction can have, they're
 both in the same direction, right?
 There's a whole motion that I can't resist whatsoever.
 But if I move it up, and I'm more antipodal,
 then something beautiful happens.
 So not only are they going in opposite directions.
 That's good.
 But because they're coming from a different cross product,
 they span a different space.
 So this one can accommodate wrenches
 that are in that plane.
 And this one can accommodate wrenches
 that are in that plane.
 It makes a nice little butterfly kind of looking thing.
 The rules of now saying, can I find
 something that resists me in either of those forces
 are using what's called the Minkowski sum.
 If you try to ask, if I apply a new wrench,
 can the sum of the two wrenches resist my motion,
 then that's equivalent to asking if the wrench is
 in the Minkowski sum of those two.
 And now that these are spanning the space,
 I have a nice strong wrench that I can resist with.
 So maybe you're totally on board now.
 That would be awesome.
 But at very least, when things line up, they look pretty--
 right, I mean, they look like they're covering a lot of space.
 It's an effective grasp.
 And it's different if I move them off to the side.
 But the story is still sort of the same.
 So as a general strategy, if all things are created equal,
 I'd like to be nicely aligned around the center of mass,
 because then there's no gravitational torque
 to resist whatsoever.
 And I only have to worry about external forces.
 But if I don't know much about the object, what we did last
 time was we said, let's just go ahead and just
 pick antipodal grasps.
 Because without knowing more, that's a pretty good strategy.
 Yeah?
 So because of the Minkowski sum thing,
 that's like the one blue thing dragged
 across the other blue thing.
 Exactly.
 Does that create-- does that fill the whole 3D space
 all around it so it can resist everything?
 That's what I want.
 That's the picture I want.
 The details are just a little bit more subtle.
 So the question was-- let me repeat.
 So the Minkowski sum you should think about
 as taking one element from the first set
 and then applying the entire second set
 to every element of the first set.
 So it's like you can drag it around.
 So take this cone here and apply it
 to every possible force in there.
 And so yes, it looks, from this picture,
 like you can create the whole space.
 The only thing that's a little bit misleading
 is because I've done the projection of the two--
 of the six-dimensional space under the 3D,
 you don't get to independently pick x, y, z and torque.
 So you have to be a little bit more careful about saying that.
 But to first order, absolutely, the picture
 I want you to have is that the Minkowski sum of that
 resists all torques.
 Yeah?
 Pure torques.
 I can say with confidence right from this picture,
 you can resist pure torques, all pure torques.
 Yes?
 So that's even in the torque if it was like the two points
 like this and then it was this way.
 Right.
 So think about how that would happen.
 So the reason that you can produce arbitrarily large,
 it's a function of your normal force.
 It means you'd have to squeeze harder.
 But if you're willing to squeeze harder,
 then under this friction law, you
 can resist all of those torques.
 Yes?
 Sorry, could we see the case again
 where they're not antithetical?
 Yeah.
 Yeah.
 I think the difference about spanning space is not--
 I think the coupling becomes more important.
 I think the optimality of antipodal grasps maybe
 isn't completely visible in this projection.
 If you think about only spanning the spaces.
 But maybe the magnitude already tells you
 something that it could be good.
 OK.
 It's a little better maybe, but that's this wrench
 visualization.
 You can play with it.
 I pushed it to the deep note.
 So you can play with that and tell me if it makes sense
 or doesn't make sense.
 I want to hear if it was worth it.
 There was a weird--
 I spent way too much time making this animation.
 Because there was a bug in--
 or not a bug, but a known limitation in the way
 the WebGL works in the browser.
 I was sure that my math was wrong for hours.
 And then it was not my fault directly.
 Anyways.
 So you better have learned something from that.
 OK.
 And the Minkowski sum is sort of the beautiful part of that.
 OK.
 So let me tell you now about back to the clutter clearing.
 So we were talking about this project
 with the robot moving things back and forth.
 That's kind of our short-term goal.
 You should be able to program a robot
 to do stuff like this with no knowledge of the objects,
 with dense clutter, and to do its thing.
 So that's the other thing I did.
 Hopefully it'll load in a reasonable amount of time here.
 As I finished that example, remember
 how I showed you my halfway example
 and it ran directly into the camera and it dropped the thing
 and you guys all laughed.
 We made a big file.
 So let me-- I can run it locally too.
 [SIGHS]
 OK, I'll run it locally instead.
 It's taking a long time to load.
 I made a big file, partly because I replaced the red
 bricks with other YCB objects.
 I thought that would be a little bit more interesting.
 But those stupid meshes are so big-- or the texture maps
 are so big that it takes a long time for it
 to come in the browser when I'm in campus Wi-Fi.
 OK, here goes.
 Random YCB objects thrown in a bin.
 Now it's going to start-- it's going to do our whole pipeline.
 So there's cameras here.
 They are going to do the point cloud processing.
 It actually waited for a second for the initial conditions
 to fall.
 It's going to then take the point clouds,
 do some point cloud processing, find the normals,
 take random samples, do the antipodal grasp, the grasp
 selection.
 It tries 100 random grasps.
 It picks the best one by our little grasp metric.
 And then it does everything we've talked about so far.
 It makes a simple plan of the gripper frames.
 It interpolates that into a piecewise pose trajectory.
 It then runs differential IK.
 This is the whole tool chain.
 And it's going to do its thing.
 It's going to pick up and slowly move all the objects.
 It will still fail every once in a while, but it's pretty good.
 And actually, the failures are to the point
 now where I think they're pedagogically interesting.
 It's not like I didn't spend enough time with the code.
 It's like we haven't done motion planning yet.
 And so we need a better tool for this.
 And this will go all day long.
 Every time you run it, it'll have
 different initial conditions.
 It also has some fairly-- not sophisticated,
 but the basic-- that's awesome-- some basic recovery maneuvers.
 So sometimes it'll pick-- remember,
 the antipodal grasps are just a heuristic.
 Sometimes it'll pick right on the edge of a soup
 can or something like this.
 And see, that was a double pick that it
 started to do right there.
 It rotated.
 It didn't drop this one.
 But if it does fail to grasp, it'll lift up.
 It'll realize it failed to grasp.
 It'll go back down.
 It also, when it can't find a grasp--
 I'm going to write all of this down.
 When it can't find the grasp in the first bin,
 it'll transition to the second bin
 and start picking from the other bin out.
 See, look, it stopped.
 And it realized that it did that.
 And it's going back to pick a different thing.
 That extra level of robustness makes this something that--
 [LAUGHTER]
 But it knew it.
 See, it realized it, I think.
 Yeah, come on.
 And try it again.
 See, that was one where I was like, you know, that failed.
 That's a lesson.
 And it'll keep going to the point
 where I just let it run at night.
 And I came back.
 And the funny thing was, all the little things
 you do to make it robust-- so for instance,
 I actually put an invisible floor underneath there.
 Why?
 Because it every once in a while does drop objects,
 just like the real robot did.
 And if they fall to negative infinity,
 then it makes my contact solver fail,
 because that's numerically bad to have a negative infinity
 and a number around 0.
 So now there's a floor.
 And they can only fall to like 1 meter.
 And then because there's soup cans,
 I had to put lips on the floor.
 Because they would fall to the floor.
 And they would roll off the side and then
 fall to negative infinity.
 So all the little things got figured out.
 And now it just will run all night.
 And with some probability, depending
 on how long you sleep, it'll come back.
 And many of the things will be on the floor.
 But it didn't crash.
 And it kept going and does its thing.
 The other interesting one that maybe I kind of hope we see,
 but I have mixed feelings about, is
 that differential IK can get itself in a bad state.
 Yeah, you see, it didn't try to grasp there.
 But the hand is unable to get where it's trying to grasp.
 Because the differential IK is commanding a certain thing.
 It's getting a certain different thing.
 But look, it's going to try, I think, five times.
 And then it'll give up and say, I'm done with this bin.
 I'm going to go do the other bin.
 That was even worse.
 Is that five?
 Maybe this is five.
 See?
 That's like intelligence right there.
 OK, yeah, and so there is one other super interesting thing
 it does, which is that sometimes differential IK will actually--
 because it's not thinking about the joint constraints
 of the robot.
 It could choose a trajectory that goes very close to the base.
 And then the arm starts kind of getting a little funky.
 And the Jacobian, it'll get itself kind of tangled up
 with just a differential IK view of the world.
 Because it hasn't thought of anything
 about the joint constraints of the robot.
 It's just thinking about moving a hand through the world.
 And that's an impoverished view of what the robot has to do.
 So the last thing I had to do to make
 it sort of nice and robust is that if it got itself
 all tangled up, and if the tracking error of the hand
 compared to the commanded hand got large,
 then it would say, OK, I give up.
 And it would switch.
 It would turn off differential IK,
 turn on just joint control mode, and just go,
 you know what, I'm going to come back to my comfortable home
 position, and I'll start again.
 Things got bad.
 I've got a safe space, and I'll come back in again.
 Yes?
 So in, like, I don't know, those manipulation papers
 and things like that, when you have, like,
 a video showing double results, there
 will tend to be, like, a 10x speed thing.
 So what kind of step in the whole process
 is the sort of time-consuming?
 This is all running in real time right now on my computer,
 on this laptop.
 Yeah, so that's 1x right now.
 So like, when you're, like, putting this on a real robot,
 would it be like that speed?
 The only thing that would be different on the real robot
 would be that physics would definitely run at real speed.
 And so, yeah, this is just running at real time speed.
 Yeah?
 I mean, the limitations of the motion planning
 are partly because it's simple.
 I could spend more time making motion plans,
 and then you might see the famous robot pauses,
 where it stops to think about a motion plan and then moves,
 which the community is getting better at,
 and we should aspire to not have.
 But there's no pause--
 OK, the one little thing that cheats here
 is that the simulation clock can stop while I think
 in this version of the simulation.
 But you would see that.
 If it looks like it's frozen, and even the physics
 is frozen, that's because it was thinking for a long time.
 But I don't think there's any computation that's really--
 and I'm simulating at a pretty conservative simulation DT,
 just because I wanted to not have anything--
 it can crash.
 I will list down the failure modes in a little bit.
 It can crash, but only really--
 I've only seen it crash at the initial conditions.
 If I-- my random initial bin thing
 put things in deep penetration, it could potentially blow up.
 So on a real robot, what is the--
 on actual hardware, what is the bottleneck for the speed?
 Is it the motion planning in this case?
 Like, what's stopping this from running faster?
 I could make the robot--
 I could make the trajectories faster.
 I chose relatively slow trajectories.
 If I did that, then I think the--
 yes, that's a great question.
 So there will be joint limits on the robot, joint velocity
 limits on the robot that would prevent--
 a lot of times our factory robots--
 this is originally a factory robot--
 are geared to move fairly quickly, but not super fast.
 They're not supposed to be throwing things
 across the room and stuff like that.
 And some robots are.
 So there will be joint velocities
 that will be a constraint at some point.
 On EWA in particular, there's joint--
 you can hit the joint velocity limits.
 If I just used this differential IK controller,
 then I would also worry, as I started getting close
 to those limits, that my tracking error might increase.
 And so my pick success or my running into things
 would increase.
 And a better controller at the lower level
 could-- or tighter gains, if everything's stable,
 could make that run faster.
 I hope to show you some beautiful motions of the EWAs
 in hardware soon, before we get too far in the class.
 Yes?
 [INAUDIBLE]
 You know the algorithm.
 So it looks at the point cloud.
 It doesn't know anything about the objects.
 It looks at the point cloud.
 It finds antipodal grasps.
 It takes my scoring function and takes the best antipodal grasp
 score.
 So it could do anything.
 There's actually a pretty funny failure case.
 If it drops the object immediately down in the bin,
 then it backs off, but not enough.
 I could easily have fixed this, but I just didn't.
 It backs off, but not enough.
 And then the hand is still in the camera.
 And it sees a point cloud and some really appealing
 antipodal grasp.
 So it starts going-- in thin air, trying to pick itself.
 But then after that, it gets out of the way.
 And so it's just a one-time snafu, and I let it go.
 OK.
 So I feel like you know how to do-- oh, sorry.
 Go ahead.
 [INAUDIBLE]
 I cropped.
 So I feel like it's not cheating,
 since I know where the bins are in this application,
 to just crop the point cloud to be in the interior of the bin.
 So it won't find that.
 Although, one strategy for getting that one--
 I guess that one's just going to hit me every single time
 in this run, right?
 Because it's never going to move.
 And every time it's going to be the last one, it tries to pick.
 So every time it's finished with the x bin,
 it's going to try to do this for five or six times.
 On the real robot, we actually-- so we did something
 where we kind of pushed it out of the corner.
 That was a pretty good way.
 But the other thing we did, actually, to your point,
 was it was really good to pick the bin and shake it.
 That was another good way to get things out of the corner.
 But I didn't let it do that.
 So you have the entire toolkit for this,
 except for the high-level recovery-type planning.
 And so I want to talk about that now.
 Lesson learned.
 I won't put an enormous file in the middle of my presentation.
 OK.
 I'm just going to leave it there,
 even though I'm not quite ready for that,
 just so that the browser is responsive.
 OK.
 So the way that I programmed that was with a state machine--
 switch off over here-- which is the simplest
 type of programming at the sort of task level.
 Where did I put the-- oh, here it is.
 It's getting old.
 OK.
 So I want to talk a bit about programming
 sort of at the task level.
 That's what people often call it.
 Not the individual motion.
 This is maybe as opposed to motion planning.
 Task planning is high level.
 What should I do in what order to accomplish
 my long-term goals?
 Should I be picking out of the bin on the x-axis
 or the bin on the y-axis, for instance?
 Should I be-- did I drop something?
 Do I have to stop and take a recovery?
 These are the high-level plans.
 OK.
 And the simplest approach we'll talk about
 would just be writing a finite state machine.
 Which, if you've taken an algorithms class,
 you've seen these.
 It's called an FSMs, for instance.
 It's their theory of computation class.
 It thinks about state machines and what they can describe
 and the like.
 OK.
 So this robot, or this demo, has four states.
 It would be better if it had more, probably.
 But I implemented four and got pretty far with that.
 So the first one is just waiting for objects to settle.
 My warm-up phase.
 Because remember, I initialized the objects in the sky,
 and they fall down.
 And until I put this mode in, it would try to pick--
 it would take the point cloud when the things were still
 falling, and then it would be trying
 to pick objects in the sky.
 But by the time it gets there, they were long gone.
 So that was ridiculous.
 And I decided to add an initial setup phase where it just
 waits for a second or two.
 And then I'm going to be picked from-- I'm in a mode where
 I'm picking from the x bin, which is just
 what I'm calling the bin that's on the positive x-axis,
 because I didn't have a left or right, obviously.
 A different mode for picking from the y bin, which
 is actually on the negative y-axis.
 And then I have this go home mode,
 which is when things get bad, it shakes itself out of problems
 and does the right thing most of the time.
 Cool.
 So I can think about the way that those modes interact
 by drawing a simple state machine diagram.
 I guess there's also technically one more which would be done.
 I didn't call it a state because I just
 used it as a cert statement.
 If it can't find any objects, if it tries like six times
 and there's no place to pick in six times,
 then it's like I'm just going to assert failure.
 But I've got my weight.
 I've got my pick from x, my pick from y, and then my go home.
 And it's useful to think about organizing
 the behaviors of the robot in a graph type structure, where
 I can think about from waiting, after I've
 done waiting for the objects to fall,
 I allow it to transition to either of these two
 possible states.
 It prefers to go to pick x.
 I'll tell you the conditions in a second.
 Certainly, if I'm picking x and I've
 decided there's nothing left to pick,
 then I can transition to picking y.
 If I'm picking from the y bin and I can't find any graphs,
 I'll switch back to picking from the x.
 My logic is basically deciding what
 would I do to continue here and do
 I need to transition to the other mode.
 Similarly, from either of those states,
 if I get myself with large tracking error,
 then I can just give up and go home.
 We didn't see it happen once in that.
 It hopefully doesn't happen too often.
 And I can do that from either of these states.
 And the go home, technically, it almost always
 goes to pick x, but it could actually
 go to pick y if there was nothing in x.
 Maybe I'll even say it really kind of goes to pick x
 and then transitions from there here.
 That's how I coded it.
 So this edge here, if I were to just draw it,
 what are the conditions on this?
 The condition on this is am I done waiting?
 More than one second has elapsed.
 And I found a grasp in the x bin.
 This is simple stuff.
 But that's the logic I encoded on that edge, effectively.
 And similarly, I'll transition from here to here.
 If I can't find any more grasps here,
 and I find a grasp over in y, then I'll
 switch mode picking in y.
 That way, it will repeatedly pick from y
 until it's roughly done.
 So it's staying in this state.
 And then it'll go back.
 If I had no internal mode logic, then it might pick from x.
 And then the next time, it might find x again.
 It'd be kind of annoying.
 It would just move one object back and forth.
 I want some sort of persistence saying
 to pick from x until you're done,
 then pick from y until you're done.
 So I make these edges not transition
 until you've failed to find a grasp.
 No grasp.
 Now, this turns out to be a powerful methodology.
 I can transition to this if I can't find any grasps.
 That's the no grasp cases.
 This is a very simple version of the basic state machinery.
 But you'd be surprised how many robots
 you've seen out there that are probably
 using something very much like this
 to program the task level.
 The reason you have to think about it like this-- so we're
 used to writing procedural code.
 You're used to writing code that says, do this, then do this,
 then do this, for loop, while loop.
 That's the way we're all most used to writing code.
 You don't get to do that if you have to be a robot
 or in a simulation where you have to have an answer,
 you have to have an action, something that you must output
 at every time step.
 I can't be somewhere in the while loop doing my thing.
 You must have an answer at every time.
 And that's what changes the programming paradigm
 from instead of the pure procedural logic
 to something that's organized more like a state machine
 or more like behavior trees or more something like this.
 It's the requirement that you must tell me
 what to do at every time step.
 You're not allowed to go off and think forever.
 All right, so how do you implement that as a system?
 Because we're trying to use this--
 we're going to ultimately write our planner system.
 Well, it's not that hard to do.
 I'm actually very interested-- maybe at the end of the lecture
 I'll ask you again about what you
 think would help you the most in your projects.
 Maybe you can even tell me on your project proposals.
 Because I've always felt like low level control, motion
 planning, even perception, we've given you
 a lot of tools for that.
 But people feel limited in their projects by this level
 because I haven't given you lots of tools for this.
 So you could tell me what is most useful.
 But the simplest version, these kind of state machine diagrams
 are very easy to write in the systems framework.
 All you do is use the simplest version of it.
 There are more advanced versions that actually do event detection
 to decide to transition.
 Let's leave that out for now.
 Let's just say every dt, every time step,
 I'm going to start up, I'm going to wake up,
 I'm in one of these modes.
 And I take whatever action that's inside there
 and I output that to my output port.
 And I'll decide should I transition to the next one
 for the next time step.
 That's a simple discrete time difference equation type
 system.
 It happens to have complicated logic, potentially.
 The conditions about whether you change,
 it's not a linear dynamical system.
 But it's a perfectly reasonable conditional difference
 equation.
 And you can do that by just saying,
 I'm going to go ahead and tell myself to update.
 In that example, I updated every 0.1 seconds.
 The simulator is taking time steps at like a millisecond.
 So the physics is updating very often.
 The planner is only updating 10 times a second
 because you don't need to decide things
 if I drop the thing at 100 or even 1,000 hertz.
 And then, so basically it says, call my update function
 every 0.1 seconds.
 And then I'm going to declare that I have some state.
 I tried to use words and I used a Python enum
 just to make it readable.
 So if I want to use a Python enum as my state,
 we call that an abstract value state.
 I can tell you if you like why it's abstract value.
 Maybe actually, maybe you care.
 It's a programming paradigm called type erasure.
 So basically, I want to be able to pass things
 through the systems framework without understanding the type.
 Think about converting it to like a void pointer, void star
 pointer, or something like that.
 And I could take any-- if I take any kind of type
 and I want to turn it into something
 that I reason about in the systems framework,
 I can make it an abstract value.
 It erases the type.
 It passes it through.
 But the people who read it know how to put the type back on.
 That's what the abstract value make and abstract value get
 does, is it just erases the type and passes it through
 and then adds the type back.
 So if it looks weird in the code,
 it's doing something clever, standard?
 I don't know.
 OK.
 So I just say I've got a state here,
 which is of this planner state type.
 And its initial value is wait for objects to settle.
 Right?
 Simple.
 And then in my update, I can get out of my context.
 So that just declares something that lives in the context, which
 has a erased type.
 I don't even know what it is.
 But the context can reason about it with the type erasure idea.
 And I can get it back at the beginning of my update
 context, and then I can write it to my context
 at the end, whatever my new decision of my new mode was.
 And in the middle, I can do all kinds
 of logic about which state I should go to next,
 what my actions are during that state.
 That's the basic way that you write a difference equation
 that's using some sort of abstract type,
 like a mode that I'm in.
 Do people like when I talk about that?
 Or this isn't real robotics?
 Yeah?
 OK.
 OK, good.
 And then the other piece that's happening here
 to make that work is when I transition from wait
 to pick, for instance, when I transition into either pick
 or pick x, pick y, or even go home,
 I'm actually calling my planner.
 Only on the transition, not when I'm inside it.
 I call my planner saying, when I transition in,
 I'm going to decide from where I am right now,
 how do I go down and pick and place?
 It's using exactly the stuff you've known.
 I pulled it right out of the previous notebook.
 I've used make gripper frames, make gripper pose trajectory,
 make gripper commanded trajectory, right out
 of the existing examples.
 But I want to save that plan so I don't have to recompute it,
 so I add that to my context.
 I just say I'm going to take the whole plan, which
 is a piecewise pose or piecewise polynomial,
 and I'll register that with the context also.
 That way it's available when I'm inside here.
 I can pull up what's my plan, and I can just
 evaluate the trajectory at the current time.
 That's the architecture.
 So you can imagine assembling pretty--
 I just write if statements in order
 to make this state machine logic.
 And I can make my plans and declare any internal state
 that these things need with the systems framework.
 And it can do all these things.
 So the architecture then is I actually pulled out--
 whenever you can be more modular in the code,
 that way I can reuse this in the next example or whatever.
 I try to pull things out.
 So I have the grasp selection algorithm
 is put out into one system.
 It expects three cameras to tell me what the point cloud are.
 But I use two copies of this.
 I use one for the y bin and one for the x bin.
 I just tell them for the one bin,
 you're going to use camera 0, 1, 2.
 The other one, you're going to use cameras 3, 4, 5.
 Body poses is one of the output of this manipulation station,
 which just tells me where my gripper is.
 That's what roughly it's the internal state
 estimate of the manipulation station.
 And it does the sampling algorithm.
 And it tells me what the cost was and the best pose,
 the best cost and the best pose.
 If that cost is infinite, then I've
 considered myself to have failed the plan.
 Now the cool thing is the systems framework
 tries to optimize some of these operations for you.
 So if nobody asks for the output of the grasp selector,
 then it doesn't do any computation.
 It only does that computation on when you say,
 give me your output.
 So the algorithm here just says, sample 100 points, whatever.
 But it's not running most of the time.
 It's only running when I transition in the planner
 on this, or I decide to transition,
 and I ask, what's your grasp?
 So there's just a mental switch of saying,
 how do you write code that gives an answer every time?
 And state machines are a way to think about that.
 But otherwise, it's pretty standard stuff.
 The planner, it has a bigger job,
 just because this x-bin grasp and y-bin grasp
 are coming from the two grasp selectors.
 But it has to know where the arm is
 and what's the current gripper open and closed.
 Basically, this is the gripper pose,
 and this is the gripper open close.
 And then it outputs-- it makes its plan.
 It looks at what time it is.
 It looks at the plan that it's already saved,
 and it just pulls out xwg, the gripper commanded pose,
 hands that down to differential IK.
 All the other three are just to make it
 so you can stop using diff IK and go home if you need to.
 I have to be able to switch between using diff IK
 or not using diff IK downstream.
 That's what my control mode is.
 I have to send the EWA position trajectory when I'm doing that.
 And I also have to tell diff IK to stop trying to integrate
 and just look at the current state of the robot.
 Otherwise, if you turned off diff IK for a while
 and then turned it back on, it would have no idea where you are
 and would command something completely wrong.
 So you just have to be able to say, hey, diff IK,
 look at your current state.
 Don't try to integrate forward.
 So I would say those are just details.
 I was borderline about whether I should even put them in,
 but I think it just adds so much more robustness
 and it exercises the ideas a little bit more.
 So as an example, I thought that was useful to have.
 Yes?
 [INAUDIBLE]
 Yeah.
 [INAUDIBLE]
 Yes.
 Yes.
 So as you were asking your question,
 I was anticipating your question.
 I realized I forgot the self transitions.
 But you asked something different.
 So I will ignore that error.
 That is completely hard coded.
 Just to make the demo interesting,
 I basically pick a random point in the bin to put it down.
 It goes to a stereotype gripper.
 This is the make gripper trajectory.
 I just have key frames.
 And every time I'm going to put something down,
 I pick a random place somewhere in the box.
 And I pick that.
 And then I do the pre-pick, post-pick stuff to set it down.
 And every once in a while, you'll
 see something ridiculous, like it'll stack mustard bottles.
 Because it just randomly picked the same place
 a couple times in a row.
 And that wasn't a good idea.
 But next time, probably if that's a tall mustard bottle
 tower, it'll tend to knock it over next time.
 And randomness has been restored.
 Entropy is restored.
 Good.
 So I just want you to feel like you can do that.
 And you have to look at the example,
 or you can just pull this example up and adapt it.
 I want that to be available for you.
 But it's really-- you can author pretty complicated things
 pretty quickly.
 And people do.
 So well, let me just-- I guess I'll say my failure modes first
 here.
 I kind of made a list as I was watching it,
 like what actually makes it fail.
 And I could burn down any one of these
 and make it never happen.
 But I thought it's interesting at this stage of the class
 and say what's actually failing.
 The first one is my initial conditions, like I said,
 is kind of silly.
 And with some probability, like the mustard bottle
 could be inside the soup can.
 And multi-body plant says I can't
 find forces that will get me out of collision in one step.
 And it fails on time zero, basically.
 So if it does that, forgive me.
 Just start it again.
 It's all good.
 Again, I could make a better initial guess sequence
 to resolve that.
 Motion planning becomes a real bottleneck.
 This simple heuristic of grasps is too simple.
 And it will occasionally bump into things.
 It will occasionally collide the object
 it's picked up with the bins.
 It'll occasionally go too close to its own base.
 So if it chose to go from that corner to that corner,
 and I just made a straight line interpolation,
 it'll go way close to its own base.
 And the arm starts like, I don't know what to do about that.
 We haven't respected the joint limits of the robot.
 And so differential IK becomes problematic
 for those kind of reasons.
 Ergo, we will spend a week on motion planning in a few weeks.
 And we'll make that way better.
 Perception we talked about before, even at the antipodal.
 You see it in this demo regularly.
 It'll pick at the corner of an object.
 And then the object will be swinging around.
 Because why?
 Because the wrench generated by the gravitational force
 is outside the current grasp's friction cone.
 And so it'll swing.
 It does do double picks, like we talked about,
 because it doesn't know what an object is.
 The phantoms, like I said, it tries to pick itself.
 If the hand was in the point cloud,
 it'll be like, oh, that's a good place to pick.
 It goes up and just picks the phantom hand.
 That one would be really easy to fix.
 But I just thought it was hilarious, so I left it.
 And then the other ones are actually super interesting,
 I would say.
 Like, it's actually very hard to do much better in this case.
 We will do it.
 We'll try to do it.
 But we'll use force control and nonprehensile manipulation,
 is what it's called, to do things that it's not just
 picking in place.
 We'll have to use the side of our hand
 to push objects into the corner, apply
 forces to get it to lift up so we can get our hand under it.
 It's not just about getting your hand around it and squeezing.
 It's going to be much more than that.
 And when objects get stuck in the corner--
 I actually had to remove the Cheez-It box from my list.
 Cheez-It boxes are not allowed in this demo,
 because they constantly would block all the other objects
 with something that was too big for the gripper
 to pick up from the top.
 And I was just like, no Cheez-It boxes.
 That's it.
 That's for next week.
 OK.
 So if I were to just-- I wanted to even-- because we
 got to a certain level of robustness-- this
 is like the weakest robustness I would ever
 talk about being robust.
 But it's actually interesting to think about,
 like, what would I do if I were to start now
 applying our stronger tools to make that more robust?
 There's actually very strong tools you could apply.
 And if you've had an internship at an autonomous driving
 startup or something like this, you might have used some
 of these tools.
 The difference of going from a little toy project
 to something that has to work every time, which
 is far from right now.
 But there's great tools that we could use.
 And they're all available if you choose
 to use them for your projects or whatever.
 But for instance, when we were working on this,
 this was also very complicated at the task level.
 And one of the things that happens
 is when you're doing so many different things
 and you have task level interacting
 with low-level controllers and all these things,
 really bizarre things can go wrong.
 When you're writing code, you should
 try to write-- almost always, you
 should try to write unit tests or component-level tests.
 That's the only real way to write code,
 is to, every time you write a small function,
 make sure that function does what you want.
 But for weird interactions between the components
 that happen when you start taking all this complexity in,
 you have to do more.
 You have to do some level of system integration testing.
 And we're already getting to this point
 with a simple clutter clearing demo.
 So the way that we do this-- and again,
 this is part of the motivation behind the systems framework,
 declare your state, declare your randomness--
 is we have a Monte Carlo test suite.
 You can run off in the cloud lots of clutter clearing
 simulations.
 They're all deterministic, given the initial conditions
 of the initial context, which includes the random seed that
 governs everything afterwards.
 And you just let it go off into the cloud,
 and you write a little thing that
 detects whether it's a good success or a failure.
 And it tells you in the next morning how often you've failed.
 It'll even make a little movie and put it in your inbox
 if you want.
 And say, it'll record the last few seconds
 and tell you these are the statistics of failure,
 and these are the specific failure cases.
 And you just hammer on that.
 You hammer on that, and you actually
 make your Monte Carlo tests harder
 if I make the initial conditions more diverse
 or if I add more sensor noise.
 On that project, it was actually very interesting,
 because we had the red team that was
 trying to make the noise models more aggressive
 and other things like this.
 They were trying to make the simulation harder to pass.
 And then we had the people that were
 trying to make the robot better.
 And so there'd be a night where someone would see,
 like, git commit 1492 or something like this.
 And then suddenly the score would go way down,
 because it would start failing more.
 And you realize, oh, they added new noise models
 to the dish rack detector.
 And then you see, like, three commits later, someone fixed it
 and then it goes back up.
 And my dream was that we would go up and hit 99.999%.
 That's not what happened.
 We stayed basically flat, because the red team was just
 as fast as the good guys.
 But that was good, because on the real robot,
 on the real hardware, we got to 99.9%.
 But we made the simulation so aggressively bad,
 it'd be like an autonomous driving always trying to drive
 down through, I don't know, downtown New Delhi
 or something like that.
 Or like a bumper cars or something like that.
 It was the simulation was throwing everything at you
 all the time.
 And the real robot was way easier by the end.
 And you can use the Monte Carlo simulation suite if you like.
 We don't have the cloud version of it in the public drake,
 but we're thinking about putting it over.
 And there were some really subtle bugs
 that it would find that we actually found in simulation.
 And then we're like, no way that could happen on hardware.
 And then sure enough, we saw it happen on hardware.
 It was really fun to like-- because actually,
 when we started turning on all this, the first thing we did
 is we found all kinds of bugs in our physics engine.
 Not like easy bugs, we got f equals ma wrong,
 but like numerical issues when things fall to infinity
 and stuff like that.
 Really subtle things.
 But then when-- or sorry, really like annoying things
 that were bad failures, I would say.
 But then as we kept running it, we
 got to the really good failure.
 We burned out all the sim to real kind of stuff.
 And we found very subtle bugs.
 And this is my favorite little example.
 OK, so let me see if I can--
 yeah.
 So there was this case where the robot would pick up a mug.
 And we saw this in reality.
 Pick up a mug, go to set it down, and be like, hmm.
 And then we would set the mug down,
 and try to pull the rack out, and pick up the mug.
 Actually, no, it didn't even pull the rack out.
 It would just go like this.
 And it would set it back down, and then pick the mug up.
 Set it back down.
 And it just would get in this infinite loop.
 We're like, OK, that's a super low probability event, right?
 That was never going to happen.
 It happened in the real robot.
 And it was actually an infinite loop, effectively
 an infinite loop.
 And this is why.
 OK, it was the night that the guys added the sensor
 noise to the dish rack locator.
 OK, so what they did is it was a very simple noise model.
 It just said the perception system for the dish rack
 would just, on every step, pick a random number.
 It would adjust the true dish rack location by some Gaussian.
 OK?
 And there was a particular location
 of the dish rack that found this weak threshold that somebody
 had put in their code, where basically,
 with very high probability, when it started making the motion,
 with very high probability, it would
 think the dish rack was out enough to put a mug down.
 But over the time it took to actually put the mug down,
 with very high probability, it would
 find at least one sample that said the dish rack was too far
 in.
 And I had to go back and set it down.
 So the whole planner would go, OK, fine, put it down.
 But then by the time it went again,
 with very high probability, it was out enough.
 And this thing would just go like this.
 The day we saw that in the real robot,
 we were like, a simulation is great.
 And it was like this.
 This is what I mean about the crazy, hard simulation.
 The robot was trying to do, in simulation,
 it was trying to put dishes away while the dish rack was going
 like this, which is nuts.
 But being robust to this inferred robustness
 in the real world.
 OK.
 So this type of programming paradigm is very simple.
 But it's actually used all the time.
 My favorite example, actually, I used it
 and under-actuated it, too.
 Mark Raybert used to talk about his simple controller
 for a hopping robot.
 And the best thing about it is it fits completely on one page.
 And it looks like this.
 He's got a flight phase, then a landing phase,
 then a compression of the spring phase.
 And then it pushes off with the leg.
 And it unloads.
 And it's back in the flight phase.
 He used the language of state machines
 to talk about his controller and used that language
 to describe a fairly complicated controller on a page
 in his book.
 It's one of my favorite examples of rich behavior coming out
 of a simple state machine.
 And for me, it's a goal that we should aspire to
 with our best control designs.
 Something so simple could achieve something so complex.
 But normally, when we do optimization-based control,
 we don't get that simple stuff out.
 I don't know if you've seen this version.
 I know Boston Dynamics puts out awesome videos all the time.
 This was one from a few years ago.
 This was Spot opening doors.
 Andy was a student here.
 Maybe that's why I like it so much.
 But Spot is really good at opening doors.
 I've never seen the code.
 I know a lot of people that work there
 but have conversations informally.
 But I don't know anything for real.
 But I'm pretty sure that's a state machine that's
 doing all the same kind of logic we're doing here,
 where if someone pulls my butt, then probably that case
 wasn't specifically hand-coded.
 But it's surprisingly robust behavior
 in the real world on a real robot.
 I like to call them robot whisperers, people
 who know how to write these state machines really well
 and can make really robust behaviors out.
 Now, our goals as researchers are probably
 to do less hand-designed state machines, type controllers
 like that.
 But they're getting it done in industry today.
 And if you look in the robotics open source toolbox,
 there are, for instance, SMOC.
 It's actually pretty dated now.
 People still use it, though.
 This is a state machine.
 Right in Ross, people have state machine toolboxes and the like.
 If you look at the individual skills and the disloading
 task--
 I should run that again as I say it--
 there was a simple state machine there.
 It was actually written in a way that
 was kind of a mix of procedural logic and state machine logic.
 But it's even non-trivial every time you pick up a mug.
 You approach.
 You see that little visual servoing?
 That's ICP.
 Remember I told you about that before?
 It does a little ICP.
 I thought the mug was going to be here.
 I'm going to use my hand cameras to adjust it.
 And then I'm going to insert to grasp, retract,
 move to my pre-place, all the stuff we're doing here.
 And that was done at the low level control
 for the disloading demo.
 My favorite one, actually, that programmed a really rich
 behavior was when it picked up plates.
 So this is the sim.
 Obviously, this is real.
 That was a pretty complicated maneuver
 to get this big hand with a big robot
 to reliably pick up plates like that.
 And it was coded with the same sort of logic,
 where it would be a kind of-- the way we had it authored
 in code was a kind of a mix of procedural and state machine
 logic, but a simple script.
 It's kind of what you'd expect.
 And the art is in picking the parameters
 of these little transitions and the like,
 but the script is exactly what anybody would write down,
 pretty much.
 So for instance, I asked Siwan, who wrote this, to describe it.
 And he's funny.
 He's like, you kind of scooped the gripper into a solid grasp.
 And these are terminated with sensor-based feedback.
 So it would kind of scoop up until it felt collision
 with the hand in the palm.
 And then it would stop that and transition to the next thing.
 But these state machines are real.
 And they allow you to program pretty complicated things.
 Those couple examples were all at the sort of low level.
 People find that the state machines-- this picture,
 this view of the world, or the one for your new system,
 kind of works when you have, I don't know, 10 states
 or something like that, or a simple structure.
 Maybe that one's relatively sequential,
 so you tend to go forward until you break out.
 But if you get more serious and you
 try to build a more and more complicated robot out of this,
 then this paradigm breaks, roughly.
 It breaks in a couple ways.
 It just becomes very complicated to author a big diagram
 and get all of the transitions that you'd want correct.
 And people found out that it was very hard to write one state
 machine and take-- you maybe have a subgraph here.
 You think, I'd really love to take that subgraph
 and use it in a slightly different application,
 have a modular sense in the state machines.
 And something about the state machine architecture,
 people decided it was very hard to do that.
 It's very hard to take a piece and use it.
 Because you have to sever all the edges coming in.
 And that changes the behavior of the state machine.
 And then you have to rewire it to all the new places.
 And that is so delicate that it was very hard to do that.
 So the different machinery that people have authored,
 this actually came from the computer games world.
 This is like gamers were writing big state machines
 to make their Unreal games and the like.
 And it just broke.
 And so they invented a new paradigm
 called behavior trees, which are very state machine-like.
 Since it's late, I won't talk about the details here.
 We have a problem that will help you think through it.
 But it's another programming paradigm that's similar to this.
 But the computation is organized a little bit differently.
 At every time step, you run through the behavior tree.
 And they can tell you whether you're done
 and you should stop, or whether you should
 continue on to the next one.
 Each of these nodes has a little bit of programming logic.
 And the consensus has been that this slightly different
 programming paradigm allows you to write much bigger machines
 that are much more modular, where people will frequently
 take their behavior tree, a big chunk out of this behavior tree
 and stick it in another behavior tree
 and immediately be happy with the behavior.
 It really does come out of gaming.
 That's where it started.
 But roboticists use it too.
 In fact, to prove that, I found another reference
 to the ROS behavior tree package.
 This is PyTrees ROS.
 And it's actually a really nicely architected software
 package.
 And you can make a behavior tree fit in the same way
 we did a state machine.
 That could be a system that's reasoning
 about all the discrete logic and making-- that fits right
 into the system's framework.
 I haven't made it easy for you to do that.
 And so again, at the very end, I'll ask just--
 or over time as you think about it,
 I could try to make some of these things easier to do.
 But OK, I'm sorry that I'm just going to keep going
 a little bit since I won't do the stretch right now.
 So the interesting thing is that a lot of the state machine
 and even the behavior trees, they
 get you to a certain place, but oftentimes people need more.
 And the reason you tend to need more
 is if you have some very long term planning required.
 If there are consequences of your actions
 now that need to be sequenced in a certain order
 to achieve a long term goal.
 Like maybe I need to have all the mugs in the top shelf
 in order to accomplish my dish loading.
 And that takes it to a different paradigm, which
 is the planning paradigm, where you
 tend to take these low level skills
 and combine them into some sort of a skill framework.
 And we'll work on skills and motion planning
 a lot going forward.
 But I want to foreshadow that here
 with just thinking about how it's related to state machines.
 So for the dish loading example, there
 were a bunch of skills like pushing things out
 of the corner, picking up mugs, picking up silverware.
 These were all authored as different skills.
 And then rather than write down one time some big automaton,
 or some big state machine, we would
 define the rules of interaction and use a planner
 to decide what the action was on every time step.
 OK.
 And that's got a long history in AI.
 Strips is the original name of it, this planning architecture,
 where you just say, I've got an initial state, I've got a goal.
 And basically, think of it as doing graph search
 on a discrete graph, saying I'm going to do this action, then
 this action, and then this action, in order
 to accomplish a long term goal.
 You have an initial state, a goal state,
 and a set of actions where those actions are authored
 as I have a potential edge, saying
 based on what conditions must be satisfied for me
 to write to do a pick x, and then what conditions
 will be satisfied if I do pick x for a while.
 And strips is the old version.
 Padiddle, the planning domain definition language,
 is the newer, richer language of these things.
 I could tell-- I'll just move past the details of that.
 But I think it's super interesting to look
 at how this worked in the specific application
 of the dish loading.
 So we had our skill concept, our action primitive,
 which basically you had to-- in order to write an action
 primitive in the framework, you had to basically say,
 given the current state, could I run my skill right now?
 Yes or no.
 And if I were to run that skill, what would be the new state?
 And that's pretty much it.
 And the skills that we did, the actions that we implemented,
 were-- this is the entire list.
 Open dishwasher door, close dishwasher door,
 start dishwasher.
 That was pretty funny.
 It wasn't actually plugged in, because we
 weren't allowed to run water to the lab
 in that particular way, which is probably a good idea
 in retrospect.
 But the robot, we built a whole skill
 with a capacitive sensor, because it
 was a smart dishwasher.
 So you needed to put one of your stylists
 from your standard stylus to make a capacitive sensor work.
 And we mounted it on the back of the EWA.
 And we had a skill that would just go and boop.
 And the dishwasher would go doo-doo-doo-dee.
 And we were good.
 Yes?
 What's the optimistic part?
 Optimistic program.
 Yeah, right.
 Is there a pessimistic one, too?
 Just program.
 Yeah.
 So my guess is-- I actually don't know.
 I would go look.
 But it's possible to get unjammed
 by these kind of things.
 There was an aggressive version that was more optimistic.
 That's pretty funny.
 I copied the list, but I didn't actually
 look at that one carefully.
 Yes?
 So there's no learning so far, right?
 It's fully planned.
 Exactly.
 That's a great point.
 There's no learning so far.
 And I think that's-- my last slides are like,
 how do you put learning into this?
 But this is not learning so far.
 Second question.
 How do we evaluate the goal?
 Because for example, if you have a dishwasher
 and you want to put a mug into the sticky thing
 in the dishwasher, and for some reason
 it drops out after you put it-- so you have actually
 to go to cheat and put it all out.
 But then, like, the door is the whole space,
 so you kind of put watermarks there.
 It's a hard problem.
 So yes, in the limitations of this,
 one of the big limitations is that you
 need to define a state, a symbolic state of the world.
 And I'll put this on a slide in a minute.
 And you need to somehow have the perception system tell you
 what the state is and have a model for how
 that state's going to evolve.
 And that is a really hard thing to do.
 And it's a limitation of the framework.
 So goals are typically authored in a logical way,
 but they are often impoverished because of all the things
 that can happen in the world.
 The dishwasher state was things like,
 are the clean items put away?
 The number of clean items put away,
 the number of dirty items available.
 It's a very discrete state that summarizes
 what the robot perceived in the sink.
 And that's all right out of the code.
 But if you take those discrete logic
 and author yourself a bunch of skills,
 then you can do a little planning on the fly
 and get all the robustness that you see.
 So this is like the example of someone
 went and closed the dishwasher rack
 and they had to set the mug down.
 That's exactly what it was.
 The robustness here made us susceptible
 to the crazy, noisy racks.
 So Boston Dynamics, they kick the robot.
 But we just closed the dishwasher.
 We could have kicked the dishwasher, I guess.
 But this time, instead of a state machine
 making that robustness, it's a planner
 that makes that robustness.
 You just say, these are the actions I can do.
 This is when I'm allowed to do it
 and what the outcomes are going to be.
 And I roughly do a graph search, a richer version of graph search
 to make that happen.
 And you can get the same levels of robustness out.
 This is exactly what you were asking.
 The problem with this approach, the limitation
 of this approach, is that it requires somehow
 to take those discrete states and acquire them
 from perception.
 And you have to have a model of how
 that state is going to evolve if I apply certain actions.
 And these can be very brittle in the real world.
 So there's been famous debates about this.
 And one of the most famous, maybe,
 was Rod Brooks, who wrote a nice paper called
 "Elephants Don't Play Chess," arguing against intelligence
 without reason.
 But there was a line of arguments
 that he made basically saying the AI community is going down
 the wrong direction by trying to summarize the world
 with symbolic states.
 It's just too hard.
 Elephants don't play chess.
 They don't do planning like this on every time step.
 That's the chess analogy.
 And when he was saying this, it was right before he went off
 and started iRobot and made Roomba,
 which was highly successful without playing chess,
 with just kind of bumping around.
 And you know how it did that, the Roomba application,
 initially at least.
 It's probably evolved now.
 But it was basically-- Rod's version of it
 was called the subsumption architecture.
 But it was the precursor to behavior trees.
 Roomba was much more like a behavior tree.
 And it was saying, don't rely on planners.
 It's too brittle for the real world.
 Symbols whose grounding in physical reality
 has rarely been achieved, says Rod.
 And then I'll just end saying that I do think
 one of the new challenges-- and you'll see lots of papers
 about this right now--
 is how do I make this sort of skills framework,
 long-term decision making, work when those skills are learned?
 Where now, the way people are picking up plates at TRI now
 is with policies, controllers, that
 were acquired by learning.
 And we have demonstrations that are much more robust.
 Perception failures and other things like this now,
 they can pick up plates very robustly coming out
 of machine learning.
 And the question is, if I want to put that
 into a skills type framework so a high level task planner can
 reason about it, or write a state machine about it,
 or whatever, how do you combine the new tools from learning
 with the classic tools from planning?
 And that's a big topic we'll talk more about.
 Good, the last thing I'll say is that we're
 making the-- because you have your project proposal next
 week, the PSAT will be a little smaller this time
 to accommodate.
 But please, again, ask us about project proposals
 and use the pre-proposals as a chance to get good feedback.
 OK, thanks.
 [INAUDIBLE]
 Oh.
 [INAUDIBLE]
 [INAUDIBLE]
 [INAUDIBLE]
 In particular, if you switch from under-preparedness
 to preparedness, how do you do that?
 [SIDE CONVERSATION]
 [BLANK_AUDIO]
